<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jiarui Lu</title>
  <icon>https://www.gravatar.com/avatar/68ce4f3ec3d87765e9ad4be701622026</icon>
  <subtitle>Non-Stop</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jaredlujr.github.io/"/>
  <updated>2020-09-11T00:54:55.847Z</updated>
  <id>http://jaredlujr.github.io/</id>
  
  <author>
    <name>Jiarui Lu</name>
    <email>jaredlujr@gmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>algorithm-intro</title>
    <link href="http://jaredlujr.github.io/2020/09/11/algorithm-intro/"/>
    <id>http://jaredlujr.github.io/2020/09/11/algorithm-intro/</id>
    <published>2020-09-11T00:50:41.217Z</published>
    <updated>2020-09-11T00:54:55.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="算法原理-Algorithm-1"><a href="#算法原理-Algorithm-1" class="headerlink" title="算法原理-Algorithm(1)"></a>算法原理-Algorithm(1)</h1><h2 id="算法研究的意义"><a href="#算法研究的意义" class="headerlink" title="算法研究的意义"></a>算法研究的意义</h2><p>可计算性 -&gt; 能行可计算性 -&gt; 算法设计与分析(本课程) -&gt; 编程语言实现 -&gt; 软件系统集成</p><p>例：插入排序O(N^2)但是在小数组上性能很棒，归并排序方便并行，且可以与插入排序组合。</p><h2 id="算法的定义"><a href="#算法的定义" class="headerlink" title="算法的定义"></a>算法的定义</h2><p>计算：可由给定的计算模型机械之行的计算步骤序列成为该模型下的一个计算。</p><p>算法（形式定义）：算法是满足下列特征的计算(given by Donald Knuth) *有三本书可以用来学算法</p><ol><li>输入：必须有大于零的输入量。</li><li>输出：应当有一个或以上的输出作为计算的结果。</li><li>确定性：算法描述无歧义，每一步都是严格定义和确定的动作。</li><li>有穷性/终止性：通常要求算法有限步内必须停止。</li><li>可行性：每一个动作都能够被精确地机械执行。</li></ol><h2 id="通用计算模型：图灵机"><a href="#通用计算模型：图灵机" class="headerlink" title="通用计算模型：图灵机"></a>通用计算模型：图灵机</h2><p>由 有穷控制器 + 无限长的纸带 组成，有穷控制器可以通过左右移动的磁头读写纸带。现在计算机可执行的计算都可以由图灵机在有穷步完成。</p><p>所有的计算机都属于图灵机，也属于冯诺伊曼架构。</p><h2 id="现代计算模型：RAM"><a href="#现代计算模型：RAM" class="headerlink" title="现代计算模型：RAM"></a>现代计算模型：RAM</h2><p>Random Access Machine用于算法度量的一种，与实际机器无关的模型。RAM机的处理器是单线程处理器，指令完全按顺序执行，即不考虑并行操作。有以下特点：</p><ul><li>每个基本操作（原子）需要1个时间单位（loop和subroutine不是基本操作）</li><li>每次存储器访问需要一个时间单位（不考虑内存溢出问题，也不考虑cache命中/miss时间）</li><li>解决给定的问题所需时间，由RAM机种操作的时间单位总和度量；所需的空间由RAM机占用的存储单元的数量来度量。</li></ul><p>算法复杂性考量：<strong>空间 时间</strong></p><h2 id="算法问题的描述—形式语言"><a href="#算法问题的描述—形式语言" class="headerlink" title="算法问题的描述—形式语言"></a>算法问题的描述—形式语言</h2><p>Input Output两个set，分别为问题R的输入集合和输出集合，问题可以定义为一个关系$R \subset I \times O$.</p><p>如排序问题sorting（形式定义）：<br>Input = {<a1,a2,...,an>| ai整数}；<br>Output = {<b1,b2,...,bn>| bi整数，b1$\le$b2$\le$…}<br>SORT = {…子集匹配}</p><p>虽然更数学，没有bias，但是形式定义不太直观描述。</p><h2 id="算法描述方法—伪代码"><a href="#算法描述方法—伪代码" class="headerlink" title="算法描述方法—伪代码"></a>算法描述方法—伪代码</h2><ul><li>不面向特定编程语言</li><li>简洁清晰为上，抛弃细节（关注重点，直击要害），可以使用自然语言描述一个可能复杂的问题</li><li>只关心流程的正确性，不关心编译，变量声明等</li><li>不关心软件工程问题，忽略数据抽象，模块性，错误处理</li><li>举例：Euclidean算法</li></ul><h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><p>目的：</p><ul><li>求解问题：得到问题的解，或者执行一流程操作</li><li>优化问题，从众多算法中找到最优（效率，解的优化程度）算法</li></ul><p>学习思路：</p><ul><li>一般到特殊：将常见算法进行分类总结，以宏观角度指导【未知】算法设计</li><li>特殊到特殊：学习不同领域的众多算法，类比地去解决</li></ul><p><strong>算法图谱</strong>：一般思考顺序</p><p><img src="algorithms/problem-solve.png" alt="img"></p><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><ul><li>算法 正确性分析</li><li>算法 复杂性分析</li><li>算法 实验分析</li></ul><h3 id="算法正确性"><a href="#算法正确性" class="headerlink" title="算法正确性"></a>算法正确性</h3><p>如果它对于<strong>每一个输入</strong>都最终停止并产生正确的输出结果。</p><p><em>/不正确的表现</em>：</p><ul><li>对所有或者部分输入不停止</li><li>对所有输入都停止，但是对部分输入产生错误输出</li></ul><p>以上是确定性算法。</p><p><em>/随机算法特征</em>：</p><ul><li>对所有输入都停止</li><li>可能产生错误解，但是与输入不相关，且错误解的产生可控</li></ul><p><em>/近似算法特征</em>：</p><ul><li>对所有输入停止</li><li>对所有输入产生近似解</li></ul><p>算法正确性的证明方法：</p><p>！注意程序调试通过$\ne$算法正确！</p><ul><li>证明算法对所有输入停止</li><li>证明算法对所有输入产生正确解</li><li>常用“归纳法”</li></ul><h4 id="循环不变式"><a href="#循环不变式" class="headerlink" title="循环不变式"></a>循环不变式</h4><p>即算法中循环所具有的性质，主要用来说明算法的正确性。（循环也是算法复杂的来源之一）</p><p>需要：（前两条正是数学归纳法）</p><ul><li>“初始化”：在循环第一次迭代前循环不变式为真</li><li>”保持“：如果循环的某次迭代前为真，下次迭代之前仍为真</li><li>“终止”：循环中止时依然为真，且提供一个有助于证明算法正确性的性质。</li></ul><p>举例：插入排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INSERTION-SORT(A)</span><br><span class="line">  for j &#x3D; 2 to A.length</span><br><span class="line">    key &#x3D; A[j]</span><br><span class="line">    i &#x3D; j-1</span><br><span class="line">    while i&gt;0 and A[i] &gt; key</span><br><span class="line">      A[i+1] &#x3D; A[i]</span><br><span class="line">      i &#x3D; i-1</span><br><span class="line">    A[i+1] &#x3D; key</span><br></pre></td></tr></table></figure></p><h3 id="算法复杂性"><a href="#算法复杂性" class="headerlink" title="算法复杂性"></a>算法复杂性</h3><ul><li>复杂度分析的目的：预测算法对不同规模输入所需要资源，为算法选择提供依据</li><li>复杂度分析的结果：所需资源随输入变化的函数</li><li>复杂度分析三个方面：<ul><li>输入规模</li><li>时间复杂度</li><li>空间复杂度</li></ul></li><li>主要研究在输入规模足够大的情况下的增长趋势（即渐进复杂度）。有以下的特性：<ul><li>最坏复杂度</li><li>最好复杂度</li><li>平均复杂度</li><li>均摊复杂度</li></ul></li></ul><p><strong>时间复杂度</strong>：对特定输入规模得到结果所需要原子操作的“步”数。</p><ul><li>是输入规模的函数</li><li>暗含假定每一步的执行需要常数时间（相同）</li><li>完全不考虑算法之外的问题</li></ul><p><strong>空间复杂度</strong>：对特定输入规模得到结果所需要利用的存储空间的大小。</p><p>举例：插入排序的复杂度分析</p><ul><li>for循环，步长位移 (*注意终止条件位移也是耗时的)</li><li>赋值</li><li>加法，乘法</li><li>条件语句（按照最差:每次均进入 / 最好:每次均不进入情况计算最好/最坏复杂度）</li><li>注意每层循环内语句都要加一层累加$\sum$</li><li>平均复杂度（一般按照均匀分布选择条件语句执行与否）</li></ul><p>一般算法的复杂度分析<strong>重点关注循环</strong>，找到循环层-&gt;阶数。</p><h3 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h3><ol><li>确定实验目的</li><li>决定度量效率的标准和度量单位</li><li>准备测试样例</li><li>用程序实现待测算法</li><li>插入计数器等，运行算法并加载特定输入样例，记录实验数据</li><li>分析</li></ol><p><strong>度量标准</strong>：</p><ul><li>操作次数<ul><li>原子操作</li><li>timer</li></ul></li><li>wallclock（程序外部测量，如手动计时或者系统UNIX的time命令）<ul><li>程序开始和结束</li><li>C/C++可以用clock()</li><li>Java可以用System类的currentTimeMillis()方法</li></ul></li></ul><p><strong>样例准备</strong>：</p><ul><li>随机数生成</li><li>利用结果图分析</li><li>内推法 外推法</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;算法原理-Algorithm-1&quot;&gt;&lt;a href=&quot;#算法原理-Algorithm-1&quot; class=&quot;headerlink&quot; title=&quot;算法原理-Algorithm(1)&quot;&gt;&lt;/a&gt;算法原理-Algorithm(1)&lt;/h1&gt;&lt;h2 id=&quot;算法研究的意义
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Maximum entropy(Maxent) model</title>
    <link href="http://jaredlujr.github.io/2020/04/15/maximum-entropy/"/>
    <id>http://jaredlujr.github.io/2020/04/15/maximum-entropy/</id>
    <published>2020-04-15T07:51:23.000Z</published>
    <updated>2020-04-15T15:27:17.460Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Maximum-Entropy-theory"><a href="#Maximum-Entropy-theory" class="headerlink" title="Maximum Entropy theory"></a>Maximum Entropy theory</h2><p>Maximum Entropy theory is a general idea in the view of information theory. Every matter tends to have a increasing entropy, including the information.</p><p>For any random event, the entropy is a measure of its uncertainty, and here is the extreme case being that all of the possibilities have the same probability to happen.</p><p>In statistical opinion, the entropy is a measure of the <strong>smoothness</strong> of the (probability) distribution of data. Just like in the statistical thermodynamics, the tendency of exhaustion on the matter’s order. Here, we can see the limit of entropy increasing is the uniform distribution.</p><script type="math/tex; mode=display">p(y|x_i) = \frac{1}{|Y|}</script><p>where the conditional probability is introduced, and it is used to describe the typical samples in machine learning: $T=\{(x^{(i)},y^{(i)}),i=1,\dots,n\}$.</p><p>Now we give the strict definition of the information(data) entropy by:</p><script type="math/tex; mode=display">H( p ) = - \sum_{x\in X} \sum_{y\in Y} p(y|x)log(p(y|x))</script><p>where the double $\sum$ means the consideration of entropy is on the product set $X\times Y$, not only for given dataset $T$.</p><p>The variable $p$ of $H(p)$ is in nature a given <strong>probability distribution</strong>, and its entropy $H$ is like a score to evaluate it.</p><p>The basic idea of Maximum Entropy is to solve the following optimization:</p><script type="math/tex; mode=display">p^\ast(y|x) = arg\max_{p\in P} H(p)</script><p>$s.t. ~ \sum_{y\in Y} p(y|x)=1 , \forall x.$</p><p>Obviously without any extra constraints, the uniform distribution is the solution. However, in the real world scenario, the constraints will be given as the observation, the known data. So our goal is to maximum the entropy without violating the know data, say letting the unknown part be <strong>equiprobable</strong>.</p><p>Just like the maximum margin in SVM, the maximum entropy provides a criterion, an objective function for the search in the hypothesis space.</p><h2 id="Maximum-entropy-model"><a href="#Maximum-entropy-model" class="headerlink" title="Maximum entropy model"></a>Maximum entropy model</h2><p>Given training set $T=\{(x_i,y_i)\}$</p><p>We firstly consider the empirical union distribution and side distribution as follows:</p><script type="math/tex; mode=display">\widetilde P (X=x,Y=y) = \frac{1(X=x,Y=y)}{N}</script><script type="math/tex; mode=display">\widetilde P(X=x) = \frac{1 (X=x)}{N}</script><p>Here is simply counting the frequency of the samples.</p><p>Then, use feature function $f(x,y)$ to describe some fact of input $x$ and output $y$:</p><script type="math/tex; mode=display">f_d(x,y)=\begin{cases}1, & x,y ~ \text{satisfy  fact d} \\0, & otherwise\end{cases}</script><p>Then the expectation of feature function $f (x,y)$ w.r.t $\widetilde P (X=x,Y=y)$ is given by:</p><script type="math/tex; mode=display">E_{\widetilde P}(f) = \sum_{x,y} \widetilde P(x,y) f(x,y)</script><p>The expectation of feature function w.r.t $\widetilde P(X=x)$ and model</p><script type="math/tex; mode=display">P(Y | X)</script><p>is given by:</p><script type="math/tex; mode=display">E_P(f) =\sum_{x,y} \widetilde P(x)P(y|x) f(x,y)</script><p>Thus, we can see that the feature function $f_d$ is introduced to activate the feature $d$ so that we can only count on the activated samples.</p><p>If the model does acquire the information during training, we may assume they are equal, i.e.</p><script type="math/tex; mode=display">E_{\widetilde P}(f) =E_P(f)</script><p>or</p><script type="math/tex; mode=display">\sum_{x,y} \widetilde P(x,y) f(x,y)=\sum_{x,y} \widetilde P(x)P(y|x) f(x,y)</script><p>Here are the constraints of Maxent model, and the number of feature functions is the same as that of constraints.</p><h3 id="Definition-Maxent"><a href="#Definition-Maxent" class="headerlink" title="Definition: Maxent"></a>Definition: Maxent</h3><p>Let</p><script type="math/tex; mode=display">C\equiv \{P\in \mathbb P |E_{\widetilde P}(f_i) =E_P(f_i),i=1,\dots,d\}</script><p>be the collection of those models satisfying the given constraints, then the <strong>conditional entropy</strong> of some $P$ is:</p><script type="math/tex; mode=display">H(P)=-\sum_{x,y} \widetilde P(x) P(y|x) log(P(y|x))</script><p>And the selected model is exactly the model with maximum entropy.</p><p>Note that we have introduced $d$ constraints as</p><script type="math/tex; mode=display">E_{\widetilde P}(f_i) =E_P(f_i)</script><p>but also we have $\sum_{y\in Y} p(y|x)=1 , \forall x.$ as our normalization constraint.</p><p>It may comes as confusion about the constraints with its feature functions. Let us consider a very simple case, there is always one probability distribution satisfying the constraints:</p><script type="math/tex; mode=display">P(y|x_i)=\begin{cases}1, &y=y_i\\0, &otherwise\end{cases}</script><h2 id="Solving-the-Maxent"><a href="#Solving-the-Maxent" class="headerlink" title="Solving the Maxent"></a>Solving the Maxent</h2><p>By Lagrangian and solving the dual problem, we have the following form of $P$ which will be the solution:</p><script type="math/tex; mode=display">P_w(y|x) = \frac{e^{\sum_{i=1}^d w_if_i(x,y)}}{\sum_{y}e^{\sum_{i=1}^d w_if_i(x,y)}}</script><p>or inner product form:</p><script type="math/tex; mode=display">P_w(y|x) = \frac{e^{\vec w \vec f}}{\sum_{y}e^{\vec w \vec f}}</script><p>The denominator $Z_w(x) = \sum_{y}e^{\vec w \vec f} $is called the partition function, and $w$ are <strong>Lagrangians</strong> of the constraints.</p><p>And then the final solution $P$ to the Maxent is the <strong>MLE</strong> of $P_w$:</p><script type="math/tex; mode=display">\argmax_w L_{\widetilde P} (P_w) = \sum_{x,y}\widetilde P(x,y)\vec w \vec f - \sum_{x} \widetilde P(x) logZ_w(x)</script><p>In fact, the Maxent model and Logistic Regression model have similar form, so they are also called the <strong>log-linear</strong> model.</p><p>To conclude, the central idea of Maxent is to fit in the known data and let the unknown one as “max-entropy” as possible.</p><hr><p><strong>Reference</strong>:</p><ul><li>Statistical Learning methods, by Li Hang (2012)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Maximum-Entropy-theory&quot;&gt;&lt;a href=&quot;#Maximum-Entropy-theory&quot; class=&quot;headerlink&quot; title=&quot;Maximum Entropy theory&quot;&gt;&lt;/a&gt;Maximum Entropy theo
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="NLU" scheme="http://jaredlujr.github.io/tags/nlu/"/>
    
      <category term="Statistics" scheme="http://jaredlujr.github.io/tags/statistics/"/>
    
  </entry>
  
  <entry>
    <title>How to do feature selection in Machine learning?</title>
    <link href="http://jaredlujr.github.io/2020/04/14/feature-selection/"/>
    <id>http://jaredlujr.github.io/2020/04/14/feature-selection/</id>
    <published>2020-04-14T14:06:26.000Z</published>
    <updated>2020-04-14T15:14:18.268Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>“ There ain’t no such thing as a free lunch” is appropriate for machine learning.</p></blockquote><p>Model selection is an inevitable question for any machine learning-based task. Before we actually dive into the search of the best model, how to effectively evaluate a model on given dataset $T=\{(x^{(i)},y^{(i)})\}$.</p><p>The main focus should be on the overfitting, which can also be the biggest problem in machine learning. To be more specific, we need to do the trade-off between bias, the accuracy on the given dataset, and variance, the expected accuracy on the original data distribution.</p><p>Two extreme cases are that, taking polynomial model for example, the linear fitting and high-order polynomial fitting. Suppose our loss function is MSE, then linear model may underestimate the data complexity thus giving “underfitting” results. On the contrary, the high-order polynomial can easily overfitting.</p><ul><li>High-robustness model, with low fitting effect</li><li>High-fitting effect, with low-robustness</li></ul><p>Generally, the expected loss comes from three parts:</p><ul><li>bias, the model fails on the dataset</li><li>variance, the model fails on the new data</li><li>noise, from the problem itself</li></ul><h3 id="Pillars"><a href="#Pillars" class="headerlink" title="Pillars"></a>Pillars</h3><p>We usually give the optimization manners of given model by the following things to consider:</p><ul><li>Consistency, say to guarantee generalization</li><li>Model convergence speed</li><li>Generalization capacity control</li><li>A strategy for good learning algorithm</li></ul><h3 id="How-to-control-generalization-capacity"><a href="#How-to-control-generalization-capacity" class="headerlink" title="How to control generalization capacity?"></a>How to control generalization capacity?</h3><p>There is a simple rule for the risk evaluation:</p><p>Risk expectation = Empirical risk + Confidence interval</p><p>To directly minimize the Empirical risk by following ERM is often not a good idea; instead, we want to minimize the Empirical risk and Confidence interval at the same time.</p><p>This is called <strong>Structural Risk Minimization(SRM).</strong> Like SVM, the margin defined in SVM actually offers an effective way to simultaneously decrease the both terms. However, the Logistic Regression is only ERM.</p><p>When it comes to SRM, we are also talking about the regularization. If we introduce soft restriction, so-called penalty term over weights, the condition will be better, like Ridge regression and Lasso regression.</p><script type="math/tex; mode=display">RG(w,b) = S\{(y^{(i)}-<w|x^{(i)}>-b)^2 , i=1,\dots,L\} + \lambda ||w||_2</script><p>In terms of MAP in Bayes estimation, one interesting thing is that if the prior distribution is Gaussian, then the MAP estimation is in nature $l2$ regularized MLE.</p><h3 id="Methods-for-model-selection-Cross-validation"><a href="#Methods-for-model-selection-Cross-validation" class="headerlink" title="Methods for model selection: Cross validation"></a>Methods for model selection: Cross validation</h3><ul><li>hold-out cross validation { [training set : dev set] : test set}</li><li>$k$-fold cross validation</li><li>leave-one-out cross validation</li></ul><p>The implementation is straightforward, so the content is left out.</p><h2 id="Feature-selection"><a href="#Feature-selection" class="headerlink" title="Feature selection"></a>Feature selection</h2><p>Having determined the framework of our model, we move on consider the feature selection. You may wonder why bother doing so? Look back on the quotation in the head, more features will definitely lead to less efficient, or worse results.</p><ul><li>some algorithms scale (say computationally) poorly with increased dimension</li><li>irrelevant features for the classification can confuse the algorithms</li><li>redundant features adversely <strong>Affect regularization</strong></li><li>The model can be heavy if the features are large</li></ul><p>Given $n$ features, there are $2^n$ possible feature subsets, or our selection results. We mainly expect the removal of <strong>redundant useless features</strong>, say high relevancy with another features, and <strong>irrelevant features</strong>, approximately uniform distribution for each class.</p><p>There are three main types of methods for selection:</p><ol><li>Filter(recommended)<ul><li>algorithm independent</li><li>many choices</li><li>small computational cost</li><li>perserve data manifold structure</li></ul></li><li>Wrapper<ul><li>enumerate following orders</li><li>computationally heavy</li><li>one by one operation</li></ul></li><li>Embedded<ul><li>regularization models are typical example</li><li>when training, it forces some features to be small even zero</li></ul></li></ol><hr><p><strong>Reference</strong>:</p><p><a href="http://cs229.stanford.edu/notes/cs229-notes5.pdf" target="_blank" rel="noopener">Stanford CS229-note5</a> by Andrew Ng</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;“ There ain’t no such thing as a free lunch” is appropriate for machine learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Model selection is an
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Inclusion-exclusion Principle</title>
    <link href="http://jaredlujr.github.io/2020/04/14/inc-exc-principle/"/>
    <id>http://jaredlujr.github.io/2020/04/14/inc-exc-principle/</id>
    <published>2020-04-14T12:18:29.000Z</published>
    <updated>2020-04-14T13:25:08.905Z</updated>
    
    <content type="html"><![CDATA[<p>The inclusion-exclusion principle is a famous one in combinatorics, and it can be applied to the discussion of set, counting, and in particular finite probability space.</p><p>Using the languange of set, a simple example of I-E principle is as follow:</p><script type="math/tex; mode=display">|A\cup B\cup C|=|A|+|B|+|C|-|A\cap B|-|A\cap C|-|B\cap C|+|A\cap B\cap C|.</script><p>where $’+’$ means including while $’-‘$ indicates the exclusion: we always firstly include all the “atoms”(cannot be further divided) without overlapping consideration of them. Then we minus specific portion by pairwise intersections, which is the simplest case being more complex than the “atoms”. Repeat until the $n$-tuple-wise, the whole stuff is included or excluded.</p><p>Note that we switch the “add-minus” operations one after another, say there is no consecutive same operation for $’+’$ or $’-‘$. This is like, when we cannot decide the accurate quantity, we would like to give an approximate value and re-evaluate the difference, and do it again and agian until the terminal.</p><p>After a little thought we have that the sign in front of the last intersection is $’+’$ if $n$ is odd, or $’-‘$ if $n$ is even.</p><p>Now, we will introduce the generalized definition of I-E principle.</p><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>Suppose we have finite set $A$ and a set of properties over $a\in A$: $X=\{P_1,\dots,P_n\}$. The goal is to count the number of those elements that <strong>at least have property</strong> $P_j \in T\subset x$.</p><p>Equivalently, we may give a subset of $A’s$ power set: $A_1,A_2,\dots,A_n \subset A$, and define:</p><script type="math/tex; mode=display">x\in A \text{ has property } P_i \Leftrightarrow x\in A_i</script><h2 id="Formula-1"><a href="#Formula-1" class="headerlink" title="Formula-1"></a>Formula-1</h2><p>Then we give the following relation:</p><script type="math/tex; mode=display">N_{=}(T) = \sum_{T\subset S}(-1)^{|S|-|T|} N_{\ge}(S)</script><p>where $N_\ge(T)$ is the number of the elements with at least properties in $T$( it may also satisfy the property in $X-T$ ), and $N_=(T)$ means exactly having the properties in $T$.</p><p>Remarks:</p><ul><li>the sum over each $S$ including $T$</li><li>$S\subset X$</li></ul><p>In particular, we consider the extreme case that $T=\empty$, then</p><script type="math/tex; mode=display">N_=(\empty) = \sum_{S}(-1)^{|S|} N_{\ge}(S) = |A| - \sum_{|S|=1} N_{\ge}(S) + \sum_{|S|=2} N_{\ge}(S) + \dots + (-1)^n N_\ge (X)</script><h3 id="Combinatorics-Explanation"><a href="#Combinatorics-Explanation" class="headerlink" title="Combinatorics Explanation"></a>Combinatorics Explanation</h3><p>Consider the contribution of each $x\in A$ to the R.H.S.:</p><p>If $x\in A$ fails to have all the properties contained in $T$, then it contributes $0$ to the counting;</p><p>Else if we may assume there will be extra $m&gt;0$ properties that $x$ satisfying besides $T$, we have:</p><script type="math/tex; mode=display">0<m\leq|X|-|T|</script><p>(We can also introduce a intermediate set as $Z\in X$ containing $m$ properties.)</p><p>Also, it is easy to obtain that the contribution here is $\sum_{i=0}^m \binom{m}{i}(-1)^i = (1-1)^m=0$ and</p><script type="math/tex; mode=display">i=|S|-|T|</script><p>Then in the end, only $m=0$, i.e.,</p><script type="math/tex; mode=display">S=T,|S|=|T|</script><p>survive, which is exactly the L.H.S. It means that the sum in the R.H.S. in nature is self-eliminated and only have the $N_=(T)$ left.</p><h2 id="Formula-2"><a href="#Formula-2" class="headerlink" title="Formula-2"></a>Formula-2</h2><p>Equivalent to the formula-1, we give the following statement:</p><p>Let $A_1,A_2,\dots,A_n$ are subsets of finite set $A$, and $\forall T\subset [n]$ as index set, have</p><script type="math/tex; mode=display">\left| (\bigcap_{i\in T} A_i) \cap (\bigcap_{j\notin T} \bar A_j)\right| = \sum_{T\subset S \subset [n]} (-1)^{|S|-|T|} \left|\bigcap_{i\in S}A_i\right|</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The inclusion-exclusion principle is a famous one in combinatorics, and it can be applied to the discussion of set, counting, and in part
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Combinatorics" scheme="http://jaredlujr.github.io/tags/combinatorics/"/>
    
      <category term="Counting" scheme="http://jaredlujr.github.io/tags/counting/"/>
    
  </entry>
  
  <entry>
    <title>Image processing using PIL and NumPy</title>
    <link href="http://jaredlujr.github.io/2020/04/11/image-process-pil/"/>
    <id>http://jaredlujr.github.io/2020/04/11/image-process-pil/</id>
    <published>2020-04-11T08:15:31.000Z</published>
    <updated>2020-04-11T08:39:22.925Z</updated>
    
    <content type="html"><![CDATA[<p>By reading the images by PIL-Image module and storing as NumPy-ndarray, we can conveniently process the images.</p><p>RGB (color) images become 3D ndarray (row (height) x column (width) x color (3)), and black and white (grayscale) images become 2D ndarray (row (height) x column (width)).</p><p>Range: $0-255$, each dimension.</p><h2 id="Read-an-image-and-store-as-ndaaray"><a href="#Read-an-image-and-store-as-ndaaray" class="headerlink" title="Read an image and store as ndaaray"></a>Read an image and store as ndaaray</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">im = np.array(Image.open(<span class="string">'lena_square.png'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the datatype and be chosen by adding parameter</span></span><br><span class="line"><span class="comment"># dtype=np.int32</span></span><br><span class="line"><span class="comment"># dtype=np.float</span></span><br></pre></td></tr></table></figure><h2 id="Saving-ndarray-as-image-file"><a href="#Saving-ndarray-as-image-file" class="headerlink" title="Saving ndarray as image file"></a>Saving ndarray as image file</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pil_img = Image.fromarray(im_f.astype(np.uint8))</span><br><span class="line">pil_img.save(<span class="string">'lena_square_save.png'</span>)</span><br></pre></td></tr></table></figure><p>Note that if the pixel value is represented by $0.0 \to 1.0$ (normalized), it is necessary to multiply by $255$ and convert to uint8 and save.</p><h2 id="Single-color-version-of-image"><a href="#Single-color-version-of-image" class="headerlink" title="Single-color version of image"></a>Single-color version of image</h2><p>The image file can be converted into the single-color version by avoiding the color mixing up: setting other color values to $0$ and concatenate them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">im = np.array(Image.open(<span class="string">'data/src/lena_square.png'</span>))</span><br><span class="line"></span><br><span class="line">im_R = im.copy()</span><br><span class="line">im_R[:, :, (<span class="number">1</span>, <span class="number">2</span>)] = <span class="number">0</span></span><br><span class="line">im_G = im.copy()</span><br><span class="line">im_G[:, :, (<span class="number">0</span>, <span class="number">2</span>)] = <span class="number">0</span></span><br><span class="line">im_B = im.copy()</span><br><span class="line">im_B[:, :, (<span class="number">0</span>, <span class="number">1</span>)] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">im_RGB = np.concatenate((im_R, im_G, im_B), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="Inversion"><a href="#Inversion" class="headerlink" title="Inversion"></a>Inversion</h2><p>A negative-positive inverted image can be generated by subtracting the pixel value from the max value (255 for uint8).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im = np.array(Image.open(<span class="string">'data/src/lena_square.png'</span>).resize((<span class="number">256</span>, <span class="number">256</span>)))</span><br><span class="line"></span><br><span class="line">im_i = <span class="number">255</span> - im</span><br></pre></td></tr></table></figure><h2 id="Binarization"><a href="#Binarization" class="headerlink" title="Binarization"></a>Binarization</h2><p>First, we can convert the image to grayscale as follows.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im = np.array(Image.open(<span class="string">'data/src/lena_square.png'</span>).convert(<span class="string">'L'</span>).resize((<span class="number">256</span>, <span class="number">256</span>)))</span><br><span class="line">print(type(im))</span><br><span class="line"><span class="comment"># &lt;class 'numpy.ndarray'&gt;</span></span><br></pre></td></tr></table></figure><p>To be more complex, the threshold can be set and by applying the boolean values trick we can filter the image into black or white.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">im_bool = im &gt; th</span><br><span class="line">print(im_bool)</span><br><span class="line"><span class="comment"># [[ True  True  True ...,  True  True False]</span></span><br><span class="line"><span class="comment">#  [ True  True  True ...,  True  True False]</span></span><br><span class="line"><span class="comment">#  [ True  True  True ...,  True False False]</span></span><br><span class="line"><span class="comment">#  ...,</span></span><br><span class="line"><span class="comment">#  [False False False ..., False False False]</span></span><br><span class="line"><span class="comment">#  [False False False ..., False False False]</span></span><br><span class="line"><span class="comment">#  [False False False ..., False False False]]</span></span><br><span class="line"></span><br><span class="line">im_bin_128 = (im &gt; th) * <span class="number">255</span></span><br><span class="line">print(im_bin_128)</span><br><span class="line"><span class="comment"># [[255 255 255 ..., 255 255   0]</span></span><br><span class="line"><span class="comment">#  [255 255 255 ..., 255 255   0]</span></span><br><span class="line"><span class="comment">#  [255 255 255 ..., 255   0   0]</span></span><br><span class="line"><span class="comment">#  ...,</span></span><br><span class="line"><span class="comment">#  [  0   0   0 ...,   0   0   0]</span></span><br><span class="line"><span class="comment">#  [  0   0   0 ...,   0   0   0]</span></span><br><span class="line"><span class="comment">#  [  0   0   0 ...,   0   0   0]]</span></span><br></pre></td></tr></table></figure><p><em>For display effect, the white pixel has the value of $255$ and the black one has $0$.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;By reading the images by PIL-Image module and storing as NumPy-ndarray, we can conveniently process the images.&lt;/p&gt;
&lt;p&gt;RGB (color) images
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Python" scheme="http://jaredlujr.github.io/tags/python/"/>
    
      <category term="CV" scheme="http://jaredlujr.github.io/tags/cv/"/>
    
      <category term="NumPy" scheme="http://jaredlujr.github.io/tags/numpy/"/>
    
      <category term="Image-processing" scheme="http://jaredlujr.github.io/tags/image-processing/"/>
    
  </entry>
  
  <entry>
    <title>TD-based Estimation of Action-Value Function</title>
    <link href="http://jaredlujr.github.io/2020/04/10/td-prediction2/"/>
    <id>http://jaredlujr.github.io/2020/04/10/td-prediction2/</id>
    <published>2020-04-10T07:13:20.000Z</published>
    <updated>2020-04-11T08:14:56.770Z</updated>
    
    <content type="html"><![CDATA[<p>On top of our discussion of Monte-Carlo and TD(0) algorithm, we now turn to consider the intermediate one between them.</p><h3 id="n-step-TD"><a href="#n-step-TD" class="headerlink" title="n-step TD"></a>n-step TD</h3><p>If we let TD target $R_{t+1}+\gamma V(S_{t+1})$ look $n$ steps into the future.</p><p><img src="nstep.png" alt="alt nstep"></p><p>Then the algorithm will transform gradually from $TD(0)$ to $MC$.</p><p>We define the $n$-step return:</p><script type="math/tex; mode=display">G_{t}^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})</script><p>where $n=1$ is the same as TD(0), and $n=\infty$ is the same as MC, which will reach the end of an episode. The update formula is</p><script type="math/tex; mode=display">V(S_t) := V(S_t) + \alpha(G_t^{(n)} - V(S_t))</script><h3 id="TD-lambda-forward"><a href="#TD-lambda-forward" class="headerlink" title="TD($\lambda$): forward"></a>TD($\lambda$): forward</h3><p>The update of Model-free algorithm is based on the episode, say the observation. How to make full use of these information to update is our focus.</p><p>We may consider the “decay” way by factor $\lambda$ of n-step return $G_{t}^{(n)}$ in the following form as $\lambda$-return $G_{t}^{\lambda}$:</p><p><img src="lambda1.png" alt="alt lambda"></p><script type="math/tex; mode=display">G_t^{\lambda} = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}</script><p>and update</p><script type="math/tex; mode=display">V(S_t) := V(S_t) + \alpha(G_t^{\lambda} - V(S_t))</script><p>where $(1-\lambda) \cdot \lambda^{n-1}$ is the weighted decay:</p><ul><li>Normalization: $\sum (1-\lambda) \cdot \lambda^{n-1} = 1$</li><li>$\lambda=0$, just same as TD(0)</li><li>$\lambda=1$, degenerate to MC (only the last term $G_t^{\infty}$ survive)</li></ul><p><img src="lambda2.png" alt="alt lambda2"></p><h3 id="TD-lambda-backward"><a href="#TD-lambda-backward" class="headerlink" title="TD($\lambda$): backward"></a>TD($\lambda$): backward</h3><p>If we view the TD($\lambda$) backward, we may find some mechanism of the update.</p><p>By looking backward on an episode, we can define the <strong>Eligibility trace</strong> as follows:</p><script type="math/tex; mode=display">E_0(s) = 0</script><script type="math/tex; mode=display">E_t(s) = \gamma \lambda E_{t-1}(s) + 1(S_t=s)</script><p>where the future states are not involved.</p><p>With TD-error as:<br>$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$</p><p>Now the update formula has the form</p><script type="math/tex; mode=display">V(s) := V(s) + \alpha \delta_t E_t(s)</script><p><img src="bv.png" alt="alt bv"></p><h4 id="What-does-the-E-t-s-mean"><a href="#What-does-the-E-t-s-mean" class="headerlink" title="What does the $E_t(s)$ mean?"></a>What does the $E_t(s)$ mean?</h4><p>The Eligibility trace is like a credit assigned to a given state in episodes, and we would like to assign credit to <strong>most frequent states and most recent states</strong>, which are respectively two terms in the definition.</p><p>If we perform offline updates, say update value function after a batch accumulation, then the forward-view and backward-view is <strong>identical</strong>.</p><p>For example, consider an episode whre $s$ is visited once at time-step $k$, then</p><p>$TD(\lambda)$ eligibility trace discounts time since visit,</p><script type="math/tex; mode=display">E_t(s) = \gamma\lambda E_{t+1}(s) + 1(S_t=s) =\begin{cases}0 & if ~ t< k\\(\gamma\lambda)^{t-k} & if ~  t\ge k\end{cases}</script><p>It means that the $E_t(s)$ depends on the “distance” from the appearance at $k$ to current time-step $t$ and target state $s$. We can see it as a decay-weighted effected of visiting a state.</p><h3 id="TD-lambda-and-TD-1"><a href="#TD-lambda-and-TD-1" class="headerlink" title="TD($\lambda$) and TD($1$)"></a>TD($\lambda$) and TD($1$)</h3><p>Under the offline update, the sum of TD errors telescopes into MC error and TD(1) roughly equivalent to every-visit Monte-Carlo. Since the error is accumulated, so the total update result is exactly the same as MC, say the “shattered” MC.</p><p>In summary, the comparison among these algorithms is made as the following table:</p><p><img src="comp.png" alt="alt comp"></p><hr><p><strong>Reference</strong>: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;On top of our discussion of Monte-Carlo and TD(0) algorithm, we now turn to consider the intermediate one between them.&lt;/p&gt;
&lt;h3 id=&quot;n-ste
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="Reinforcement-learnning" scheme="http://jaredlujr.github.io/tags/reinforcement-learnning/"/>
    
      <category term="Model-free" scheme="http://jaredlujr.github.io/tags/model-free/"/>
    
  </entry>
  
  <entry>
    <title>Coset and quotient set of a subgroup</title>
    <link href="http://jaredlujr.github.io/2020/04/10/coset/"/>
    <id>http://jaredlujr.github.io/2020/04/10/coset/</id>
    <published>2020-04-10T06:01:45.000Z</published>
    <updated>2020-04-11T08:12:13.245Z</updated>
    
    <content type="html"><![CDATA[<p>Given group $G$, an important method to study the property of such group is to make the partition into smaller unit. With the help of a subgroup $H$ of $G$, it may be realized.</p><p>In the following part, such partition induced by subgroup $H$ is equivalent to the equivalence relation induced by $H$, or the quotient set generated by $H$.</p><h3 id="Coset-and-Quotient-set"><a href="#Coset-and-Quotient-set" class="headerlink" title="Coset and Quotient set"></a>Coset and Quotient set</h3><p>We directly give the following definition:</p><p>$\overset{H}{\sim}$ is such an equivalence relation in $G$, which indicates that</p><script type="math/tex; mode=display">a\overset{H}{\sim} b, b\in G \Leftrightarrow ab^{-1}\in H</script><p>As far as we’ve concerned, any equivalence relation gives partition of a set: into <strong>several equivalence classes</strong>. Under $a \overset{H}{\sim} b$, the equivalence class of $a$ by $G$ is exactly $aH$, which is called the <strong>left coset</strong> of subgroup $H$. Distinct left cosets of $H$ finish the exhaust of $G$:</p><script type="math/tex; mode=display">G= \bigcup_{a\in L} aH</script><p>where $L$ is the set of generating elements belonging to different equivalence classes.</p><p>Then we denote the number of different left coset of $H$ as $[G:H]$, the index of $H$.</p><p>In fact, there is another equivalence relation as $(\overset{H}{\sim})’$, which will induce the <strong>right coset</strong> of $H$ in $G$. However, what’s interesting is that $[G:H]$ is the same under any of these two equivalence relations, and we have $L^{-1} = R$, where $R$ is the generating elements for right cosets.</p><p>Thus, the index $[G:H]$ is well-defined since it is now independent from the $\overset{H}{\sim}$.</p><p><strong>(Lagrange)</strong> Let $H\le G$, then</p><script type="math/tex; mode=display">|G|= |H|[G:H]</script><p>Let $L=\{a_1,a_2,\dots,a_n\}$, then we have obtained the quotient set as a collection of different equivalence classes:</p><script type="math/tex; mode=display">G/H = \{a_1H,\dots,a_nH\}</script><p>Then we will discuss the normal subgroup, which plays an important role in the quotient set partition.</p><p>If we want the quotient set holds the operation among each element(now they are the sets), we will have $aH=Ha$. In fact, we can prove that</p><p><strong>Theorem</strong> $H\triangleleft G \Leftrightarrow G/H $ is a group with operation in $G$.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Given group $G$, an important method to study the property of such group is to make the partition into smaller unit. With the help of a s
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Group-theory" scheme="http://jaredlujr.github.io/tags/group-theory/"/>
    
      <category term="Quotient-set" scheme="http://jaredlujr.github.io/tags/quotient-set/"/>
    
      <category term="Abstract-algebra" scheme="http://jaredlujr.github.io/tags/abstract-algebra/"/>
    
  </entry>
  
  <entry>
    <title>PAC assumption</title>
    <link href="http://jaredlujr.github.io/2020/04/08/pac-assumption/"/>
    <id>http://jaredlujr.github.io/2020/04/08/pac-assumption/</id>
    <published>2020-04-08T15:29:49.000Z</published>
    <updated>2020-04-09T00:28:45.710Z</updated>
    
    <content type="html"><![CDATA[<p>Probability approximately correct(PAC) assumption is a plausible but not enough general assumption which ignores the noises: <strong>the training data was drawn from the same distribution $D$</strong>.</p><p>Consider the setting of linear classification, a very simple case, one approach is to try to minimize the training error, and pick the “best”:</p><script type="math/tex; mode=display">\hat\theta = arg\min_\theta \hat\epsilon(h_\theta)</script><p>We call this process <strong>empirical risk minimization</strong>(ERM), which is regarded as the most “basic” learning algorithm. If abstacting away from the specific parameterization of hypotheses, instead, we consider the hypothesis class $H$, where all the classifiers lie. Foe example, in terms of neural networks, we could let $H$ be the set of all classifers <strong>representable</strong> by some neural network architecture. (which is an incredibly wide range of function domain)</p><p>In the case of finite hypothesis space $H$, we can derive the estimation of generalization error by applying <strong>the union bound</strong> and <strong>Chernoff bound</strong>, and give the result in a form including:</p><ul><li>the number of samples $m$</li><li>the confidence $1- \delta$</li><li>$k$, as the order of hypothesis space $H$</li><li>the discrepancy range of errors $\gamma$</li></ul><p>Focusing on the first term! It provides some insights about how many training examples we need to make an enough guarantee on the correctness of our model, in the following way:</p><script type="math/tex; mode=display">m \ge \frac{1}{2\gamma^2}log\frac{2k}{\delta}</script><p>The training set size $m$ that a certain given method or algorithm requires in order to achieve a certain level of performance is also called the <strong>algorithm’s sample complexity.</strong></p><p>We can also express the generalization error of the best hypothesis we can find in the following relative way:</p><script type="math/tex; mode=display">\epsilon(\hat h) \leq (min_{h\in H}\epsilon (h) + 2\gamma)</script><p>which means that the uniform convergence occurs with probability at least $1-\delta$. Thus, we bound the error in some ways.</p><p>As for the infinite $H$ case, we change the way of analysis and define the <strong>Vapnik-Chervonenkis dimension(VC dimension)</strong> of given hypothesis class $H$, written $VC(H)$. It is the size of the largest set that is <strong>shattered</strong> by $H$.</p><p>We say that $H$ shatters $S$ if $H$, i.e. considering all the $h\in H$, are able to realize any labeling on $S$. The discussion focuses on fixed arrangement of points in fixed amount $d$ but arbitrary labeling. This is to say, in order to prove that $VC(H)$ is at least $d$, we need to show only that there exists at least one specific set of size $d$ that $H$ can shatter.</p><p>For example, the linear hypotheses $h\in \mathbb R^2$ cannot shatter any four points on the plane, and in fact it cannot either shatter the three points in a line, but can work otherwise. Thus, the VC dimension is $3$.</p><p>With $VC(H)=d$ defined, we are equipped with a proper tool to describe the error bound, at least the order of it. Generally speaking, how the $\epsilon(\hat h)$ will change along with the number of examples $m$. It can be proved that such proportion stays linear.</p><p><strong>Theorem</strong> Given $H$, let $d=VC(H)$. Then with probability at least $1-\delta$, we have:</p><script type="math/tex; mode=display">\epsilon(\hat h) \leq \epsilon(h^\ast) + O\left( \sqrt{\frac{d}{m}log \frac{m}{d} + \frac{1}{m}log \frac{1}{\delta}}\right)</script><hr><p><strong>Reference</strong>:</p><p><a href="http://cs229.stanford.edu/notes/cs229-notes4.pdf" target="_blank" rel="noopener">Stanford CS229-note4</a> by Andrew Ng<br>Statistical Learning methods, by Li Hang (2012)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Probability approximately correct(PAC) assumption is a plausible but not enough general assumption which ignores the noises: &lt;strong&gt;the 
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="Learning-theory" scheme="http://jaredlujr.github.io/tags/learning-theory/"/>
    
      <category term="Error-estimation" scheme="http://jaredlujr.github.io/tags/error-estimation/"/>
    
  </entry>
  
  <entry>
    <title>Learning theory in Machine learning</title>
    <link href="http://jaredlujr.github.io/2020/04/08/learning-theory/"/>
    <id>http://jaredlujr.github.io/2020/04/08/learning-theory/</id>
    <published>2020-04-08T14:23:19.000Z</published>
    <updated>2020-04-14T14:16:06.553Z</updated>
    
    <content type="html"><![CDATA[<p>Learning theory is necessary when it comes to properly evaluate a specific learning algorithm. In Numerical Analysis, many evaluation methods and standards are established to help judge “how good” the algorithm or model is, such as errors, convergence speed, convergence domain and so on. Likewise, we eagerly want to know about the machine learning methods.</p><p>In terms of the goal in machine learning, most of them are classification. More generally speaking, we would like to learning something derived from a given dataset, say some amount of data points $T = \{(x^{(i)},y^{(i)})\}$. The Machine Learning is also called Statistical Learning, since most of the the tasks are partially or totally based on the data.</p><p>However, the data is sampled as the observation from human, which may contain some noises, or even worse, not strictly follow specific distribution. The topic in this article is to make an exposition: under some assumptions, how to evaluate a given model, even if we cannot accurately give what the errors are, but nevertheless restrict them within some boundaries.</p><h3 id="Trade-off-bias-or-variance"><a href="#Trade-off-bias-or-variance" class="headerlink" title="Trade-off: bias or variance"></a>Trade-off: bias or variance</h3><p>Like what happens in the classic interpolation and curve fitting scenario, there exists a trade-off to use simple or complex function(hypothesis) to generate the best “resonance” to the given data.</p><p>In machine learning, we define the generalization error of a hypothesis is its expected error on examples not necessarily in the training set.</p><p>We will call the errors resulted from over-simple hypothesis as <strong>bias</strong>; on the contrary, that of data over-fitting is called <strong>variance</strong>.</p><p>This is straightforward! Because the observed data source does not probably obey the probability distribution as far as we’ve concerned, so the emergence of error makes sense and inevitable; however, we can absolutely make the best fitting with no error at all by memorizing the data, say using higher-polynomial with tons of parameters. It is exactly what has happened in the curve fitting case: the fitting line loses any regularity but be extremely rigid.</p><h3 id="Empirical-error-and-real-error"><a href="#Empirical-error-and-real-error" class="headerlink" title="Empirical error and real error"></a>Empirical error and real error</h3><p>Actually, we have to agree on one thing that we can never know the “real” distribution of a given dataset (otherwise, there is no need to perform machine learning experiment!), so the only perception is how well the model works on such dataset.</p><h4 id="Empirical-risk-error"><a href="#Empirical-risk-error" class="headerlink" title="Empirical risk (error)"></a>Empirical risk (error)</h4><p>Empirical risk has a natural definition by simply counting the average misclassified samples:</p><script type="math/tex; mode=display">\hat\epsilon(h) = \frac{1}{m} \sum_{i=1}^m 1\{h(x^{(i)}) \neq y^{(i)}\}</script><p>However, what we really care is the following one: <strong>generalization error</strong>, which indicates the real performance on arbitrary data from real distribution.</p><script type="math/tex; mode=display">\epsilon(h) = P_{(x,y)\sim D} (h(x)\neq y)</script><p>If we draw a new examle $(x,y)$ from the distribution $D$, $h$ will misclassify it.</p><p>However, in order to give a clear insight about the generalization error, we need to talk about the hypotheses under the <strong>PAC</strong> assumptions.</p><h2 id="What-is-model-What-is-learning"><a href="#What-is-model-What-is-learning" class="headerlink" title="What is model? What is learning?"></a>What is model? What is learning?</h2><p>Let us re-consider the specific definition of <strong>model</strong>. A model to be trained is in nature the <strong>hypothesis space</strong> over which learning performs its search. For example, the linear decision boundary in 3-dimensional space.</p><p>The the <strong>model parameters</strong> come along with the space, now it is the “affiliated” vector, say numerical values or structure-determined variables that give rise to a definite hypothesis.</p><p>Then we use <strong>learning algorithm</strong> to perform the data-driven search over the hypothesis space, like an optimization process, but not the same. It is a computer-involved stuff, so we name it “algorithm”.</p><p>As for <strong>hyperparameters</strong>, they are the tunable aspects of the given model(some models may have none). Generally speaking, the learning algorithm will not update hyperparameters, and instead, they are up to users.</p><hr><p><strong>Reference</strong>:</p><p><a href="http://cs229.stanford.edu/notes/cs229-notes4.pdf" target="_blank" rel="noopener">Stanford CS229-note4</a> by Andrew Ng<br>Statistical Learning methods, by Li Hang (2012)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Learning theory is necessary when it comes to properly evaluate a specific learning algorithm. In Numerical Analysis, many evaluation met
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="Learning-theory" scheme="http://jaredlujr.github.io/tags/learning-theory/"/>
    
      <category term="Error-estimation" scheme="http://jaredlujr.github.io/tags/error-estimation/"/>
    
  </entry>
  
  <entry>
    <title>Recurrence Relation</title>
    <link href="http://jaredlujr.github.io/2020/04/07/recurrence-relation/"/>
    <id>http://jaredlujr.github.io/2020/04/07/recurrence-relation/</id>
    <published>2020-04-07T11:58:33.000Z</published>
    <updated>2020-04-07T14:35:13.164Z</updated>
    
    <content type="html"><![CDATA[<p>There are basically three ways to determine a sequence $\{a_n,n\ge 0\}$, explicitly or implicitly.</p><ul><li>Explicit expression as: $a_n = A(n)$</li><li>Generation function of $a_n$ as $A(x) = \sum_{n=0}^\infty a_n x^n$</li><li>Build the <strong>Recurrence Relation</strong>, which contains<ul><li>relation: $a_n = F(a_{n-1},a_{n-2},\dots, a_{n-r};n)$</li><li>initial condition: $a_1;a_2;\dots$</li></ul></li></ul><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>For example, the <strong>Hanoi</strong> is a typical problem of it, which has the following recurrence relation:</p><script type="math/tex; mode=display">a_n=2a_{n-1} +1, n \ge 2</script><script type="math/tex; mode=display">a_1 = 1</script><p>Iteratively solve the relation, we have explicit form as follow:</p><script type="math/tex; mode=display">a_n = 2a_{n-1} + 1 = 2(2a_{n-2}+1) +1 =\dots = 2^n -1</script><p><strong>Fibonacci</strong> and <strong>Catalan</strong> Sequence are also famous examples for recurrence. Especially we can solve the Fibonacci thoroughly:</p><h4 id="Fibonacci"><a href="#Fibonacci" class="headerlink" title="Fibonacci"></a>Fibonacci</h4><script type="math/tex; mode=display">f_n = f_{n-1} + f_{n-2}. f_0=f_1= 1</script><p>We can solve the general formula of $f_n$ by generation function.</p><p>Let $F(x)= \sum_{n\ge 0} f_n x^n$, then</p><script type="math/tex; mode=display">F(x) = f_0+f_1 x + \sum_{n\ge 2} f_nx^n = 1+ x+x(F(x)-1) + x^2F(x) =1+xF(x) + x^2F(x)</script><p>Thus,</p><script type="math/tex; mode=display">F(x) = \frac{1}{1-x-x^2} = \frac{1}{\sqrt5} (\frac{\alpha}{1-\alpha x} - \frac{\beta}{1-\beta x}) = \frac{1}{\sqrt5}(\alpha^{n+1}- \beta^{n+1}) x^n</script><p>where $\alpha={1\over2}(1+\sqrt5), \beta ={1\over 2}(1-\sqrt5)$, the eigenvalue of the recurrence relation.</p><p>The practical instance for Fibonacci is the number of $n-$sequence of $\{0,1\}$ with no adjacent $1$, like $1010001\dots$</p><h4 id="Catalan"><a href="#Catalan" class="headerlink" title="Catalan"></a>Catalan</h4><p>Catalan describes the possible associative combination of $n$-element string $x_1x_2\dots x_n$, where the associative law is not necessary to satisfy between $xy$.</p><p>The recurrence relation is graceful:</p><script type="math/tex; mode=display">c_n = c_0 c_{n-1} + c_1 c_{n-2} + \dots+ c_{n-1}c_0 = \sum_{j=0}^{n-1} c_j c_{n-1-j}; ~c_0 = c_1=1</script><h3 id="General-recurrence-relation"><a href="#General-recurrence-relation" class="headerlink" title="General recurrence relation"></a>General recurrence relation</h3><p>Here we briefly introduce several common recurrence relations, where each of them has corresponding general solution.</p><h4 id="1-First-order-Linear-recurrence-sequence-LRS"><a href="#1-First-order-Linear-recurrence-sequence-LRS" class="headerlink" title="1. First order Linear recurrence sequence(LRS)"></a>1. First order Linear recurrence sequence(LRS)</h4><script type="math/tex; mode=display">u_n=a(n) u_{n-1} + b(n) ,a(n)\neq 0, n\ge 0</script><p>By dividing $a(n)$, we instead study the recurrence relation of new sequence $\tilde{u_{n}}$ with constant coefficiencts.</p><h4 id="2-r-order-linear-recurrence-relation-with-constant-coefficients"><a href="#2-r-order-linear-recurrence-relation-with-constant-coefficients" class="headerlink" title="2. $r$-order linear recurrence relation with constant coefficients"></a>2. $r$-order linear recurrence relation with constant coefficients</h4><p>Just like what we do in the differential equation: homogeneous and non-homogeneous case to discuss.</p><script type="math/tex; mode=display">u_n = \sum_{j=1}^r c_j u_{n-j} + g(n)</script><p>where $g(n)$ is the non-homogeneous term, and $c_j$ is irrelevant to $n$.</p><p>For example, the Fibonacci $f_n = f_{n-1} + f_{n-2} $ is $2$-order homogeneous LRS.</p><p>For $r$-order homogeneous LRS, we have the general solution which is determined by solving eigenpolynomial and eigenvalue.</p><script type="math/tex; mode=display">u_n = \sum_{j=1}^r c_j u_{n-j}</script><script type="math/tex; mode=display">c(x ) = x^r - \sum_{j=1}^r c_j x^{r-j} = (x-\alpha_1)^{e_1}\dots (x-\alpha_s)^{e_s}</script><p>where $s$ is the number of distinct solution and $e_s$ is the corresponding multiplicity of $\alpha_s$. Also, $e_1+\dots+e_s=r$.</p><p>Then</p><script type="math/tex; mode=display">u_n = p_1(n) \alpha_1^n + \dots + p_s(n) \alpha_s^n, ~deg(p_i)<e_i</script><p>And the sumup number of coefficiencts in $p_i(n)$ is $r$ can be uniquely determined by initial value $u_0,u_1,\dots,u_{r-1}$.</p><p>For non-homogeneous case, we need to find a special solution that satisfies:</p><script type="math/tex; mode=display">u_n=u_n' + \sum_{i=1}^s p_i(n)\alpha_i^n</script><h3 id="Convolutional-recurrence-relation"><a href="#Convolutional-recurrence-relation" class="headerlink" title="Convolutional recurrence relation"></a>Convolutional recurrence relation</h3><p>We define that the sum $\sum_{j=0}^n u_j v_{n-j}$ is the convolution of vector $(u_0,\dots,u_n)^T,(v_0,\dots,v_n)^T$. And such recurrence relation is called Convolutional Form.</p><p>As mentioned above, the Catalan Sequence has the following form:</p><script type="math/tex; mode=display">c_n =  \sum_{j=0}^{n-1} c_j c_{n-1-j}; ~c_0 =c_1= 1</script><p>We can solve it by generating function and eigenfunction tricks, finally we have:</p><script type="math/tex; mode=display">c_n = \frac{1}{n+1} \binom{2n}{n}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;There are basically three ways to determine a sequence $\{a_n,n\ge 0\}$, explicitly or implicitly.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explicit expression as: $
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Combinatorics" scheme="http://jaredlujr.github.io/tags/combinatorics/"/>
    
      <category term="Sequence" scheme="http://jaredlujr.github.io/tags/sequence/"/>
    
      <category term="Recurrence" scheme="http://jaredlujr.github.io/tags/recurrence/"/>
    
  </entry>
  
  <entry>
    <title>Generating function of integer partition</title>
    <link href="http://jaredlujr.github.io/2020/04/07/integer-partition-gf/"/>
    <id>http://jaredlujr.github.io/2020/04/07/integer-partition-gf/</id>
    <published>2020-04-07T10:56:25.000Z</published>
    <updated>2020-04-07T12:19:16.641Z</updated>
    
    <content type="html"><![CDATA[<p>Partition of a positive number, is also an important counting objective in Combinatorics. As a combination form of counting, it means that the number of possible partitions of $n$. For example,</p><script type="math/tex; mode=display">4 = 3+1=2+2=2+1+1=1+1+1+1</script><p>Totally $4$ ways to take the partition, we denoted that $p(4) = 4$, where $p(n)$ is general form of integer partition.</p><h3 id="Common-restricted-integer-partitions"><a href="#Common-restricted-integer-partitions" class="headerlink" title="Common restricted integer partitions"></a>Common restricted integer partitions</h3><p>There are several restricted integer partition:</p><ul><li>$p(n,r)$: counting those with exactly $r$ parts; notice that $p(n) = \sum_k p(n,k)$</li><li>$n\in \mathbb N, H\subset \mathbb N \to p_H(n)$, where each part belong to $H$</li><li>In particular, $r,n\in \mathbb N \to p_{[r]}(n)$, where each part belong to $[r]$, say each part is less than or equal to $r$</li><li>$p_{\neq}(n)$ denotes the integer partition with distinct parts</li><li>$p_{odd}(n),p_{even}(n)…$</li></ul><h3 id="Ferrers-diagram"><a href="#Ferrers-diagram" class="headerlink" title="Ferrers diagram"></a>Ferrers diagram</h3><p>In fact, there is a convenient way to represent any partition straightforward, by Ferrers diagram:</p><div align=center><img src="ferrers.png" width="40%" height="40%"></div><p>which is a bijective image with $6+4+3+1$.</p><h3 id="Generating-Function"><a href="#Generating-Function" class="headerlink" title="Generating Function"></a>Generating Function</h3><p>Integer partition can also be analysed by generating Function, and any restricted partition can be correspondingly transformed to the variant function.</p><p>Generally, we write the generating function as follows:</p><script type="math/tex; mode=display">P(x) = \sum_{n=0}^\infty p(n)x^n = \prod_{j=1}^\infty (1-x^j)^{-1}</script><p><strong>Why? Let’s see</strong></p><p>Expand the $P(x)$:</p><script type="math/tex; mode=display">(1+x+x^2+\dots)(1+x^2+x^4+\dots)\dots = \prod_{j=1}^\infty (1-x^j)^{-1}</script><p>The coefficient of $x^n$ is contributed from each factor. Let $x_j$ be the number of parts containing $j$ elements, say $”1”$. Then we transform the known problem into the following Diophantus equation:</p><script type="math/tex; mode=display">x_1 + 2x_2 +\dots  = n</script><p>Then the result is clear.</p><p>Furthermore, we can add extra restriction by adjust the product form, take $p_H(n)$ for example:</p><script type="math/tex; mode=display">P_H(x) = \sum_{n=0}^\infty p_H(n) x^n = \prod_{j\in H} (1-x^j)^{-1}</script><h3 id="Inverse-generating-Function"><a href="#Inverse-generating-Function" class="headerlink" title="Inverse generating Function"></a>Inverse generating Function</h3><p>We define the inverse of $P(x)$ as $Q(x) =\sum_{n=0}^\infty q(n) x^n \equiv \prod_{j\in H} (1-x^j)$, such that $P(x)Q(x)=1$.</p><p>The $q(n)$ satisfies the following equation:</p><script type="math/tex; mode=display">q(n) = q_0(n) - q_1(n)</script><p>where $q_0(n)$ is the number of partitions of $n$ with distinct even parts while $q_1(n)$ is distinct odd one. Moreover, we have more direct expression of $q(n)$:</p><script type="math/tex; mode=display">q(n)=\begin{cases}(-1)^k, & \text{when } n=\frac{3k^2\pm k}{2} \\0 & \text{otherwise}\end{cases}</script><p>which can be proved by Ferrers diagram through discussion covering each cases.</p><h3 id="Euler’s-Identity"><a href="#Euler’s-Identity" class="headerlink" title="Euler’s Identity"></a>Euler’s Identity</h3><p>On top of the explicit expression of $q(n)$ above, we can immediately deduce the following formula:</p><h4 id="Theory-1"><a href="#Theory-1" class="headerlink" title="Theory-1"></a>Theory-1</h4><script type="math/tex; mode=display">\prod_{j=1}^\infty (1-x^j) = \sum_{n=0}^\infty q(n)x^n = 1+ \sum_{k=1}^\infty (-1)^k \left(x^{\frac{3k^2-k}{2}} + x^{\frac{3k^2+k}{2}}\right)</script><script type="math/tex; mode=display">= 1-x-x^2+x^5+x^7-x^{12}-x^{15}+ \dots</script><h4 id="Theory-2"><a href="#Theory-2" class="headerlink" title="Theory-2"></a>Theory-2</h4><p>Because the inverse property of $Q(x)$ by $P(x)Q(x) =1$, with plugged in, we have the further conclusion:</p><script type="math/tex; mode=display">p(n) = p(n-1) + p(n-1) - \dots  = \sum_{k=1}^\infty (-1)^{k-1} \left(p(n-\frac{3k^2-k}{2}) + p(n-\frac{3k^2+k}{2}) \right)</script><p>And we let $p(m)\equiv 0, \text{ when } m&lt;0$.</p><hr><p><strong>References:</strong> Wikipedia: <a href="https://en.wikipedia.org/wiki/Partition_(number_theory" target="_blank" rel="noopener">Partition</a>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Partition of a positive number, is also an important counting objective in Combinatorics. As a combination form of counting, it means tha
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Combinatorics" scheme="http://jaredlujr.github.io/tags/combinatorics/"/>
    
      <category term="Generating function" scheme="http://jaredlujr.github.io/tags/generating-function/"/>
    
      <category term="Partition" scheme="http://jaredlujr.github.io/tags/partition/"/>
    
  </entry>
  
  <entry>
    <title>Parity of permutation</title>
    <link href="http://jaredlujr.github.io/2020/04/06/parity-of-p/"/>
    <id>http://jaredlujr.github.io/2020/04/06/parity-of-p/</id>
    <published>2020-04-06T01:39:14.000Z</published>
    <updated>2020-04-06T02:10:54.683Z</updated>
    
    <content type="html"><![CDATA[<p>The subgroups of the symmetric group $S(\Omega)$ over $\Omega$ are called transformation group on $\Omega$. In particular, if $\Omega$ as finite set with $n$ elements, then the subgroups are called permutation groups. $\forall \sigma \in S_n$ forms an one-to-one on $\Omega$, which is also a <strong>permutation</strong>.</p><script type="math/tex; mode=display">\left(\begin{array}{ccc}1 & 2 & 3 & 4 & 5\\\sigma(1) & \sigma(2) & \sigma(3) & \sigma(4) & \sigma(5)\end{array}\right)</script><p>According to Carley, $\forall $ group $G$ is isomorphic with some transformation group; $\forall $ finite group $G$ is isomorphic with some permutation group. This implies the permutations exhibit great importance in the finite algebra structure.</p><h3 id="Property-of-permutation"><a href="#Property-of-permutation" class="headerlink" title="Property of permutation"></a>Property of permutation</h3><p>In fact, we have the following propsition or property of permutation.</p><ul><li>$\forall \sigma \in S_n$ has disjoint cyclic decomposition, which $\exist ~1$<ul><li>cycle can be seen as a special permutation only acting on some elements</li></ul></li><li>If two cycles share no common elements, then they can exchange (exchangable)</li><li>$\forall$ $k$-ary cycle can be further decomposed into product of $k-1$ transportation<ul><li>$2$-ary cycle is called transportation<script type="math/tex; mode=display">(a_1\dots,a_k) = (a_1a_k)(a_1a_{k-1})\dots(a_1a_2)</script></li></ul></li><li>Any transportation $(i,j)$ can be written as the product of<br>$2|j-i|-1$ adjacent transportations. For example:</li></ul><script type="math/tex; mode=display">(2~5) = (2~3)(3~4)(4~5)(4~3)(3~2)</script><h3 id="Parity-of-permutation"><a href="#Parity-of-permutation" class="headerlink" title="Parity of permutation"></a>Parity of permutation</h3><p>Actually, we would like to give a definition of the number of transportations after the decomposition of a permutation $\sigma$, due to the good properties of such discussion.</p><p>Like the collection of all even permutations constructs a subgroup of $S_n$, denoted as $A_n$ and called $n$-ary <strong>alternating group;</strong> and all the odd permutations form the coset of $A_n$. Also, $A_n$ is the normal subgroup of $S_n$ with $index~=2$.</p><p>And let $\sigma_1, \sigma_2$ be even permutation, $\tau_1, \tau_2$ be odd permutation, we have :</p><ul><li>$\sigma \tau$ is odd</li><li>$\sigma_1 \sigma_2$, $\tau_1 \tau_2$ is even</li><li>The parity of permutation is equivalent to the parity of the number of decomposed transportations</li></ul><p>However, we have to prove that such parity is “well-defined”, since the transportation-decomposition is definitely not unique. But we will show that the number of factors in such decomposition holds parity.</p><h3 id="Proof-that-the-parity-of-a-permutation-is-well-defined"><a href="#Proof-that-the-parity-of-a-permutation-is-well-defined" class="headerlink" title="Proof that the parity of a permutation is well-defined"></a>Proof that the parity of a permutation is well-defined</h3><p><em>(using polynomial)</em></p><p>First we define the parity of a permutation is a sign function as:</p><script type="math/tex; mode=display">sgn: S_n \to \{-1,1\}</script><p>where alternating group $A_n$, say the subgroup of all even permutation is the preimage of $+1$.</p><p>Then for $\forall \sigma \in S_n$, we define</p><script type="math/tex; mode=display">sgn(\sigma) = \frac{P(x_{\sigma(1)}, \dots, x_{\sigma(n)})}{P(x_1,\dots,x_n)}</script><p>where polynomial $P(x_1,\dots,x_n) = \prod_{i&lt;j}(x_i-x_j)$; for example, in the case $n=3$, we have $P(x_1,x_2,x_3) = (x_1-x_2)(x_1-x_3)(x_2-x_3)$.</p><p>Apparently there are both identical $\binom{n}{2}$ factors in the denominator and numerator, with <strong>sign</strong> as the only difference. Then we can show that such difference is exactly the sign $sgn(\sigma)$ we have already defined:</p><ul><li>The permutation $\sigma$ is equivalent to its transportation decomposition, which is not unique though.</li><li>Each transportation, as $2$-ary cycle, contribute $-1$ to the whole product, since $(x_i-x_j) = (-1)(x_j-x_i)$</li><li>We see that if $\sigma,r$ are two permutation, then:</li></ul><script type="math/tex; mode=display">sgn(\sigma ~ \tau) = \frac{P(x_{\sigma(\tau(1))}, \dots, x_{\sigma(\tau(n))})}{P(x_1,\dots,x_n)} = \frac{P(x_{\tau(1)}, \dots, x_{\tau(n)})}{P(x_1,\dots,x_n)} \cdot \frac{P(x_{\sigma(\tau(1))}, \dots, x_{\sigma(\tau(n))})}{P(x_{\tau(1)},\dots,x_{\tau(n)})} = sgn(\sigma) sgn(\tau)</script><ul><li>Thus, $sgn(\sigma)$ is the same mapping as we defined earlier. And the parity of $\sigma$ is definite because they only have one value and are indeed well-defined.</li></ul><hr><p><strong>Reference</strong><br>Wikipedia: <a href="https://en.wikipedia.org/wiki/Parity_of_a_permutation" target="_blank" rel="noopener">Parity of a permutation</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The subgroups of the symmetric group $S(\Omega)$ over $\Omega$ are called transformation group on $\Omega$. In particular, if $\Omega$ as
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Group-theory" scheme="http://jaredlujr.github.io/tags/group-theory/"/>
    
      <category term="Abstract-algebra" scheme="http://jaredlujr.github.io/tags/abstract-algebra/"/>
    
      <category term="Permutation" scheme="http://jaredlujr.github.io/tags/permutation/"/>
    
  </entry>
  
  <entry>
    <title>State Pattern by OOP in Python3</title>
    <link href="http://jaredlujr.github.io/2020/04/04/state-pattern/"/>
    <id>http://jaredlujr.github.io/2020/04/04/state-pattern/</id>
    <published>2020-04-04T02:27:13.000Z</published>
    <updated>2020-04-04T02:47:17.643Z</updated>
    
    <content type="html"><![CDATA[<p>State pattern, or state machine is one of the patterns in <strong>Design pattern</strong>. It is proposed to aviod redundancy of code, if tons of if-else statements are needed, which makes it extremely hard to maintain and read the code.</p><p>Here is the solution, by defining the conditions as abstract states, by means of python-class implemented in Python3.</p><h2 id="State-Pattern"><a href="#State-Pattern" class="headerlink" title="State Pattern"></a>State Pattern</h2><p>Goal: To handle different states. Create a state machine which is able to perform corresponding operations given state. If-else free.</p><p>In this designment, each state instance(object) has only static methods without storing any attribute data.</p><h3 id="Primal-class-file-I-O"><a href="#Primal-class-file-I-O" class="headerlink" title="Primal class: file I/O"></a>Primal class: file I/O</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connection</span>:</span></span><br><span class="line">    <span class="comment"># if-else piled up, with states as "open" or "close"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.state = <span class="string">'CLOSED'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state != <span class="string">'OPEN'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line">        print(<span class="string">'reading'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state != <span class="string">'OPEN'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line">        print(<span class="string">'writing'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'OPEN'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Already open'</span>)</span><br><span class="line">        self.state = <span class="string">'OPEN'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'CLOSED'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Already closed'</span>)</span><br><span class="line">        self.state = <span class="string">'CLOSED'</span></span><br></pre></td></tr></table></figure><h3 id="Better-solution"><a href="#Better-solution" class="headerlink" title="Better solution"></a>Better solution</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connection1</span>:</span></span><br><span class="line">    <span class="comment"># define a class for each state</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.new_state(ClosedConnectionState)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_state</span><span class="params">(self, newstate)</span>:</span></span><br><span class="line">        self._state = newstate</span><br><span class="line">        <span class="comment"># Delegate to the state class</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.read(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.write(self, data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.open(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.close(self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection state base class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConnectionState</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(conn, data)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Implementation of different states</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClosedConnectionState</span><span class="params">(ConnectionState)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(conn, data)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(conn)</span>:</span></span><br><span class="line">        conn.new_state(OpenConnectionState)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Already closed'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OpenConnectionState</span><span class="params">(ConnectionState)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(conn)</span>:</span></span><br><span class="line">        print(<span class="string">'reading'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(conn, data)</span>:</span></span><br><span class="line">        print(<span class="string">'writing'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Already open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(conn)</span>:</span></span><br><span class="line">        conn.new_state(ClosedConnectionState)</span><br></pre></td></tr></table></figure><p>But what is “staticmethod”?</p><p>Here is the document in Python3.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class staticmethod(object)</span><br><span class="line"> |  staticmethod(function) -&gt; method</span><br><span class="line"> |</span><br><span class="line"> |  Convert a function to be a static method.</span><br><span class="line"> |</span><br><span class="line"> |  A static method does not receive an implicit first argument.</span><br><span class="line"> |  To declare a static method, use this idiom:</span><br><span class="line"> |</span><br><span class="line"> |       class C:</span><br><span class="line"> |           @staticmethod</span><br><span class="line"> |           def f(arg1, arg2, ...):</span><br><span class="line"> |               ...</span><br><span class="line"> |</span><br><span class="line"> |  It can be called either on the class (e.g. C.f()) or on an instance</span><br><span class="line"> |  (e.g. C().f()).  The instance is ignored except for its class.</span><br><span class="line"> |</span><br><span class="line"> |  Static methods in Python are similar to those found in Java or C++.</span><br><span class="line"> |  For a more advanced concept, see the classmethod builtin.</span><br></pre></td></tr></table></figure><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = Connection()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c._state</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">ClosedConnectionState</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="class"><span class="title">Traceback</span> <span class="params">(most recent call last)</span>:</span></span><br><span class="line">    File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    File <span class="string">"example.py"</span>, line <span class="number">10</span>, <span class="keyword">in</span> read</span><br><span class="line">        <span class="keyword">return</span> self._state.read(self)</span><br><span class="line">    File <span class="string">"example.py"</span>, line <span class="number">43</span>, <span class="keyword">in</span> read</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line">RuntimeError: Not open</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.open()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c._state</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">OpenConnectionState</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="class"><span class="title">reading</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">write</span><span class="params">(<span class="string">'hello'</span>)</span></span></span><br><span class="line"><span class="class"><span class="title">writing</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">close</span><span class="params">()</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">_state</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">__main__</span>.<span class="title">ClosedConnectionState</span>'&gt;</span></span><br></pre></td></tr></table></figure><hr><p><strong>Reference</strong>: <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener">_David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition_</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;State pattern, or state machine is one of the patterns in &lt;strong&gt;Design pattern&lt;/strong&gt;. It is proposed to aviod redundancy of code, if
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Python3" scheme="http://jaredlujr.github.io/tags/python3/"/>
    
      <category term="OOP" scheme="http://jaredlujr.github.io/tags/oop/"/>
    
      <category term="Design-pattern" scheme="http://jaredlujr.github.io/tags/design-pattern/"/>
    
  </entry>
  
  <entry>
    <title>Monte-Carlo and Temporal-Difference Learning</title>
    <link href="http://jaredlujr.github.io/2020/04/03/mctd-evaluation/"/>
    <id>http://jaredlujr.github.io/2020/04/03/mctd-evaluation/</id>
    <published>2020-04-03T11:45:10.000Z</published>
    <updated>2020-04-10T07:16:52.459Z</updated>
    
    <content type="html"><![CDATA[<p>Model-free prediction (evaluation) is to estimate the value function of an unknown MDP, say $\langle S,A,P,R,\gamma \rangle$. And it includes the Monte-Carlo learning and temporal-difference learning methods.</p><h2 id="Monte-Carlo-Reinforcement-learning"><a href="#Monte-Carlo-Reinforcement-learning" class="headerlink" title="Monte-Carlo Reinforcement learning"></a>Monte-Carlo Reinforcement learning</h2><p>As a famous stochastic simulation method, MC methods plays an important role in a wide range of scientific problems. In reinforcement learning, MC-based methods learn directly from episodes of experience, say a whole observing trajectory.</p><ul><li>Model-free: have no prior knowledge of MDP transitions/ rewards.</li><li>Value = average <strong>sample</strong> returns<ul><li>i.e., use arithmetic mean to replace the real expectation</li></ul></li><li>Requirement of applying MC: all episodes <strong>must terminate</strong></li></ul><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><ul><li><p>Goal: learn latent(real) value function $v_\pi$ <strong>from episodes</strong> of experience under policy $\pi$</p></li><li><p>Originally, the return at time $t$(also, state $s$) is the total discounted $\gamma$ rewards:</p><ul><li>$G_t = R_{t+1} + \dots + \gamma^{T-t-1}R_T$</li></ul></li><li><p>And corresponding value function is the <strong>expected return</strong>:</p></li></ul><script type="math/tex; mode=display">v_\pi(s) = \mathbb{E}_\pi [G_t|S_t=s]</script><h3 id="Algorithm-First-visit-MC-policy-evaluation"><a href="#Algorithm-First-visit-MC-policy-evaluation" class="headerlink" title="Algorithm: First-visit MC policy evaluation"></a>Algorithm: First-visit MC policy evaluation</h3><p>The MC algorithm for prediction is to evaluate each state $s$ given policy $\pi$. Here, the first-visit MC is introduced, which means we only take account of the <strong>first time-step $t$</strong> that state $s$ is visit in some episode. We simply count the appearing times with its return $S(s)$, where we compute the return(adding dicounted rewards along the trajectory). Then estimate the value by $V(s) = S(s)/N(s)$. By law of large numbers,</p><script type="math/tex; mode=display">V(s) \to v_\pi(s) ~ as ~ N(s) \to \infty</script><p><img src="fvmc.png" alt="alt fvmc"></p><h3 id="Every-visit-Monte-Carlo-Policy-Evaluation"><a href="#Every-visit-Monte-Carlo-Policy-Evaluation" class="headerlink" title="Every-visit Monte-Carlo Policy Evaluation"></a>Every-visit Monte-Carlo Policy Evaluation</h3><p>Another way to consider is that every time-step t that state s is visited in an episode. We will also obtain the results.</p><h3 id="Incremental-Mean"><a href="#Incremental-Mean" class="headerlink" title="Incremental Mean"></a>Incremental Mean</h3><p>If we rewrite the mean as :</p><script type="math/tex; mode=display">\mu_k = {1\over k}\sum_{j=1}^k x_j = \mu_{k-1} + {1\over k}(x_k - \mu_{k-1})</script><p>Then the update form can be changed into:</p><p>$N(S_t) := N(S_t) + 1$</p><p>$V(S_t) := V(S_t) + {1\over N(S_t)}(G_t - V(S_t))$</p><p>which is the increment Monte-Carlo Updates.</p><p>In non-stationary problems, where episodes may not provide “consistent” information, it can be useful to track a runing mean, i.e. forget old episodes.</p><p>$V(S_t) := V(S_t) + \alpha (G_t - V(S_t))$</p><h2 id="Temporal-Difference-learning"><a href="#Temporal-Difference-learning" class="headerlink" title="Temporal-Difference learning"></a>Temporal-Difference learning</h2><ul><li>Model-free also</li><li>Learning from incomplete episodes, by bootstrapping(update involving estimate)</li><li>Updates a guess towards a guess!</li></ul><h3 id="The-difference-between-TD-and-MC"><a href="#The-difference-between-TD-and-MC" class="headerlink" title="The difference between TD and MC"></a>The difference between TD and MC</h3><p>Take Incremental every-visit MC for example,</p><p>We update value $V(S_t)$ toward actual return $G_t$ as :</p><script type="math/tex; mode=display">V(S_t) := V(S_t) + \alpha (G_t - V(S_t))</script><p>In $TD(0)$ — simplest TD algorithm, we replace the $G_t$ with a guess:</p><script type="math/tex; mode=display">V(S_t) := V(S_t) + \alpha (R_{t+1}+\gamma V(S_{t+1}) - V(S_t))</script><p>where $R_{t+1}+\gamma V(S_{t+1})$ is called the <strong>TD target</strong>; $R_{t+1}+\gamma V(S_{t+1}) - V(S_t)$ is called the <strong>TD error</strong>.</p><p>And the rest of algorithm is similar.</p><h3 id="Algorithm-TD-0-for-estimating-v-pi"><a href="#Algorithm-TD-0-for-estimating-v-pi" class="headerlink" title="Algorithm: TD(0) for estimating $v_\pi$"></a>Algorithm: TD(0) for estimating $v_\pi$</h3><p><img src="td.png" alt="alt td"></p><h3 id="Remarks-of-TD"><a href="#Remarks-of-TD" class="headerlink" title="Remarks of TD"></a>Remarks of TD</h3><ul><li>return $G_t = R_{t+1} + \dots + \gamma^{T-t-1}R_T$ is <strong>unbiased estimate</strong> of $v_\pi(S_t)$</li><li>true TD target (over $v_\pi$), $R_{t+1}+\gamma v_\pi(S_{t+1}$ is also unbiased estimate of $v_\pi(S_t)$</li><li>whileTD target (over $v_\pi$), $R_{t+1}+\gamma v_\pi(S_{t+1}$ is also biased estimate</li><li>TD target has much lower variance than the return</li><li>however, TD target estimate has bias;<ul><li>MC has better convergence properties</li></ul></li></ul><p>Note that:</p><ul><li>MC is not very sensitive to initial value</li><li>TD is more <strong>sensitive</strong></li></ul><h2 id="Batch-MC-and-TD"><a href="#Batch-MC-and-TD" class="headerlink" title="Batch MC and TD"></a>Batch MC and TD</h2><p>If we only have finite observation data, say $K$ episodes totally:</p><script type="math/tex; mode=display">s_1^1, a_1^1, r_2^1 \dots, s_{T_1}^1</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">s_1^K, a_1^K, r_2^K \dots, s_{T_1}^K</script><p>Then we can <strong>repeatedly sample episode</strong> $k\in [1,K]$ and apply MC or TD(0) to episode k. It still works good!</p><p>For certainty equivalence, we can prove that the batch MC or TD:</p><ol><li>MC converges to solution with <strong>minimum mean-squared error</strong>(MSE), and best fit to the <strong>observed returns</strong>:</li></ol><script type="math/tex; mode=display">\sum_{k=1}^K \sum_{t=1}^{T_k} (G_t^k - V(s_t^k))^2</script><ol><li>TD converges to solution of <strong>maximum likelihood Markov model</strong>, and solution to the MDP model $\langle S,A,P,R,\gamma \rangle$ best fits the observed data, say sequence.( common MLE )</li></ol><script type="math/tex; mode=display">P^a_{s,s'} = {1\over N(s,a)} \sum_{k=1}^K \sum_{t=1}^{T_k} 1 \{s_t^k,a_t^k,s_{t+1}^k = s,a,s'\}</script><script type="math/tex; mode=display">R_s^a = {1\over N(s,a)}\sum_{k=1}^K \sum_{t=1}^{T_k} 1\{s_t^k,a_t^k = s,a\} r_t^k</script><h2 id="Summary-and-comparison"><a href="#Summary-and-comparison" class="headerlink" title="Summary and comparison"></a>Summary and comparison</h2><p>Generally speaking, TD <strong>exploits Markov property</strong> (one-step determination) and usually more <strong>efficient in Markov env</strong>; while MC does not exploit and is usually used in non-Markov env.</p><h3 id="Diagram-explanation"><a href="#Diagram-explanation" class="headerlink" title="Diagram explanation"></a>Diagram explanation</h3><ul><li>For Monte-Carlo: $V(S_t) := V(S_t) + \alpha (G_t - V(S_t))$</li></ul><p><img src="mctree.png" alt="alt mctree"></p><ul><li>For Temporal-Difference: $V(S_t) := V(S_t) + \alpha (R_{t+1}+\gamma V(S_{t+1}) - V(S_t))$</li></ul><p><img src="tdtree.png" alt="alt tdtree"></p><ul><li>Also, here is how Dynamic Programming algorithm works:</li></ul><p><img src="dptree.png" alt="alt dptree"></p><hr><p><strong>Reference</strong>: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Model-free prediction (evaluation) is to estimate the value function of an unknown MDP, say $\langle S,A,P,R,\gamma \rangle$. And it incl
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="Reinforcement-learning" scheme="http://jaredlujr.github.io/tags/reinforcement-learning/"/>
    
      <category term="Monte-Carlo" scheme="http://jaredlujr.github.io/tags/monte-carlo/"/>
    
  </entry>
  
  <entry>
    <title>Generating Function</title>
    <link href="http://jaredlujr.github.io/2020/04/01/genefunction/"/>
    <id>http://jaredlujr.github.io/2020/04/01/genefunction/</id>
    <published>2020-04-01T11:37:09.000Z</published>
    <updated>2020-04-07T12:18:52.028Z</updated>
    
    <content type="html"><![CDATA[<p>A series of numbers shared similar features, can often be described as coefficients of some function expansion. Then we will call such function the <strong>generating function</strong>.</p><h2 id="generating-Function"><a href="#generating-Function" class="headerlink" title="generating Function"></a>generating Function</h2><p>In order to obtain a sequence ${a_k: k \geq 0}$, we can represent it by a power series:</p><script type="math/tex; mode=display">g(x) = \sum_{i=0}^\infty a_k x^k = a_0 + a_1 x + a_2 x^2 + \dots</script><p>If such representation holds, then the analysis of such sequence( often only for specific term of such sequence ) can now be changed into the analysis of its generating function.</p><h3 id="Straightforward-example"><a href="#Straightforward-example" class="headerlink" title="Straightforward example"></a>Straightforward example</h3><ol><li>Given $n$, the coefficients of binomial have the form:</li></ol><script type="math/tex; mode=display">\sum_{i=0}^\infty \binom{n}{k} x^k = (1+x)^n</script><ol><li>$k$-ary repeatable combination over $n$-ary set, i.e., to select $k$ elements from $n$ different ones when repetition is allowed; or the number of non-negative integer solutions to equation $x_1+\dots + x_n = k$:</li></ol><script type="math/tex; mode=display">\left( \binom{n}{k}\right) = \binom{n+k-1}{k}</script><script type="math/tex; mode=display">g(x) = (1-x)^{-n} =\sum_{i=0}^\infty \left( \binom{n}{k}\right) x^k</script><p>Hint: consider the generating process of term $x^k$ and what the number $\left( \binom{n}{k}\right) = \binom{n+k-1}{k}$ describes.</p><ol><li>Constrained diophantine equation</li></ol><p>Similar to $case 2$, write the expansion series as polynomial or incomplete series.</p><p>In general, to select $k$ elements from $n$-ary set where the number of element $a_i$ is constrained as set $M_i$. The number of such combinations is denoted as $c_k$, then its generating function is:</p><script type="math/tex; mode=display">g(x) = (1-x)^{-n} =\sum_{i=0}^\infty c_k x^k = \prod_{i=1}^n \left( \sum_{m\in M_i} x^m \right)</script><p>By detailedly observing the form of the R.H.S. above, it makes total sense as an enumeration.</p><p>Many cases from this pattern can construct one-to-one with a lot of counting problems.</p><h3 id="Exponential-generating-function"><a href="#Exponential-generating-function" class="headerlink" title="Exponential generating function"></a>Exponential generating function</h3><p>There is a useful type of generating function as exponential generating function. The only difference with general form above is the factorial denominator $k!$.</p><p>Exponential generating function is introduced to solve <strong>permutation counting problem.</strong> It has the following general form:</p><script type="math/tex; mode=display">\sum_{i=0}^n a_k \frac{x^k}{k!} = a_0 + a_1 x + a_2 x^2 + \dots</script><ol><li>The $k$-ary unrepeatable permutations of $n$-ary set is $(n)_k$, and</li></ol><script type="math/tex; mode=display">\sum_{i=0}^n (n)_k \frac{x^k}{k!} = (1+x)^n</script><ol><li>common Exp generatingf function</li></ol><ul><li>$\{a_k=1, k \geq 0\}$ —- $e^x$;</li><li>$\{a_k=(-1)^k, k \geq 0\}$ —- $e^{-x}$;</li><li>$\{a_k={k!}, k \geq 0\}$ —- $1\over 1-x$;</li></ul><ol><li>To make $k$-ary permutation from $n$-ary set $S=\{ b_1,b_2,\dots, b_n\}$ where the number of element $b_i$ is constrained as set $M_i, m_i\in M_i$. The number of such combinations is denoted as $p_k$, then its generating function is:</li></ol><script type="math/tex; mode=display">\sum_{i=0}^\infty p_k {x^k\over k!} = \prod_{i=1}^n \left( \sum_{m\in M_i} {x^m\over m!} \right)</script><p>where the coefficient of $x_k\over k!$ is</p><script type="math/tex; mode=display">\sum_{m1+\dots+m_n=k, m_i \in M_i} \frac{k!}{m_1!\dots m_n!}</script><p>In particular, if $M_i = \mathbb{N_0}$, then the exponential generating function is $(\sum_{j=0}^\infty \frac{x^j}{j!})^n = e^{nx}$, which is equivalent to that of $n^k$:</p><script type="math/tex; mode=display">\sum_{i=0}^\infty n^k {x^k\over k!}</script><h3 id="Allocation-problem"><a href="#Allocation-problem" class="headerlink" title="Allocation problem"></a>Allocation problem</h3><p>In fact, there exists an one-to-one between permutation and allocation: $k$-ary repeatable permutation is equivalent to the allocation of $k$ distinguishable balls into $n$ distinguishable boxes. We may also add some constraints to the two primal problems to make it more complex.</p><h3 id="Exp-generating-function-of-Stirling-numbers"><a href="#Exp-generating-function-of-Stirling-numbers" class="headerlink" title="Exp. generating function of Stirling numbers"></a>Exp. generating function of Stirling numbers</h3><p><strong>Prop1</strong><br>The exponential generating function of Stirling numbers of 2nd kind:</p><script type="math/tex; mode=display">\sum_{n=0}^\infty S(n,k) \frac{x^n}{n!} = {1\over k!}(e^x -1 )^k</script><p><strong>Corollary1</strong></p><script type="math/tex; mode=display">S(n,k) = {1\over k!} \sum_{i=0}^k (-1)^i \binom{k}{i} (k-i)^n</script><p>Correspondingly, we have the form for 1st kind Stirling.</p><p><strong>Prop2</strong><br>The exponential generating function of Stirling numbers of 2nd kind:</p><script type="math/tex; mode=display">\sum_{n=0}^\infty s(n,k) \frac{x^n}{n!} = {1\over k!}(ln(x+1) )^k</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;A series of numbers shared similar features, can often be described as coefficients of some function expansion. Then we will call such fu
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Combinatorics" scheme="http://jaredlujr.github.io/tags/combinatorics/"/>
    
      <category term="Generating function" scheme="http://jaredlujr.github.io/tags/generating-function/"/>
    
      <category term="Counting" scheme="http://jaredlujr.github.io/tags/counting/"/>
    
  </entry>
  
  <entry>
    <title>Note of SVM-(4)</title>
    <link href="http://jaredlujr.github.io/2020/03/31/svm4/"/>
    <id>http://jaredlujr.github.io/2020/03/31/svm4/</id>
    <published>2020-03-31T13:05:37.000Z</published>
    <updated>2020-04-01T13:59:50.442Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Soft-Margin-non-separable-dataset"><a href="#Soft-Margin-non-separable-dataset" class="headerlink" title="Soft Margin: non-separable dataset"></a>Soft Margin: non-separable dataset</h2><p>SVM works with a prerequisite that the given dataset is linearly separable. Even the kernel trick provides an incredible approach to help increase the likelihood that that data is separable, but not always does. Generally, a speck of noise samples, say outliers, will fail the SVM due to its hard requirement $y^{(i)}(w^T x^{(i)} + b) \geq 1$. In order to improve this, we introduce the regularized (slacking) term $\xi_i$, such that the problem has changed into:</p><script type="math/tex; mode=display">\min_{w,b} {1\over 2} ||w||^2 + C \sum_{i=1}^m \xi_i</script><script type="math/tex; mode=display">s.t. \; y^{(i)} (w^Tx^{(i)} + b ) \geq 1- \xi_i,</script><script type="math/tex; mode=display">\xi_i\geq 0,  i =1,2,\dots,m</script><p>On top of that, examples are now permitted to have (functional) margin less than $1$. However, the “permission” will add extra penalty to the objective function by $C\xi_i$, which is a trade-off.</p><p>We can form the Lagrangian:</p><script type="math/tex; mode=display">L(w,b,\xi,\alpha,r) = {1\over 2} ||w||^2 + {C\over 2} \sum_{i=1}^m \xi_i^2 - \sum_{i=1}^m \alpha_i [y^{(i)}(w^T x^{(i)} + b) - 1 + \xi_i] -\sum_{i=1}^m r_i \xi_i</script><p>where $\alpha_i, r_i$ are the Lagrange multipliers.</p><p>The dual form of the problem:</p><script type="math/tex; mode=display">\max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i  - {1\over 2} \sum_{i=1}^{m} \sum_{j=1}^m \alpha_i \alpha_j y_i y_j K(x_i, x_j)</script><script type="math/tex; mode=display">s.t. \; 0 \leq \alpha_i \leq C, i=1,\dots,m</script><script type="math/tex; mode=display">\sum_{i=1}^m \alpha_iy^{(i)} = 0</script><p>By solving which, we can also obtain the optimal classifier like what we do in the linear separable scenario.</p><p><strong>Remarks</strong>:</p><ul><li>$C$, as the penalty factors, is a hyperparameter in SVM. It indicate the measure of the penalty of wrongly classified samples;</li><li>When $C$ increase, the contribution from wrongly classified samples will flood the primal objective, thus “caring the noises” too much, which is inclined to overfit.<ul><li>If $C$ goes to $\infty$, then it recovers to the hard margin: now we build the hyperplane <strong>along the contour of the “real” boundary</strong>.</li><li>If $C$ approaches $0$, it confines the $\alpha_i$ too hard around $0$ and ignores the outliers.</li></ul></li><li>Now the minimize of objective include:<ul><li>maximize the margin (w.r.t $w$)</li><li>minimize the number of wrongly classified points</li></ul></li></ul><h2 id="SMO-algorithm"><a href="#SMO-algorithm" class="headerlink" title="SMO algorithm"></a>SMO algorithm</h2><h3 id="Coordinate-ascent"><a href="#Coordinate-ascent" class="headerlink" title="Coordinate ascent"></a>Coordinate ascent</h3><p>Introduce by John Platt, SMO (sequential minimal optimization) algorithm gives an efficient way of solving the dual problem. It mainly focuses on how we utilize the samples to update the parameters.</p><p>Consider the following unconstrained optimization problem:</p><script type="math/tex; mode=display">\max_\alpha W(\alpha_1,\dots,\alpha_m)</script><p>The SMO algorithm uses so-called <strong>coordinate ascent</strong> to finish the update:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Loop until convergence:&#123;</span><br><span class="line">  For i &#x3D; 1,...,m&#123;</span><br><span class="line">    ai :&#x3D; argmax_ai W(a1,...,ai-1,a^i,ai+1,...,am)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In the inner loop, we hold all the variables fixed except for some $\alpha_i$, and re-optimize $W$ w.r.t just the parameters $\alpha_i$.</p><p><img src="ca.png" alt="alt ca"></p><h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>Coordinate ascent gives a powerful tools for update the parameters, but it fails for most constrained problems like our dual optimization problems due to the equality.</p><script type="math/tex; mode=display">\max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i  - {1\over 2} \sum_{i=1}^{m} \sum_{j=1}^m \alpha_i \alpha_j y_i y_j K(x_i, x_j)</script><script type="math/tex; mode=display">s.t. \; 0 \leq \alpha_i \leq C, i=1,\dots,m</script><script type="math/tex; mode=display">\sum_{i=1}^m \alpha_iy^{(i)} = 0</script><p>However, by altering a bit, we can let it work greatly on the SVM problem. In order not to violate the condition $\sum_{i=1}^m \alpha_iy^{(i)} = 0$, we may update two parameters at the same time:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Repeat till convergence &#123;</span><br><span class="line">  1. Select some pair ai and aj to update next (using a</span><br><span class="line">     heuristic that tries to pick the two that will allow us to</span><br><span class="line">      make the biggest progress towards the global maximum).</span><br><span class="line">  2. Reoptimize W(a) with respect to ai and aj, while holding</span><br><span class="line">   all the other ak’s (k ̸&#x3D; i, j) fixed.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compared to the coordinate ascent, we instead hold $\alpha_3,\dots,\alpha_m$ fixed and re-optimize $W$ w.r.t $\alpha_1, \alpha_2$:</p><script type="math/tex; mode=display">\alpha_1 y_1+\alpha_2 y_2 = \zeta</script><script type="math/tex; mode=display">0 \leq \alpha_1 \leq C</script><script type="math/tex; mode=display">0 \leq \alpha_2 \leq C</script><h3 id="Hinge-loss-function"><a href="#Hinge-loss-function" class="headerlink" title="Hinge loss function"></a>Hinge loss function</h3><p>Let’s look back to the primal optimization problem. We can interpret the linear SVM in another way, to minimize the following (loss) objective funtion:</p><script type="math/tex; mode=display">\sum_{i=1}^m [1- y^{(i)}(w^T x^{(i)} + b )]_+ \lambda ||w||^2</script><p>where $[z]_+$ is called hinge loss function, which is defined as:</p><script type="math/tex; mode=display">[z]_+=\begin{cases}z, & \text{ z > 0}\\0 & \text{otherwise}\end{cases}</script><p>This is to say, if given sample is correctly classified and has margin larger than $1$, the loss will be zero; on the contrary, it will be $1- y^{(i)}(w^T x^{(i)} + b)$.</p><p>The second term $\lambda ||w||^2$ is the $l_2$ regularized term.</p><p><img src="lossf.png" alt="alt lossf"></p><p>From the figure above, we can see that hinge loss function is one of the continuous upper bound of $0-1$ loss. The logistic loss function decreases after the $1$, which means giving regard to <strong>every samples</strong> even they are far away from the decision boundary.</p><p>$0-1$ loss function in fact exhibits the “real loss” but its uncontinuity gives rise to the difficulty for update.</p><hr><p><strong>Reference</strong>:<br><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener">Stanford CS229-note3</a> by Andrew Ng<br>Statistical Learning methods, by Li Hang (2012)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Soft-Margin-non-separable-dataset&quot;&gt;&lt;a href=&quot;#Soft-Margin-non-separable-dataset&quot; class=&quot;headerlink&quot; title=&quot;Soft Margin: non-separable
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="Support-vector-machine" scheme="http://jaredlujr.github.io/tags/support-vector-machine/"/>
    
      <category term="SVM" scheme="http://jaredlujr.github.io/tags/svm/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Subgroup</title>
    <link href="http://jaredlujr.github.io/2020/03/30/subgroup/"/>
    <id>http://jaredlujr.github.io/2020/03/30/subgroup/</id>
    <published>2020-03-30T01:41:30.000Z</published>
    <updated>2020-03-30T04:10:11.449Z</updated>
    
    <content type="html"><![CDATA[<p>An important concept in the group theory is the <strong>Subgroup</strong>. As part of a given group, its subgroup holds the algebra structure and has a close relation to the original group.</p><h2 id="Introduction-to-subgroup-with-relevant-concepts"><a href="#Introduction-to-subgroup-with-relevant-concepts" class="headerlink" title="Introduction to subgroup with relevant concepts"></a>Introduction to subgroup with relevant concepts</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Let $(G,~\cdot)$ be a group and $H$ is a non-empty subset of it. If the $\cdot$ is closed operation in $H$, then $(H,~\cdot)$ also forms a group, and called subgroup of $G$. We denote this relation as $H \leq G$.</p><p>Remarks:</p><ul><li>Group $G$ always conceives subgroups<ul><li>at least $G$ itself and $\{e\}$, which are called <strong>trivial subgroup</strong> of $G$</li><li>except for $G$, any subgroup of $G$ is called proper subgroup(like proper set)</li></ul></li></ul><h3 id="Propsition"><a href="#Propsition" class="headerlink" title="Propsition"></a>Propsition</h3><h4 id="Shared-inverse-and-identity-element"><a href="#Shared-inverse-and-identity-element" class="headerlink" title="Shared inverse and identity element"></a>Shared inverse and identity element</h4><p><strong>Prop1</strong> Any subgroup $H$ shares the identity element $e$ with $G$; any elements $h\in H$ has the same inverse element in $H$ as in $G$.</p><h4 id="Equivalent-statement"><a href="#Equivalent-statement" class="headerlink" title="Equivalent statement"></a>Equivalent statement</h4><p><strong>Prop2</strong> $H$, a non-empty subset of group $G$, is subgroup if and only if: $\forall a,b \in H$, then $ab^{-1} \in H$.</p><p><strong>Variant</strong> Let $ H \subset G, H \leq G \Leftrightarrow HH^{-1} = H$</p><h3 id="Subset-generated-subgroup"><a href="#Subset-generated-subgroup" class="headerlink" title="Subset-generated subgroup"></a>Subset-generated subgroup</h3><p><strong>Def</strong> Let $G$ be group, and $M$ is its subset. The least subgroup containing $M$, called <em>subgroup generated by $M$</em>, is denoted as $\langle M \rangle$; correspondingly, we name $M$ the <em>generating set</em> of $\langle M \rangle$.</p><ul><li>In particular, when $M$ contains only a single element $m$, then $\langle M \rangle$ is called <strong>cyclic group</strong>.<ul><li>For instance, $\mathbb Z$ can be seen as $1$-generated infinitecyclic group; $\mathbb Z_n$, the multiplicative group of integers modulo $n$, is generated by $\bar 1$ with order $n$.</li><li>Essentially, any cyclic group is isomorphic with one of them.</li></ul></li></ul><h3 id="The-order-of-element-in-group"><a href="#The-order-of-element-in-group" class="headerlink" title="The order of element in group"></a>The order of element in group</h3><p><strong>Def</strong> Let $a\in G$ be an element in group $G$. The least positive number $m$ rendering $a^m = e$ is called the order of $a$, which is denoted as $o(a)$.</p><p>Remarks:</p><ul><li>$o(a) = 1$ if and only if $a=e$</li><li>inverse element $a^{-1} =  a^{n-1}$</li><li>if $o(a) = \infty$, then $ m \neq s \Leftrightarrow a^m \neq a^s$</li></ul><h4 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h4><p><strong>Property 1</strong></p><script type="math/tex; mode=display">a^n = e \Rightarrow o(a)~|~n</script><p><strong>Property 2</strong></p><script type="math/tex; mode=display">o(a)< \infty, ~ o(a) ~ | ~|G|</script><p><strong>Property 3</strong></p><script type="math/tex; mode=display">o(a) < \infty, ~ o(a^t) = \frac{o(a)}{(t,o(a))}</script><p><strong>Property 4</strong></p><p>If $ o(a) = m &lt;\infty, o(b) = n &lt; \infty$, and $ab=ba$, $gcd(m,n)=1$, then $o(ab)=mn$</p><ul><li><em>Corollary1</em>: $o(a) = st \Rightarrow o(a^s) = t, (s,t\in \mathbb N_+)$</li><li><em>Corollary2</em>: $o(a) = n$, then $o(a^k) =n \Leftrightarrow gcd(k,n)=1$</li></ul><p>Remarks:</p><ul><li><p>In order to prove $o(a) = n$, we may prove $o(a)|n$ and $n|o(a)$</p></li><li><p>When solving divisible problem, we can write any number as $m=qn+r$</p></li></ul><h3 id="Common-notation-Descartes-product-of-subgroup"><a href="#Common-notation-Descartes-product-of-subgroup" class="headerlink" title="Common notation: (Descartes) product of subgroup"></a>Common notation: (Descartes) product of subgroup</h3><p>Let $H,K$ be subsetsof group $G$, then we denote:</p><script type="math/tex; mode=display">HK = \{hk,h\in H,k\in k\}</script><script type="math/tex; mode=display">H^{-1}=\{h^{-1}, h\in H\}</script><script type="math/tex; mode=display">aK = \{ak,k\in k\}, a\in G</script><p>Remarks:</p><ul><li>Not an one-to-one, we only require the existence</li><li>If $H$ is a group, then $H=H^{-1}$</li></ul><h4 id="Propsition-1"><a href="#Propsition-1" class="headerlink" title="Propsition"></a>Propsition</h4><p>Let $H,K$ be subgroups of $G$, then</p><script type="math/tex; mode=display">HK \leq G \Leftrightarrow HK = KH</script><h3 id="Common-types-of-subset"><a href="#Common-types-of-subset" class="headerlink" title="Common types of subset"></a>Common types of subset</h3><h4 id="The-Center-of-group"><a href="#The-Center-of-group" class="headerlink" title="The Center of group"></a>The Center of group</h4><p><strong>Def</strong> Let G be group, then the following subgroup</p><script type="math/tex; mode=display">C(G) = \{g\in G | gx=xg, \forall x \in G\}</script><p>is called the center of G.</p><p>Remarks:</p><ul><li>$C(G)$ is the collection of those which are exchangable with any elements in $G$</li><li>At least $e \in C(G)$</li><li>If $C(G) = G$, then $G$ is Abel Group</li><li>The order of $C(G)$ gives a measure of $G$’s exchangeability</li><li>Any subgroup $H$ of $G$, satisfies $HC(G) = C(G)H$</li></ul><h4 id="Centralizer"><a href="#Centralizer" class="headerlink" title="Centralizer"></a>Centralizer</h4><p><strong>Def</strong> Let $a\in G$, then subgroup</p><script type="math/tex; mode=display">C_G(a) = \{g\in G | ga = ag\}</script><p>is called the <strong>centralizer</strong> of $a$ in $G$, immediately we also have:</p><script type="math/tex; mode=display">C(G) = \bigcap_{a\in G} C_G(a)</script><p>(The centralizer of $a$ is a collection of those are exchangable with $a$ in $G$.)</p><p><strong>Property</strong></p><script type="math/tex; mode=display">|G| = \sum_{a\in C(G)} \frac{|G|}{|C_G(a)|} + \sum_{a\notin C(G)} \frac{|G|}{|C_G(a)|}</script><p>( <em>The first term: if $a\in C(G)$, then $C_G(a) = G$, so that $\sum 1$ -&gt; the order of center</em> )</p><h4 id="Normalizer-and-normal-subgroup"><a href="#Normalizer-and-normal-subgroup" class="headerlink" title="Normalizer and normal subgroup"></a>Normalizer and normal subgroup</h4><p><strong>Def</strong> Let subset $A \subset G$, then subgroup</p><script type="math/tex; mode=display">N_G(A) = \{g\in G | gA = Ag\}</script><p>is called the <strong>normalizer</strong> of set $A$ in $G$.</p><p><strong>Def</strong> Let $G$ be group, $N \leq G$, if:</p><script type="math/tex; mode=display">gN=Ng, \forall g \in G</script><p>or equivalently,</p><script type="math/tex; mode=display">gNg^{-1} = N, \forall g \in G</script><p>Then $N$ is the normal subgroup of $G$, denoted as  $N \triangleleft G$</p><p>(Corresponding to center, here we only require the <strong>existence</strong> of exchange result.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;An important concept in the group theory is the &lt;strong&gt;Subgroup&lt;/strong&gt;. As part of a given group, its subgroup holds the algebra struc
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Group-theory" scheme="http://jaredlujr.github.io/tags/group-theory/"/>
    
      <category term="Group" scheme="http://jaredlujr.github.io/tags/group/"/>
    
  </entry>
  
  <entry>
    <title>Policy iteration by dynamic programming</title>
    <link href="http://jaredlujr.github.io/2020/03/29/policy-iteration/"/>
    <id>http://jaredlujr.github.io/2020/03/29/policy-iteration/</id>
    <published>2020-03-29T15:14:53.000Z</published>
    <updated>2020-03-29T16:22:13.564Z</updated>
    
    <content type="html"><![CDATA[<p>There is a straightforward method to help agent solve the optimal policy and the idea behind is dynamic programming(DP).</p><h2 id="What-is-Dynamic-Programming"><a href="#What-is-Dynamic-Programming" class="headerlink" title="What is Dynamic Programming?"></a>What is Dynamic Programming?</h2><ul><li>A method for solving complex problem</li><li>By breaking down into subproblems<ul><li>solve the subproblems bit by bit</li><li>combine all solutions to subproblems</li></ul></li></ul><p>However, not every best solution can be found by DP, and two properties are required:</p><ul><li>Optimal substructure<ul><li>prerequisite for decomposition</li></ul></li><li>Overlapping subproblems<ul><li>subproblems recur many times</li><li>solution can be <strong>cached and reused</strong></li></ul></li></ul><p>Luckily, the <a href="https://jiaruilu.com/2020/03/20/2020-03-20-mdp" target="_blank" rel="noopener">Markov decision process(MDP)</a> satisfy both properties:</p><ul><li>Bellman equation give <strong>recursive decomposition</strong></li><li>Value function <strong>stores and reuses solutions</strong></li></ul><p>In the rest of sections, we will talk about how the solution is obtained through iterations. It looks just as simple as fix-point iteration.</p><p>Classified by our application purpose, the DP algorithm can basically have two forms:</p><ul><li>For prediction (policy evalution):<ul><li>Input: MDP $\langle S,A,P,R,\gamma \rangle$, policy $\pi$ or MRP $\langle S,P^\pi,R^\pi,\gamma \rangle$</li><li>Output: value function given $\pi$, v_\pi</li></ul></li><li>For control(optimization):<ul><li><ul><li>Input: MDP $\langle S,A,P,R,\gamma \rangle$</li></ul></li><li>Output: optimal value function v_\ast and optimal policy $\pi$</li></ul></li></ul><h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><ul><li>Problem: evaluate a given policy $\pi$</li><li>Solution: iterative application of Bellman expectation backup</li><li>Routine: $v_1\to v_2 \to \dots \to v_\pi$(end)</li><li>Synchronous backups: $k+1$-stage only use $k$-stage information</li><li>Iterative for each state in one update cycle</li><li>Converge to $v_\pi$ <em>(can be proved)</em></li></ul><h3 id="On-the-basis-of…-Bellman-equation"><a href="#On-the-basis-of…-Bellman-equation" class="headerlink" title="On the basis of… Bellman equation"></a>On the basis of… Bellman equation</h3><p><img src="be-diagram.png" alt="alt be"></p><h3 id="Algorithm-description"><a href="#Algorithm-description" class="headerlink" title="Algorithm description"></a>Algorithm description</h3><p><img src="policydp-algo1.png" alt="alt algo1"></p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Suppose there is a $4\times4$ grid with two terminals. And we will “help” agent to evaluate the current decision policy, which is described by converged value function.</p><p><img src="dp-iter.png" alt="alt iter"></p><p>When $k=1$, we will use the value function from $k=0$.</p><h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>On top of the policy evalution, which provides us a effective score for updating the policy to be better.</p><p>Because the policy, the way agent decides its action, is causative relative to the following value, the policy update is actually a optimization problem.</p><p>In short, we can improve the policy by acting greedily:</p><script type="math/tex; mode=display">\pi'(s) = arg\max_{a\in A} q_\pi (s,a)</script><p>Subsequently, this improves the value from any state $s$ over one step,</p><script type="math/tex; mode=display">v_{\pi'} = q_\pi (s,\pi'(s)) \max_{a\in A} q_\pi (s,a) \geq q_\pi (s,a) = v_\pi (s)</script><p>Improvements <strong>stop</strong> if the equality above holds, which also indicates that the Bellman optimality eqation has been satisfied.</p><h3 id="Greedy-selection"><a href="#Greedy-selection" class="headerlink" title="Greedy selection"></a>Greedy selection</h3><ul><li>Given a policy $\pi$<ul><li>evalution of $\pi$ following the last section</li><li>improvement the policy by acting <strong>greedily</strong> w.r.t $v_\pi$</li></ul></li></ul><script type="math/tex; mode=display">\pi' = greedy(v_\pi)</script><ul><li>In general, improved policy is optimal $\pi’ = \pi^\ast$</li><li><p>One-hot strategy</p></li><li><p>Routine: $\pi_0 \to v_{\pi_0} \to \pi_1 \to v_{\pi_1} \to \dots \to \pi_\ast \to \pi_\ast$</p></li></ul><p><img src="policydp-algo2.png" alt="alt algo2"></p><h3 id="Modified-Policy-Iteration"><a href="#Modified-Policy-Iteration" class="headerlink" title="Modified Policy Iteration"></a>Modified Policy Iteration</h3><ul><li>policy evaluation DOES not need to converge to $v_pi$<ul><li>more iteration cause computation waste</li><li>sometimes small iterations can give same results as final one</li></ul></li></ul><p><img src="dp-iter2.png" alt="alt iter"></p><ul><li>instead, set $k=k_0$ as stopping condition</li><li>or $\epsilon$-convergence</li><li>if we update policy EVERY ITERATION…<ul><li>this is equivalent to <strong>value iteration</strong></li></ul></li></ul><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><ul><li>Problem: find optimal policy $\pi$</li><li>Solution: iterative application of Bellman <strong>optimality</strong> backup</li><li>Routine: $v_1\to v_2 \to \dots \to v_\pi$(end)</li><li>Synchronous backups: $k+1$-stage only use $k$-stage information</li><li>Iterative for each state in one update cycle</li><li>Converge to $v_\pi$ <em>(can be proved)</em></li><li>Unlike policy iteration, there is NO explicit policy<ul><li>intermediate value functions may not correspond to any policy!</li></ul></li></ul><h3 id="Bellman-Optimality-Backup"><a href="#Bellman-Optimality-Backup" class="headerlink" title="Bellman Optimality Backup"></a>Bellman Optimality Backup</h3><p><img src="boe-diagram.png" alt="alt boe"></p><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="policydp-algo3.png" alt="alt algo3"></p><h3 id="Comparison-between-Synchronous-Dynamic-Programming-Algorithms"><a href="#Comparison-between-Synchronous-Dynamic-Programming-Algorithms" class="headerlink" title="Comparison between Synchronous Dynamic Programming Algorithms"></a>Comparison between Synchronous Dynamic Programming Algorithms</h3><p><img src="comp.png" alt="alt comp"></p><ul><li>All algorithms are based on state-value function $v_\pi(s)$ and $v_\ast (s)$</li><li>Complexity $O(mn^2)$ per iteration, for $m$ actions and $n$ states</li></ul><hr><p><strong>Reference</strong>: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;There is a straightforward method to help agent solve the optimal policy and the idea behind is dynamic programming(DP).&lt;/p&gt;
&lt;h2 id=&quot;What
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Machine-learning" scheme="http://jaredlujr.github.io/tags/machine-learning/"/>
    
      <category term="Dynamic-programming" scheme="http://jaredlujr.github.io/tags/dynamic-programming/"/>
    
      <category term="Reinforcement-learning" scheme="http://jaredlujr.github.io/tags/reinforcement-learning/"/>
    
      <category term="Control" scheme="http://jaredlujr.github.io/tags/control/"/>
    
  </entry>
  
  <entry>
    <title>How to use sed in Linux?</title>
    <link href="http://jaredlujr.github.io/2020/03/27/command-sed/"/>
    <id>http://jaredlujr.github.io/2020/03/27/command-sed/</id>
    <published>2020-03-27T14:30:31.000Z</published>
    <updated>2020-04-03T08:30:37.789Z</updated>
    
    <content type="html"><![CDATA[<p><strong>sed</strong> is a streaming text command-line tool in Linux and Mac OS(a little bit different). It can work perfect with RE to finish the operation of one or more than one text file, including delete, replace, insert and so on.</p><h2 id="Patterns"><a href="#Patterns" class="headerlink" title="Patterns"></a>Patterns</h2><h3 id="Command-template"><a href="#Command-template" class="headerlink" title="Command template"></a>Command template</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed [options] <span class="string">'command'</span> file(s)</span><br><span class="line">sed [options] -f scriptfile file(s)</span><br><span class="line">sed -h : to display the helper doc</span><br></pre></td></tr></table></figure><h3 id="Input-arguments"><a href="#Input-arguments" class="headerlink" title="Input arguments"></a>Input arguments</h3><p>File: filename(s) in system</p><h3 id="Command-parameters"><a href="#Command-parameters" class="headerlink" title="Command parameters"></a>Command parameters</h3><p>(<em>frequently used ones are listed below</em>)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a\ <span class="comment"># insert downward, relatively</span></span><br><span class="line">i\ <span class="comment"># insert upward, relatively</span></span><br><span class="line">d  <span class="comment"># delete</span></span><br><span class="line">s  <span class="comment"># replace</span></span><br><span class="line">h  <span class="comment"># copy the content to buffer</span></span><br><span class="line">g  <span class="comment"># use stuffs in buffer to replace</span></span><br></pre></td></tr></table></figure></p><h3 id="Replace-flags"><a href="#Replace-flags" class="headerlink" title="Replace flags"></a>Replace flags</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">g <span class="comment"># all-replace in line</span></span><br><span class="line">p <span class="comment"># print the line</span></span><br><span class="line">w <span class="comment"># write the line into a file</span></span><br><span class="line">x <span class="comment"># swap the text with the buffer</span></span><br></pre></td></tr></table></figure><h3 id="Metacharacter-set"><a href="#Metacharacter-set" class="headerlink" title="Metacharacter set"></a>Metacharacter set</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">^ <span class="comment"># match the beginning of line</span></span><br><span class="line">$ <span class="comment"># match the ending of line</span></span><br><span class="line">. <span class="comment"># match a non-new-line arbitrary character</span></span><br><span class="line">* <span class="comment"># match 0 or more character</span></span><br><span class="line">[] <span class="comment"># match a range of characters</span></span><br><span class="line">[^] <span class="comment"># match characters beyond the range</span></span><br><span class="line">\(..\) <span class="comment"># match substring and keep</span></span><br><span class="line">&amp; <span class="comment"># keep target character and replace the &amp;</span></span><br><span class="line">\&lt; <span class="comment"># match the beginning of word</span></span><br><span class="line">\&gt; <span class="comment"># match the ending of word</span></span><br></pre></td></tr></table></figure><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/^sed/ <span class="comment"># -&gt; "sedimentation ..."</span></span><br><span class="line">/sed$/ <span class="comment"># -&gt; ".... sed"</span></span><br><span class="line">/s.d/ <span class="comment"># -&gt; "sad"</span></span><br><span class="line">/*sed/ <span class="comment"># -&gt; "    sed"</span></span><br><span class="line">[] <span class="comment"># /[ss]ed/ -&gt; "sed" or "Sed"</span></span><br><span class="line">[^] <span class="comment"># /[^A-RT-Z]ed/ "Sed"</span></span><br><span class="line">\(..\) <span class="comment"># s/\(love\)able/\1rs，loveable -&gt; lovers</span></span><br><span class="line">&amp; <span class="comment"># s/love/ **&amp;** / "love"-&gt; "**love**"</span></span><br><span class="line">\&lt; <span class="comment"># /\&lt;love/ -&gt; "loveable" or "lovelorn"</span></span><br><span class="line">\&gt; <span class="comment"># /love\&gt;/ -&gt; "nolove"</span></span><br></pre></td></tr></table></figure><h2 id="Usage-instances"><a href="#Usage-instances" class="headerlink" title="Usage instances"></a>Usage instances</h2><h3 id="replacing-s"><a href="#replacing-s" class="headerlink" title="replacing: s"></a>replacing: s</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/book/books/'</span> file</span><br><span class="line"></span><br><span class="line">sed -n <span class="string">'s/test/TEST/p'</span> file <span class="comment"># only matched lines are printed out</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/book/books/g'</span> file <span class="comment"># directly editing the files without display</span></span><br></pre></td></tr></table></figure><h3 id="all-replacing-g"><a href="#all-replacing-g" class="headerlink" title="all-replacing: g"></a>all-replacing: g</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/book/books/g'</span> file</span><br><span class="line"></span><br><span class="line"><span class="comment"># if we want replacing happens from Nth match</span></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/2g'</span></span><br><span class="line">skSKSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/3g'</span></span><br><span class="line">skskSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/4g'</span></span><br><span class="line">skskskSKSKSK</span><br></pre></td></tr></table></figure><h3 id="delimiter-and-so-on"><a href="#delimiter-and-so-on" class="headerlink" title="delimiter: / : | and so on"></a>delimiter: / : | and so on</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/test/TEXT/g'</span></span><br><span class="line">sed <span class="string">'s:test:TEXT:g'</span></span><br><span class="line">sed <span class="string">'s|test|TEXT|g'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># escaping when we need their happening</span></span><br><span class="line">sed <span class="string">'s/\/bin/\/usr\/local\/bin/g'</span></span><br></pre></td></tr></table></figure><h3 id="delete-d"><a href="#delete-d" class="headerlink" title="delete: d"></a>delete: d</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/^$/d'</span> file <span class="comment"># delete empty lines</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'2d'</span> file <span class="comment"># delete 2nd line</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'2,$d'</span> file <span class="comment"># delete 2nd line to end</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'$d'</span> file <span class="comment"># delete end line</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'/^test/'</span>d file <span class="comment"># delete line with 'test' as beginning</span></span><br></pre></td></tr></table></figure><h3 id="choosing-target-lines"><a href="#choosing-target-lines" class="headerlink" title="choosing target lines: ,"></a>choosing target lines: ,</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'/test/,/check/p'</span> file <span class="comment"># prints lines from test and check</span></span><br><span class="line"></span><br><span class="line">sed -n <span class="string">'5,/^test/p'</span> file <span class="comment"># print from 5th line to the first line with beginning as 'text'</span></span><br></pre></td></tr></table></figure><h3 id="multiple-command-e"><a href="#multiple-command-e" class="headerlink" title="multiple command: e"></a>multiple command: e</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -e <span class="string">'1,5d'</span> -e <span class="string">'s/test/check/'</span> file <span class="comment"># perform double operations to each line</span></span><br></pre></td></tr></table></figure><h3 id="write-into-file-w"><a href="#write-into-file-w" class="headerlink" title="write into file: w"></a>write into file: w</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'/test/w outfile'</span> example</span><br></pre></td></tr></table></figure><h3 id="add-downward-a"><a href="#add-downward-a" class="headerlink" title="add(downward): a"></a>add(downward): a</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/^test/a\this is a test line'</span> file</span><br></pre></td></tr></table></figure><h3 id="insert-upward-i"><a href="#insert-upward-i" class="headerlink" title="insert(upward): i"></a>insert(upward): i</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/^test/i\this is a test line'</span> file <span class="comment"># before the lines with beginning as 'test'</span></span><br></pre></td></tr></table></figure><h3 id="Other-common-usage"><a href="#Other-common-usage" class="headerlink" title="Other common usage"></a>Other common usage</h3><h4 id="keep-matched"><a href="#keep-matched" class="headerlink" title="keep matched"></a>keep matched</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> this is a <span class="built_in">test</span> line | sed <span class="string">'s/\w\+/[&amp;]/g'</span> <span class="comment"># match each word and add brackets</span></span><br><span class="line">[this] [is] [a] [<span class="built_in">test</span>] [line]</span><br><span class="line"></span><br><span class="line">sed <span class="string">'s/^192.168.0.1/&amp;localhost/'</span> file</span><br><span class="line">192.168.0.1localhost</span><br></pre></td></tr></table></figure><h4 id="substring"><a href="#substring" class="headerlink" title="substring"></a>substring</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> this is digit 7 <span class="keyword">in</span> a number | sed <span class="string">'s/digit \([0-9]\)/\1/'</span></span><br><span class="line">this is 7 <span class="keyword">in</span> a number</span><br></pre></td></tr></table></figure><h4 id="combination"><a href="#combination" class="headerlink" title="combination"></a>combination</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'pattern'</span> | sed <span class="string">'pattern'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># also as:</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'pattern; pattern'</span></span><br></pre></td></tr></table></figure><h4 id="quotation"><a href="#quotation" class="headerlink" title="quotation"></a>quotation</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">test</span>=hello</span><br><span class="line"><span class="built_in">echo</span> hello WORLD | sed <span class="string">"s/<span class="variable">$test</span>/HELLO"</span></span><br><span class="line">HELLO WORLD</span><br></pre></td></tr></table></figure><h4 id="next"><a href="#next" class="headerlink" title="next"></a>next</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'/test/&#123; n; s/aa/bb/; &#125;'</span> file <span class="comment"># if test is matched then perform over the next line</span></span><br></pre></td></tr></table></figure><h4 id="quit-break"><a href="#quit-break" class="headerlink" title="quit(break)"></a>quit(break)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'10q'</span> file <span class="comment"># quit sed after print out line 10</span></span><br></pre></td></tr></table></figure><h4 id="print-odd-or-even-lines"><a href="#print-odd-or-even-lines" class="headerlink" title="print odd or even lines"></a>print odd or even lines</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'p;n'</span> test.txt  <span class="comment">#odd</span></span><br><span class="line">sed -n <span class="string">'n;p'</span> test.txt  <span class="comment">#even</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"></span><br><span class="line">sed -n <span class="string">'1~2p'</span> test.txt  <span class="comment">#odd</span></span><br><span class="line">sed -n <span class="string">'2~2p'</span> test.txt  <span class="comment">#even</span></span><br></pre></td></tr></table></figure><h2 id="Affiliated-tools-under-shell"><a href="#Affiliated-tools-under-shell" class="headerlink" title="Affiliated tools under shell"></a>Affiliated tools under shell</h2><h3 id="Get-all-filenames-as-var-by"><a href="#Get-all-filenames-as-var-by" class="headerlink" title="Get all filenames as var by `"></a>Get all filenames as var by `</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `ls | grep .jpg`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  newfile=`<span class="built_in">echo</span> <span class="variable">$file</span> | sed <span class="string">'s/dtest/cora/g'</span>`</span><br><span class="line">  mv <span class="variable">$file</span> <span class="variable">$newfile</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h3 id="String-slice"><a href="#String-slice" class="headerlink" title="String slice"></a>String slice</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="comment">#var&#125; # length of string var</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;alpha:3&#125;</span> <span class="comment"># display rest after first 3 chars</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;alpha:3:4&#125;</span> <span class="comment"># display 4 chars after first 3 chars</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;alpha: -3&#125;</span> <span class="comment"># display the last 3</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;sed&lt;/strong&gt; is a streaming text command-line tool in Linux and Mac OS(a little bit different). It can work perfect with RE to fi
      
    
    </summary>
    
    
      <category term="Computer-science" scheme="http://jaredlujr.github.io/categories/computer-science/"/>
    
    
      <category term="Linux" scheme="http://jaredlujr.github.io/tags/linux/"/>
    
      <category term="Command-line" scheme="http://jaredlujr.github.io/tags/command-line/"/>
    
      <category term="sed" scheme="http://jaredlujr.github.io/tags/sed/"/>
    
  </entry>
  
  <entry>
    <title>Counting the number of permutations</title>
    <link href="http://jaredlujr.github.io/2020/03/26/2020-03-26-stirling1/"/>
    <id>http://jaredlujr.github.io/2020/03/26/2020-03-26-stirling1/</id>
    <published>2020-03-25T16:00:00.000Z</published>
    <updated>2020-03-27T13:28:28.184Z</updated>
    
    <content type="html"><![CDATA[<p>Permutation group is an important instance in group theory. And each permutation $\pi$ can be uniquely decomposed into the combination of <strong>cycles</strong>. Then the number of such permutation in the known permutation group with specific cycles decompostion is correlated with <strong>Stirling numbers of the first kind</strong>.</p><h2 id="Term-definition"><a href="#Term-definition" class="headerlink" title="Term definition:"></a>Term definition:</h2><ul><li>finite set with order $n$: $X$, which is also isomorphic with $[n]$</li><li>the permutation group over $X$: $S_n$, obviously $\left| S_n \right| = n!$</li><li>some permutation in $S_n$:  $\pi=\pi(1)\pi(2)\dots \pi(n)$<ul><li>In fact, it is same as full-permutation</li></ul></li><li>the cyclic decompostion of permutation $\pi$: given $\pi \in S_n, i\in [n]$, there must exist a cycle $(i,\pi(i),\pi^2(i),\dots,\pi^{s-1}(i), i)$ (denoted as $C$), where $s$ is the least positive integer constructing such cycle. Then it is obvious that $\forall x \in [n]$ has to belong to one and only one cycle, and thus we have the cyclic decompostion of given permutation:</li></ul><script type="math/tex; mode=display">\pi = C_1\dots C_k</script><p>which is unique for given $\pi$, thus forming an one-to-one.</p><ul><li><p>the pattern(type) of permutation: in terms of the decompostion above, if there are totally $\lambda_s$ cycles with length of $s$, then we can write the pattern of such cyclic decompostion: $1^{\lambda_1}2^{\lambda_2}\dots n^{\lambda_n}$. Such as: (147)(25)(3)(6) is $1^2 2^1 3^1$ type.</p></li><li><p>conjugative relation ( which is also an <strong>equivalence relation</strong>): $\pi - \pi’ ~ if ~ \exists ~\rho \in S_n ~s.t. ~\pi’ = \rho\pi\rho^{-1}$</p></li><li><strong>[Prop]</strong>  $\pi, \pi’ \in S_n $ are conjugative if and only if they share the same pattern.</li></ul><h2 id="Stirling-numbers-of-the-first-kind"><a href="#Stirling-numbers-of-the-first-kind" class="headerlink" title="Stirling numbers of the first kind"></a>Stirling numbers of the first kind</h2><p>Here, we first give the definition of such Stirling numbers $s(n,k)$, then talking about its property and relations to the second kind.</p><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Given $n,k \in S_n$, the number of permutations those can be exactly decomposed into $k$ cycles is denoted as $c(n,k)$, then let</p><script type="math/tex; mode=display">s(n,k) = (-1)^{n+k} c(n,k)</script><p>which is called <em>Stirling numbers of the first kind</em>.</p><p>We appoint that</p><ul><li>$s(n,0) = c(n,0) = 0 ~(n&gt;0) $</li><li>$s(0,0)= c(0,0) = 1$</li><li>$s(n,k) = c(n,k) = 0,~ n &lt; k$</li></ul><h3 id="Recursive-relation"><a href="#Recursive-relation" class="headerlink" title="Recursive relation"></a>Recursive relation</h3><ul><li>$c(n,k) = c(n-1,k-1) + (n-1)c(n-1,k)$</li><li>$s(n,k) = s(n-1,k-1) - (n-1)s(n-1,k)$</li></ul><h3 id="Expansion"><a href="#Expansion" class="headerlink" title="Expansion"></a>Expansion</h3><p>Let $x$ be indefinite element, then</p><ul><li><script type="math/tex; mode=display">(x)_n = \sum_{k=0}^n s(n,k) x^k</script></li><li><script type="math/tex; mode=display">(x)^n = \sum_{k=0}^n c(n,k) x^k</script></li></ul><p>which are respectively $n-$ downward and upward factorial.</p><h3 id="Dual-expansion-with-“the-second-kind”"><a href="#Dual-expansion-with-“the-second-kind”" class="headerlink" title="Dual expansion with “the second kind”"></a>Dual expansion with “the second kind”</h3><p>Since have known the following expansion:</p><ul><li><script type="math/tex; mode=display">(x)_n = \sum_{k=0}^n s(n,k) x^k</script></li><li><script type="math/tex; mode=display">x^n = \sum_{k=0}^n S(n,k) (x)_k</script></li></ul><p>We can spot their dual relations at once.</p><p>Moveover, we can deduce more magic result from them two.</p><p>Because the $s(k,m) = 0$ if $m &gt; k$, we have:</p><script type="math/tex; mode=display">x^n = \sum_{k=0}^n S(n,k) (x)_k = \sum_{k=0}^n S(n,k) \sum_{m=0}^k s(k,m) (x)_m \\= \sum_{m=0}^k \left(\sum_{k=0}^n S(n,k)s(k,m) \right) x^m</script><p>_(Check it! It holds since we let some terms be zero )_</p><p>We immediately have:</p><script type="math/tex; mode=display">\sum_{k=0}^n S(n,k)s(k,m)  = \delta_{mn}</script><p>In the form of matrix, we can know that $n+1$-order matrices $S_n^{(2)} = (S(i,j))~(i,j=0,1,\dots)$ and $S_n^{(1)} = (s(i,j))$ are mutually inversive.</p><script type="math/tex; mode=display">S_n^{(1)} S_n^{(2)} = S_n^{(2)} S_n^{(1)} = \mathit I</script><hr><p><strong>Reference</strong></p><p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Stirling_numbers_of_the_first_kind" target="_blank" rel="noopener">Stirling numbers of the first kind</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Permutation group is an important instance in group theory. And each permutation $\pi$ can be uniquely decomposed into the combination of
      
    
    </summary>
    
    
      <category term="Math" scheme="http://jaredlujr.github.io/categories/math/"/>
    
    
      <category term="Combinatorics" scheme="http://jaredlujr.github.io/tags/combinatorics/"/>
    
  </entry>
  
</feed>
