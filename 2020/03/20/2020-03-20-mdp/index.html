<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Courier New:300,300italic,400,400italic,700,700italic|Georgia:300,300italic,400,400italic,700,700italic|Palatino:300,300italic,400,400italic,700,700italic|Arial:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine-learning,Reinforcement-leraning,MDP," />




  


  <link rel="alternate" href="/atom.xml" title="Jiarui Lu" type="application/atom+xml" />






<meta name="description" content="Markov Process(MP) is an important model in reinforcement learning. It provides the framework for description of agent’s state-reward-action chain. MP can be subsequently divided into Markov Reward Pr">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Markov Decision Process">
<meta property="og:url" content="http://jaredlujr.github.io/2020/03/20/2020-03-20-mdp/index.html">
<meta property="og:site_name" content="Jiarui Lu">
<meta property="og:description" content="Markov Process(MP) is an important model in reinforcement learning. It provides the framework for description of agent’s state-reward-action chain. MP can be subsequently divided into Markov Reward Pr">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-03-19T16:00:00.000Z">
<meta property="article:modified_time" content="2020-03-29T16:10:52.085Z">
<meta property="article:author" content="Jiarui Lu">
<meta property="article:tag" content="Machine-learning">
<meta property="article:tag" content="Reinforcement-leraning">
<meta property="article:tag" content="MDP">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'KV6M2IMYAC',
      apiKey: 'e963419de387ecf89ab76ca23cabaeed',
      indexName: 'jaredblog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://jaredlujr.github.io/2020/03/20/2020-03-20-mdp/"/>





  <title>Introduction to Markov Decision Process | Jiarui Lu</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-162026662-1', 'auto');
  ga('send', 'pageview');
</script>





<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jiarui Lu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Non-Stop</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-project">
          <a href="https://github.com/jaredlujr/" target="_blank" rel="section noopener">
            
            project
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://jaredlujr.github.io/2020/03/20/2020-03-20-mdp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiarui Lu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/profile1.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jiarui Lu">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Introduction to Markov Decision Process</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-20T00:00:00+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computer-science/" itemprop="url" rel="index">
                    <span itemprop="name">Computer-science</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/03/20/2020-03-20-mdp/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/03/20/2020-03-20-mdp/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2020/03/20/2020-03-20-mdp/" class="leancloud_visitors" data-flag-title="Introduction to Markov Decision Process">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>Markov Process(MP)</strong> is an important model in reinforcement learning. It provides the framework for description of agent’s state-reward-action chain. MP can be subsequently divided into <strong>Markov Reward Process(MRP)</strong> and <strong>Markov Decision Process(MDP)</strong>, respectively based on different measures.</p>
<h2 id="Elements-of-RL"><a href="#Elements-of-RL" class="headerlink" title="Elements of RL"></a>Elements of RL</h2><p>In the first section, we need to point out the definition, which is essential to understand MRP and MDP better.</p>
<p>An RL agent may include one or more of these components:</p>
<ul>
<li><p>Policy: agent’s behaviour function</p>
<ul>
<li>it is a <strong>map</strong> from state to action</li>
<li>Deterministic policy: $a=\pi(s)$</li>
<li>Stochastic policy:<script type="math/tex; mode=display">\pi(a|s) = P[A_t=a|S_t=s]</script></li>
</ul>
</li>
<li><p>Value function: how good is each state and/or action</p>
<ul>
<li>a prediction of future reward</li>
<li>used to evaluate the goodness/ badness of <strong>states</strong> and therefore select between actions<script type="math/tex; mode=display">v_\pi(s) = E_\pi [R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3}+ \dots|S_t =s ]</script></li>
</ul>
</li>
<li>Model: agent’s representation of the environment<ul>
<li>predict what the environment will do next</li>
<li>transitions: $p$ predicts the next state<script type="math/tex; mode=display">p^a_{ss'} = P[S_{t+1} = s'| S_t=s.A_t = a]</script></li>
<li>Rewards: R predicts the next(immediate) reward,e.g.<script type="math/tex; mode=display">R_{s}^a = E[R_{t+1} | S_t =s A_t = a]</script></li>
</ul>
</li>
</ul>
<p>With all of these building blocks, we can go on talking about the Markov Process with its variants.</p>
<h2 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h2><p>The central idea of MP is the <strong>Memoryless</strong>: “the future is independent of the past given the present”, i.e., once the current state is known, the history may be thrown away since the state itself is a sufficient statistic of the future.</p>
<p>_[Def]_ A state $S_t$ is Markov if and only if</p>
<script type="math/tex; mode=display">P[S_{t+1}|S_t] = P[S_{t+1}|S_1,\dots,S_t]</script><p><strong>State transition probability</strong> is defined by</p>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s]</script><p>Collection of transition probabilities from all states $s$ to all successor $s’$ is denoted by ST matrix p:</p>
<script type="math/tex; mode=display">p =
\begin{bmatrix}
p_{11} & \dots & p_{1n} \\
\vdots &  \vdots & \vdots \\
p_{n1} & \dots & p_{1n}
\end{bmatrix}</script><p><strong>where each row of the matrix sums to $1$</strong>.</p>
<p>A Markov Process (or Markov Chain) is a tuple $<S, p >$, i.e., a sequence of random states $S_1,S_2,\dots$ with the Markov property.</p>
<ul>
<li>$S$ is a (finite) set of states</li>
<li>$p$ is a state transition probability matrix</li>
</ul>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s]</script><h2 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h2><p>![alt img1]</p>
<p>The goal of agent is, informally speaking, to maximize _the total amount of reward it received._ Not only the immediate reward $R_{t+1}$, but in a long run.</p>
<p>A Markov reward process(MRP) is a Markov chain with <strong>values</strong>, as such tuple $<S,p,R,\gamma>$</p>
<ul>
<li>$S$ is a (finite) set of states</li>
<li>$p$ is a state transition probability matrix</li>
</ul>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s]</script><ul>
<li>$R$ is a reward function(of state $s$)</li>
</ul>
<script type="math/tex; mode=display">R_s = E[R_{t+1}|S_t=s]</script><ul>
<li>$\gamma$ is a discount factor, $\gamma \in [0,1]$</li>
</ul>
<p>Thus, we have designed a manner to give reward “label” to each state $s$.</p>
<h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><p>The <strong>Return</strong> $G_t$ is the total discounted reward from time-step $t$</p>
<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^\infin \gamma^k R_{t+k+1}</script><ul>
<li>The discount $\gamma \in [0,1]$ is the <strong>present value</strong> of future rewards(_decay weighted_)</li>
<li>The value of receiving reward $R$ after $k+1$ time-steps is $\gamma^k R$</li>
<li>This values immediate reward above delayed reward.<ul>
<li>$\gamma \to 0$ leads to “myopic” evaluation</li>
<li>$\gamma \to 1$ leads to “far-sighted” evaluation</li>
</ul>
</li>
</ul>
<p><strong>Remarks:</strong></p>
<ul>
<li>Avoids infinite returns in cyclic Markov processes<ul>
<li>If the task is continuous, the final time step would be $T = \infin$</li>
</ul>
</li>
<li>Uncertainty about the future may not be fully represented</li>
<li>It is sometimes possible to use undiscounted Markov reward processes (i.e. $\gamma = 1$), e.g. if all sequences terminate, which we call episodic</li>
<li>$(*)$ If the reward is financial, immediate rewards may earn more interest than delayed rewards</li>
<li>$(*)$ Animal/human behaviour shows preference for immediate reward<br>task.</li>
</ul>
<h3 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h3><p>The value function $v(s)$ gives the long-term value of state $s$, i.e., an estimation of <strong>how good it is</strong> for the agent to be in the given state.</p>
<p>The state value function $v(s)$ of an MRP is the <strong>expected return</strong> starting from state $s$</p>
<script type="math/tex; mode=display">v(s) = E[G_t|S_t = s ]</script><p>If we expand $G_t$ in the R.H.S. term, we have the <strong>Bellman Equation</strong>:</p>
<script type="math/tex; mode=display">v(s) =E[G_t|S_t = s ]= E[R_{t+1} + \gamma R_{t+2} + \dots |S_t = s ] = E[R_{t+1} + \gamma v(S_{t+1} | S_t = s)]</script><p>![alt img2]<br>It can be interpreted as iterative decompsition into two parts:</p>
<ul>
<li>immediate reward $R_{t+1}$</li>
<li>discounted <strong>value</strong> of successor state $\gamma v(S_{t+1})$</li>
</ul>
<p>![alt img3]</p>
<script type="math/tex; mode=display">v(s) = R_s + \gamma \sum_{s' in S} p_{ss'}v(s')</script><p>Write Bellman Equation as matrix form:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
v(1) \\
\vdots  \\
v(n)
\end{bmatrix} =
\begin{bmatrix}
R_1 \\
\vdots  \\
R_n
\end{bmatrix}+ \gamma
\begin{bmatrix}
p_{11} & \dots & p_{1n} \\
\vdots &  \vdots & \vdots \\
p_{n1} & \dots & p_{1n}
\end{bmatrix}+
\begin{bmatrix}
v(1) \\
\vdots  \\
v(n)
\end{bmatrix}</script><script type="math/tex; mode=display">v = R + \gamma p v</script><p>It is such a linear equation that can be solved directly:</p>
<script type="math/tex; mode=display">(1- \gamma p)v = R</script><script type="math/tex; mode=display">v = (1- \gamma p)^{-1} R</script><p>However, computational complexity is $O(n^3)$ for $n$ states, and thus direct solution is only possible for small MRPs.</p>
<p>We may try <strong>iterative methods</strong> like Dynamic programming, Monte-Carlo evaluation and so on.</p>
<h2 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h2><p>A Markov Decision Process(MDP) is a Markov reward process with <strong>decisions</strong>. It is an environment in which all states are Markov, represented as a tuple $<S,A,p,R,\gamma>$</p>
<ul>
<li>$S$ is a finite set of states</li>
<li>$A$ is a finite set of actions</li>
<li>$p$ is a state transition probability matrix</li>
</ul>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s,A_t=a]</script><ul>
<li>$R$ is a reward function(of state $s$)</li>
</ul>
<script type="math/tex; mode=display">R_s = E[R_{t+1}|S_t=s,A_t = a]</script><ul>
<li>$\gamma$ is a discount factor, $\gamma \in [0,1]$</li>
</ul>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>A policy $\pi$ is a ditribution over actions given states, say <strong>weights of each transition</strong> at specific state.</p>
<script type="math/tex; mode=display">\pi (a|s) = P[A_t = a| S_t = s]</script><ul>
<li>a policy fully defined the behavior of an agent</li>
<li>MDP policies depend on the current state( not history!)</li>
<li>policies are stationary (time-independent)</li>
</ul>
<script type="math/tex; mode=display">A_t ~ \pi(\cdot|S_t), \forall t > 0</script><p>![alt img4]</p>
<p>Remarks:</p>
<ul>
<li>The state sequence $S_1,S_2,\dots$ is a Markov process $<S,p^\pi>$</li>
<li>THe state and reward sequence $S_1,R_2,S_2,\dots$ is a Markov reward process $<S,p^\pi,R^\pi,\gamma>$</li>
</ul>
<p>where</p>
<script type="math/tex; mode=display">p^\pi_{ss'} = \sum_{a \in A} \pi(a|s)p^a_{ss'}</script><script type="math/tex; mode=display">R^\pi_{s} = \sum_{a \in A} \pi(a|s)p^a_{s}</script><p>The <strong>state-value function</strong> $v_\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\pi$</p>
<script type="math/tex; mode=display">v_\pi(s) = E_\pi [G_t|S_t=s]</script><p>The <strong>action-value function</strong> $q_\pi(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$</p>
<script type="math/tex; mode=display">q_\pi(s,a) = E_\pi [G_t|S_t=s,A_t=a]</script><p>![alt img5]</p>
<p>In fact, these two functions can also be decomposed by following Bellman Equation.</p>
<p>![alt img6]</p>
<p>![alt img7]</p>
<h2 id="Optimal-value-function-trained-result"><a href="#Optimal-value-function-trained-result" class="headerlink" title="Optimal value function (trained result)"></a>Optimal value function (trained result)</h2><p>At last, we introduce the optimal value function, i.e., the maximum ( state-, action- )value function over all policies. Like:</p>
<script type="math/tex; mode=display">v_\ast (s) = max_{\pi} v_ast (s)</script><ul>
<li>the optimal value functiion specifies the best possible performance in the MDP</li>
<li>MDP is already <strong>“solved”</strong> when we know the optimal value</li>
</ul>
<p>Under this goal, we can define a partial ordering over policies:</p>
<script type="math/tex; mode=display">\pi \geq \pi' ~ if ~ v_\pi(s) \geq v_{\pi_\ast}(s), \forall s</script><p><strong>Th.</strong></p>
<ul>
<li>There exists an optimal policy $\pi$, which is a recommended way to make action</li>
<li>All <strong>optimal policies achieve</strong> the optimal value function</li>
<li>All <strong>optimal policies achieve</strong> the optimal action-value function</li>
</ul>
<p>An <strong>optimal policy</strong> can be found by maximizing over $q_\ast(s,a)$</p>
<script type="math/tex; mode=display">\pi_ast (a|s) = 1, if a = argmax_{a \in A} q_\ast (s,a) ~( 0 otherwise )</script><p>_( There is always a deterministic optimal policy for any MDP. )_</p>
<p>Remarks:</p>
<ul>
<li>Bellman Optimality Equation is non-linear</li>
<li>No closed form solution ( in general )</li>
<li>iterative solution methods include<ul>
<li>Value Iteration</li>
<li>Policy Iteration</li>
<li>Q-Learning</li>
<li>Sarsa</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Reference</strong>:</p>
<ul>
<li>Wikipedia-<a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener">reinforcement Learning</a>;</li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># Machine-learning</a>
          
            <a href="/tags/reinforcement-leraning/" rel="tag"># Reinforcement-leraning</a>
          
            <a href="/tags/mdp/" rel="tag"># MDP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/18/2020-03-18-nlu-notes2/" rel="next" title="Supervised learning for NLU - Basics">
                <i class="fa fa-chevron-left"></i> Supervised learning for NLU - Basics
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/03/23/2020-03-23-viterbi/" rel="prev" title="Hidden Markov model and Viterbi algorithm">
                Hidden Markov model and Viterbi algorithm <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  




        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/profile1.png"
                alt="Jiarui Lu" />
            
              <p class="site-author-name" itemprop="name">Jiarui Lu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">51</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">52</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jaredlujr" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jaredlujr@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/jaredlujr" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Elements-of-RL"><span class="nav-number">1.</span> <span class="nav-text">Elements of RL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-Process"><span class="nav-number">2.</span> <span class="nav-text">Markov Process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-Reward-Process"><span class="nav-number">3.</span> <span class="nav-text">Markov Reward Process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Return"><span class="nav-number">3.1.</span> <span class="nav-text">Return</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Value-function"><span class="nav-number">3.2.</span> <span class="nav-text">Value function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-Decision-Process"><span class="nav-number">4.</span> <span class="nav-text">Markov Decision Process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy"><span class="nav-number">4.1.</span> <span class="nav-text">Policy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimal-value-function-trained-result"><span class="nav-number">5.</span> <span class="nav-text">Optimal value function (trained result)</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2020/02/25 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiarui Lu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>








        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://jaredlublog.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://jaredlujr.github.io/2020/03/20/2020-03-20-mdp/';
          this.page.identifier = '2020/03/20/2020-03-20-mdp/';
          this.page.title = 'Introduction to Markov Decision Process';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://jaredlublog.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("aD39KMm6VV9WHCqHgGem30d0-gzGzoHsz", "xGgboJIt3rSrHMBObxPGFDgF");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script>


  

</body>
</html>
