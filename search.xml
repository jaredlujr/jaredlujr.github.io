<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PAC assumption</title>
    <url>/2020/04/08/pac-assumption/</url>
    <content><![CDATA[<p>Probability approximately correct(PAC) assumption is a plausible but not enough general assumption which ignores the noises: <strong>the training data was drawn from the same distribution $D$</strong>.</p>
<p>Consider the setting of linear classification, a very simple case, one approach is to try to minimize the training error, and pick the “best”:</p>
<script type="math/tex; mode=display">\hat\theta = arg\min_\theta \hat\epsilon(h_\theta)</script><p>We call this process <strong>empirical risk minimization</strong>(ERM), which is regarded as the most “basic” learning algorithm. If abstacting away from the specific parameterization of hypotheses, instead, we consider the hypothesis class $H$, where all the classifiers lie. Foe example, in terms of neural networks, we could let $H$ be the set of all classifers <strong>representable</strong> by some neural network architecture. (which is an incredibly wide range of function domain)</p>
<p>In the case of finite hypothesis space $H$, we can derive the estimation of generalization error by applying <strong>the union bound</strong> and <strong>Chernoff bound</strong>, and give the result in a form including:</p>
<ul>
<li>the number of samples $m$</li>
<li>the confidence $1- \delta$</li>
<li>$k$, as the order of hypothesis space $H$</li>
<li>the discrepancy range of errors $\gamma$</li>
</ul>
<p>Focusing on the first term! It provides some insights about how many training examples we need to make an enough guarantee on the correctness of our model, in the following way:</p>
<script type="math/tex; mode=display">m \ge \frac{1}{2\gamma^2}log\frac{2k}{\delta}</script><p>The training set size $m$ that a certain given method or algorithm requires in order to achieve a certain level of performance is also called the <strong>algorithm’s sample complexity.</strong></p>
<p>We can also express the generalization error of the best hypothesis we can find in the following relative way:</p>
<script type="math/tex; mode=display">\epsilon(\hat h) \leq (min_{h\in H}\epsilon (h) + 2\gamma)</script><p>which means that the uniform convergence occurs with probability at least $1-\delta$. Thus, we bound the error in some ways.</p>
<p>As for the infinite $H$ case, we change the way of analysis and define the <strong>Vapnik-Chervonenkis dimension(VC dimension)</strong> of given hypothesis class $H$, written $VC(H)$. It is the size of the largest set that is <strong>shattered</strong> by $H$.</p>
<p>We say that $H$ shatters $S$ if $H$, i.e. considering all the $h\in H$, are able to realize any labeling on $S$. The discussion focuses on fixed arrangement of points in fixed amount $d$ but arbitrary labeling. This is to say, in order to prove that $VC(H)$ is at least $d$, we need to show only that there exists at least one specific set of size $d$ that $H$ can shatter.</p>
<p>For example, the linear hypotheses $h\in \mathbb R^2$ cannot shatter any four points on the plane, and in fact it cannot either shatter the three points in a line, but can work otherwise. Thus, the VC dimension is $3$.</p>
<p>With $VC(H)=d$ defined, we are equipped with a proper tool to describe the error bound, at least the order of it. Generally speaking, how the $\epsilon(\hat h)$ will change along with the number of examples $m$. It can be proved that such proportion stays linear.</p>
<p><strong>Theorem</strong> Given $H$, let $d=VC(H)$. Then with probability at least $1-\delta$, we have:</p>
<script type="math/tex; mode=display">\epsilon(\hat h) \leq \epsilon(h^\ast) + O\left( \sqrt{\frac{d}{m}log \frac{m}{d} + \frac{1}{m}log \frac{1}{\delta}}\right)</script><hr>
<p><strong>Reference</strong>:</p>
<p><a href="http://cs229.stanford.edu/notes/cs229-notes4.pdf" target="_blank" rel="noopener">Stanford CS229-note4</a> by Andrew Ng<br>Statistical Learning methods, by Li Hang (2012)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Learning-theory</tag>
        <tag>Error-estimation</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning theory in Machine learning</title>
    <url>/2020/04/08/learning-theory/</url>
    <content><![CDATA[<p>Learning theory is necessary when it comes to properly evaluate a specific learning algorithm. In Numerical Analysis, many evaluation methods and standards are established to help judge “how good” the algorithm or model is, such as errors, convergence speed, convergence domain and so on. Likewise, we eagerly want to know about the machine learning methods.</p>
<p>In terms of the goal in machine learning, most of them are classification. More generally speaking, we would like to learning something derived from a given dataset, say some amount of data points $T = \{(x^{(i)},y^{(i)})\}$. The Machine Learning is also called Statistical Learning, since most of the the tasks are partially or totally based on the data.</p>
<p>However, the data is sampled as the observation from human, which may contain some noises, or even worse, not strictly follow specific distribution. The topic in this article is to make an exposition: under some assumptions, how to evaluate a given model, even if we cannot accurately give what the errors are, but nevertheless restrict them within some boundaries.</p>
<h3 id="Trade-off-bias-or-variance"><a href="#Trade-off-bias-or-variance" class="headerlink" title="Trade-off: bias or variance"></a>Trade-off: bias or variance</h3><p>Like what happens in the classic interpolation and curve fitting scenario, there exists a trade-off to use simple or complex function(hypothesis) to generate the best “resonance” to the given data.</p>
<p>In machine learning, we define the generalization error of a hypothesis is its expected error on examples not necessarily in the training set.</p>
<p>We will call the errors resulted from over-simple hypothesis as <strong>bias</strong>; on the contrary, that of data over-fitting is called <strong>variance</strong>.</p>
<p>This is straightforward! Because the observed data source does not probably obey the probability distribution as far as we’ve concerned, so the emergence of error makes sense and inevitable; however, we can absolutely make the best fitting with no error at all by memorizing the data, say using higher-polynomial with tons of parameters. It is exactly what has happened in the curve fitting case: the fitting line loses any regularity but be extremely rigid.</p>
<h3 id="Empirical-error-and-real-error"><a href="#Empirical-error-and-real-error" class="headerlink" title="Empirical error and real error"></a>Empirical error and real error</h3><p>Actually, we have to agree on one thing that we can never know the “real” distribution of a given dataset (otherwise, there is no need to perform machine learning experiment!), so the only perception is how well the model works on such dataset.</p>
<h4 id="Empirical-risk-error"><a href="#Empirical-risk-error" class="headerlink" title="Empirical risk (error)"></a>Empirical risk (error)</h4><p>Empirical risk has a natural definition by simply counting the average misclassified samples:</p>
<script type="math/tex; mode=display">\hat\epsilon(h) = \frac{1}{m} \sum_{i=1}^m 1\{h(x^{(i)}) \neq y^{(i)}\}</script><p>However, what we really care is the following one: <strong>generalization error</strong>, which indicates the real performance on arbitrary data from real distribution.</p>
<script type="math/tex; mode=display">\epsilon(h) = P_{(x,y)\sim D} (h(x)\neq y)</script><p>If we draw a new examle $(x,y)$ from the distribution $D$, $h$ will misclassify it.</p>
<p>However, in order to give a clear insight about the generalization error, we need to talk about the hypotheses under the <strong>PAC</strong> assumptions.</p>
<hr>
<p><strong>Reference</strong>:</p>
<p><a href="http://cs229.stanford.edu/notes/cs229-notes4.pdf" target="_blank" rel="noopener">Stanford CS229-note4</a> by Andrew Ng<br>Statistical Learning methods, by Li Hang (2012)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Learning-theory</tag>
        <tag>Error-estimation</tag>
      </tags>
  </entry>
  <entry>
    <title>Recurrence Relation</title>
    <url>/2020/04/07/recurrence-relation/</url>
    <content><![CDATA[<p>There are basically three ways to determine a sequence $\{a_n,n\ge 0\}$, explicitly or implicitly.</p>
<ul>
<li>Explicit expression as: $a_n = A(n)$</li>
<li>Generation function of $a_n$ as $A(x) = \sum_{n=0}^\infty a_n x^n$</li>
<li>Build the <strong>Recurrence Relation</strong>, which contains<ul>
<li>relation: $a_n = F(a_{n-1},a_{n-2},\dots, a_{n-r};n)$</li>
<li>initial condition: $a_1;a_2;\dots$</li>
</ul>
</li>
</ul>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>For example, the <strong>Hanoi</strong> is a typical problem of it, which has the following recurrence relation:</p>
<script type="math/tex; mode=display">a_n=2a_{n-1} +1, n \ge 2</script><script type="math/tex; mode=display">a_1 = 1</script><p>Iteratively solve the relation, we have explicit form as follow:</p>
<script type="math/tex; mode=display">a_n = 2a_{n-1} + 1 = 2(2a_{n-2}+1) +1 =\dots = 2^n -1</script><p><strong>Fibonacci</strong> and <strong>Catalan</strong> Sequence are also famous examples for recurrence. Especially we can solve the Fibonacci thoroughly:</p>
<h4 id="Fibonacci"><a href="#Fibonacci" class="headerlink" title="Fibonacci"></a>Fibonacci</h4><script type="math/tex; mode=display">f_n = f_{n-1} + f_{n-2}. f_0=f_1= 1</script><p>We can solve the general formula of $f_n$ by generation function.</p>
<p>Let $F(x)= \sum_{n\ge 0} f_n x^n$, then</p>
<script type="math/tex; mode=display">F(x) = f_0+f_1 x + \sum_{n\ge 2} f_nx^n = 1+ x+x(F(x)-1) + x^2F(x) =1+xF(x) + x^2F(x)</script><p>Thus,</p>
<script type="math/tex; mode=display">F(x) = \frac{1}{1-x-x^2} = \frac{1}{\sqrt5} (\frac{\alpha}{1-\alpha x} - \frac{\beta}{1-\beta x}) = \frac{1}{\sqrt5}(
\alpha^{n+1}- \beta^{n+1}) x^n</script><p>where $\alpha={1\over2}(1+\sqrt5), \beta ={1\over 2}(1-\sqrt5)$, the eigenvalue of the recurrence relation.</p>
<p>The practical instance for Fibonacci is the number of $n-$sequence of $\{0,1\}$ with no adjacent $1$, like $1010001\dots$</p>
<h4 id="Catalan"><a href="#Catalan" class="headerlink" title="Catalan"></a>Catalan</h4><p>Catalan describes the possible associative combination of $n$-element string $x_1x_2\dots x_n$, where the associative law is not necessary to satisfy between $xy$.</p>
<p>The recurrence relation is graceful:</p>
<script type="math/tex; mode=display">c_n = c_0 c_{n-1} + c_1 c_{n-2} + \dots+ c_{n-1}c_0 = \sum_{j=0}^{n-1} c_j c_{n-1-j}; ~c_0 = c_1=1</script><h3 id="General-recurrence-relation"><a href="#General-recurrence-relation" class="headerlink" title="General recurrence relation"></a>General recurrence relation</h3><p>Here we briefly introduce several common recurrence relations, where each of them has corresponding general solution.</p>
<h4 id="1-First-order-Linear-recurrence-sequence-LRS"><a href="#1-First-order-Linear-recurrence-sequence-LRS" class="headerlink" title="1. First order Linear recurrence sequence(LRS)"></a>1. First order Linear recurrence sequence(LRS)</h4><script type="math/tex; mode=display">u_n=a(n) u_{n-1} + b(n) ,a(n)\neq 0, n\ge 0</script><p>By dividing $a(n)$, we instead study the recurrence relation of new sequence $\tilde{u_{n}}$ with constant coefficiencts.</p>
<h4 id="2-r-order-linear-recurrence-relation-with-constant-coefficients"><a href="#2-r-order-linear-recurrence-relation-with-constant-coefficients" class="headerlink" title="2. $r$-order linear recurrence relation with constant coefficients"></a>2. $r$-order linear recurrence relation with constant coefficients</h4><p>Just like what we do in the differential equation: homogeneous and non-homogeneous case to discuss.</p>
<script type="math/tex; mode=display">u_n = \sum_{j=1}^r c_j u_{n-j} + g(n)</script><p>where $g(n)$ is the non-homogeneous term, and $c_j$ is irrelevant to $n$.</p>
<p>For example, the Fibonacci $f_n = f_{n-1} + f_{n-2} $ is $2$-order homogeneous LRS.</p>
<p>For $r$-order homogeneous LRS, we have the general solution which is determined by solving eigenpolynomial and eigenvalue.</p>
<script type="math/tex; mode=display">u_n = \sum_{j=1}^r c_j u_{n-j}</script><script type="math/tex; mode=display">c(x ) = x^r - \sum_{j=1}^r c_j x^{r-j} = (x-\alpha_1)^{e_1}\dots (x-\alpha_s)^{e_s}</script><p>where $s$ is the number of distinct solution and $e_s$ is the corresponding multiplicity of $\alpha_s$. Also, $e_1+\dots+e_s=r$.</p>
<p>Then</p>
<script type="math/tex; mode=display">u_n = p_1(n) \alpha_1^n + \dots + p_s(n) \alpha_s^n, ~deg(p_i)<e_i</script><p>And the sumup number of coefficiencts in $p_i(n)$ is $r$ can be uniquely determined by initial value $u_0,u_1,\dots,u_{r-1}$.</p>
<p>For non-homogeneous case, we need to find a special solution that satisfies:</p>
<script type="math/tex; mode=display">u_n=u_n' + \sum_{i=1}^s p_i(n)\alpha_i^n</script><h3 id="Convolutional-recurrence-relation"><a href="#Convolutional-recurrence-relation" class="headerlink" title="Convolutional recurrence relation"></a>Convolutional recurrence relation</h3><p>We define that the sum $\sum_{j=0}^n u_j v_{n-j}$ is the convolution of vector $(u_0,\dots,u_n)^T,(v_0,\dots,v_n)^T$. And such recurrence relation is called Convolutional Form.</p>
<p>As mentioned above, the Catalan Sequence has the following form:</p>
<script type="math/tex; mode=display">c_n =  \sum_{j=0}^{n-1} c_j c_{n-1-j}; ~c_0 =c_1= 1</script><p>We can solve it by generating function and eigenfunction tricks, finally we have:</p>
<script type="math/tex; mode=display">c_n = \frac{1}{n+1} \binom{2n}{n}</script>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Combinatorics</tag>
        <tag>Sequence</tag>
        <tag>Recurrence</tag>
      </tags>
  </entry>
  <entry>
    <title>Generating function of integer partition</title>
    <url>/2020/04/07/integer-partition-gf/</url>
    <content><![CDATA[<p>Partition of a positive number, is also an important counting objective in Combinatorics. As a combination form of counting, it means that the number of possible partitions of $n$. For example,</p>
<script type="math/tex; mode=display">4 = 3+1
=2+2
=2+1+1
=1+1+1+1</script><p>Totally $4$ ways to take the partition, we denoted that $p(4) = 4$, where $p(n)$ is general form of integer partition.</p>
<h3 id="Common-restricted-integer-partitions"><a href="#Common-restricted-integer-partitions" class="headerlink" title="Common restricted integer partitions"></a>Common restricted integer partitions</h3><p>There are several restricted integer partition:</p>
<ul>
<li>$p(n,r)$: counting those with exactly $r$ parts; notice that $p(n) = \sum_k p(n,k)$</li>
<li>$n\in \mathbb N, H\subset \mathbb N \to p_H(n)$, where each part belong to $H$</li>
<li>In particular, $r,n\in \mathbb N \to p_{[r]}(n)$, where each part belong to $[r]$, say each part is less than or equal to $r$</li>
<li>$p_{\neq}(n)$ denotes the integer partition with distinct parts</li>
<li>$p_{odd}(n),p_{even}(n)…$</li>
</ul>
<h3 id="Ferrers-diagram"><a href="#Ferrers-diagram" class="headerlink" title="Ferrers diagram"></a>Ferrers diagram</h3><p>In fact, there is a convenient way to represent any partition straightforward, by Ferrers diagram:</p>
<div align=center>
<img src="ferrers.png" width="40%" height="40%">
</div>


<p>which is a bijective image with $6+4+3+1$.</p>
<h3 id="Generating-Function"><a href="#Generating-Function" class="headerlink" title="Generating Function"></a>Generating Function</h3><p>Integer partition can also be analysed by generating Function, and any restricted partition can be correspondingly transformed to the variant function.</p>
<p>Generally, we write the generating function as follows:</p>
<script type="math/tex; mode=display">P(x) = \sum_{n=0}^\infty p(n)x^n = \prod_{j=1}^\infty (1-x^j)^{-1}</script><p><strong>Why? Let’s see</strong></p>
<p>Expand the $P(x)$:</p>
<script type="math/tex; mode=display">(1+x+x^2+\dots)(1+x^2+x^4+\dots)\dots = \prod_{j=1}^\infty (1-x^j)^{-1}</script><p>The coefficient of $x^n$ is contributed from each factor. Let $x_j$ be the number of parts containing $j$ elements, say $”1”$. Then we transform the known problem into the following Diophantus equation:</p>
<script type="math/tex; mode=display">x_1 + 2x_2 +\dots  = n</script><p>Then the result is clear.</p>
<p>Furthermore, we can add extra restriction by adjust the product form, take $p_H(n)$ for example:</p>
<script type="math/tex; mode=display">P_H(x) = \sum_{n=0}^\infty p_H(n) x^n = \prod_{j\in H} (1-x^j)^{-1}</script><h3 id="Inverse-generating-Function"><a href="#Inverse-generating-Function" class="headerlink" title="Inverse generating Function"></a>Inverse generating Function</h3><p>We define the inverse of $P(x)$ as $Q(x) =\sum_{n=0}^\infty q(n) x^n \equiv \prod_{j\in H} (1-x^j)$, such that $P(x)Q(x)=1$.</p>
<p>The $q(n)$ satisfies the following equation:</p>
<script type="math/tex; mode=display">q(n) = q_0(n) - q_1(n)</script><p>where $q_0(n)$ is the number of partitions of $n$ with distinct even parts while $q_1(n)$ is distinct odd one. Moreover, we have more direct expression of $q(n)$:</p>
<script type="math/tex; mode=display">q(n)=
\begin{cases}
(-1)^k, & \text{when } n=\frac{3k^2\pm k}{2} \\
0 & \text{otherwise}
\end{cases}</script><p>which can be proved by Ferrers diagram through discussion covering each cases.</p>
<h3 id="Euler’s-Identity"><a href="#Euler’s-Identity" class="headerlink" title="Euler’s Identity"></a>Euler’s Identity</h3><p>On top of the explicit expression of $q(n)$ above, we can immediately deduce the following formula:</p>
<h4 id="Theory-1"><a href="#Theory-1" class="headerlink" title="Theory-1"></a>Theory-1</h4><script type="math/tex; mode=display">
\prod_{j=1}^\infty (1-x^j) = \sum_{n=0}^\infty q(n)x^n = 1+ \sum_{k=1}^\infty (-1)^k \left(x^{\frac{3k^2-k}{2}} + x^{\frac{3k^2+k}{2}}\right)</script><script type="math/tex; mode=display">= 1-x-x^2+x^5+x^7-x^{12}-x^{15}+ \dots</script><h4 id="Theory-2"><a href="#Theory-2" class="headerlink" title="Theory-2"></a>Theory-2</h4><p>Because the inverse property of $Q(x)$ by $P(x)Q(x) =1$, with plugged in, we have the further conclusion:</p>
<script type="math/tex; mode=display">p(n) = p(n-1) + p(n-1) - \dots  = \sum_{k=1}^\infty (-1)^{k-1} \left(p(n-\frac{3k^2-k}{2}) + p(n-\frac{3k^2+k}{2}) \right)</script><p>And we let $p(m)\equiv 0, \text{ when } m&lt;0$.</p>
<hr>
<p><strong>References:</strong> Wikipedia: <a href="https://en.wikipedia.org/wiki/Partition_(number_theory" target="_blank" rel="noopener">Partition</a>)</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Combinatorics</tag>
        <tag>Generating function</tag>
        <tag>Partition</tag>
      </tags>
  </entry>
  <entry>
    <title>Parity of permutation</title>
    <url>/2020/04/06/parity-of-p/</url>
    <content><![CDATA[<p>The subgroups of the symmetric group $S(\Omega)$ over $\Omega$ are called transformation group on $\Omega$. In particular, if $\Omega$ as finite set with $n$ elements, then the subgroups are called permutation groups. $\forall \sigma \in S_n$ forms an one-to-one on $\Omega$, which is also a <strong>permutation</strong>.</p>
<script type="math/tex; mode=display">\left(
\begin{array}
{ccc}
1 & 2 & 3 & 4 & 5\\
\sigma(1) & \sigma(2) & \sigma(3) & \sigma(4) & \sigma(5)
\end{array}
\right)</script><p>According to Carley, $\forall $ group $G$ is isomorphic with some transformation group; $\forall $ finite group $G$ is isomorphic with some permutation group. This implies the permutations exhibit great importance in the finite algebra structure.</p>
<h3 id="Property-of-permutation"><a href="#Property-of-permutation" class="headerlink" title="Property of permutation"></a>Property of permutation</h3><p>In fact, we have the following propsition or property of permutation.</p>
<ul>
<li>$\forall \sigma \in S_n$ has disjoint cyclic decomposition, which $\exist ~1$<ul>
<li>cycle can be seen as a special permutation only acting on some elements</li>
</ul>
</li>
<li>If two cycles share no common elements, then they can exchange (exchangable)</li>
<li>$\forall$ $k$-ary cycle can be further decomposed into product of $k-1$ transportation<ul>
<li>$2$-ary cycle is called transportation<script type="math/tex; mode=display">(a_1\dots,a_k) = (a_1a_k)(a_1a_{k-1})\dots(a_1a_2)</script></li>
</ul>
</li>
<li>Any transportation $(i,j)$ can be written as the product of<br>$2|j-i|-1$ adjacent transportations. For example:</li>
</ul>
<script type="math/tex; mode=display">(2~5) = (2~3)(3~4)(4~5)(4~3)(3~2)</script><h3 id="Parity-of-permutation"><a href="#Parity-of-permutation" class="headerlink" title="Parity of permutation"></a>Parity of permutation</h3><p>Actually, we would like to give a definition of the number of transportations after the decomposition of a permutation $\sigma$, due to the good properties of such discussion.</p>
<p>Like the collection of all even permutations constructs a subgroup of $S_n$, denoted as $A_n$ and called $n$-ary <strong>alternating group;</strong> and all the odd permutations form the coset of $A_n$. Also, $A_n$ is the normal subgroup of $S_n$ with $index~=2$.</p>
<p>And let $\sigma_1, \sigma_2$ be even permutation, $\tau_1, \tau_2$ be odd permutation, we have :</p>
<ul>
<li>$\sigma \tau$ is odd</li>
<li>$\sigma_1 \sigma_2$, $\tau_1 \tau_2$ is even</li>
<li>The parity of permutation is equivalent to the parity of the number of decomposed transportations</li>
</ul>
<p>However, we have to prove that such parity is “well-defined”, since the transportation-decomposition is definitely not unique. But we will show that the number of factors in such decomposition holds parity.</p>
<h3 id="Proof-that-the-parity-of-a-permutation-is-well-defined"><a href="#Proof-that-the-parity-of-a-permutation-is-well-defined" class="headerlink" title="Proof that the parity of a permutation is well-defined"></a>Proof that the parity of a permutation is well-defined</h3><p><em>(using polynomial)</em></p>
<p>First we define the parity of a permutation is a sign function as:</p>
<script type="math/tex; mode=display">sgn: S_n \to \{-1,1\}</script><p>where alternating group $A_n$, say the subgroup of all even permutation is the preimage of $+1$.</p>
<p>Then for $\forall \sigma \in S_n$, we define</p>
<script type="math/tex; mode=display">sgn(\sigma) = \frac{P(x_{\sigma(1)}, \dots, x_{\sigma(n)})}{P(x_1,\dots,x_n)}</script><p>where polynomial $P(x_1,\dots,x_n) = \prod_{i&lt;j}(x_i-x_j)$; for example, in the case $n=3$, we have $P(x_1,x_2,x_3) = (x_1-x_2)(x_1-x_3)(x_2-x_3)$.</p>
<p>Apparently there are both identical $\binom{n}{2}$ factors in the denominator and numerator, with <strong>sign</strong> as the only difference. Then we can show that such difference is exactly the sign $sgn(\sigma)$ we have already defined:</p>
<ul>
<li>The permutation $\sigma$ is equivalent to its transportation decomposition, which is not unique though.</li>
<li>Each transportation, as $2$-ary cycle, contribute $-1$ to the whole product, since $(x_i-x_j) = (-1)(x_j-x_i)$</li>
<li>We see that if $\sigma,r$ are two permutation, then:</li>
</ul>
<script type="math/tex; mode=display">sgn(\sigma ~ \tau) = \frac{P(x_{\sigma(\tau(1))}, \dots, x_{\sigma(\tau(n))})}{P(x_1,\dots,x_n)} = \frac{P(x_{\tau(1)}, \dots, x_{\tau(n)})}{P(x_1,\dots,x_n)} \cdot \frac{P(x_{\sigma(\tau(1))}, \dots, x_{\sigma(\tau(n))})}{P(x_{\tau(1)},\dots,x_{\tau(n)})} = sgn(\sigma) sgn(\tau)</script><ul>
<li>Thus, $sgn(\sigma)$ is the same mapping as we defined earlier. And the parity of $\sigma$ is definite because they only have one value and are indeed well-defined.</li>
</ul>
<hr>
<p><strong>Reference</strong><br>Wikipedia: <a href="https://en.wikipedia.org/wiki/Parity_of_a_permutation" target="_blank" rel="noopener">Parity of a permutation</a></p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Abstract-algebra</tag>
        <tag>Group-theory</tag>
        <tag>Permutation</tag>
      </tags>
  </entry>
  <entry>
    <title>State Pattern by OOP in Python3</title>
    <url>/2020/04/04/state-pattern/</url>
    <content><![CDATA[<p>State pattern, or state machine is one of the patterns in <strong>Design pattern</strong>. It is proposed to aviod redundancy of code, if tons of if-else statements are needed, which makes it extremely hard to maintain and read the code.</p>
<p>Here is the solution, by defining the conditions as abstract states, by means of python-class implemented in Python3.</p>
<h2 id="State-Pattern"><a href="#State-Pattern" class="headerlink" title="State Pattern"></a>State Pattern</h2><p>Goal: To handle different states. Create a state machine which is able to perform corresponding operations given state. If-else free.</p>
<p>In this designment, each state instance(object) has only static methods without storing any attribute data.</p>
<h3 id="Primal-class-file-I-O"><a href="#Primal-class-file-I-O" class="headerlink" title="Primal class: file I/O"></a>Primal class: file I/O</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connection</span>:</span></span><br><span class="line">    <span class="comment"># if-else piled up, with states as "open" or "close"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.state = <span class="string">'CLOSED'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state != <span class="string">'OPEN'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line">        print(<span class="string">'reading'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state != <span class="string">'OPEN'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line">        print(<span class="string">'writing'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'OPEN'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Already open'</span>)</span><br><span class="line">        self.state = <span class="string">'OPEN'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'CLOSED'</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Already closed'</span>)</span><br><span class="line">        self.state = <span class="string">'CLOSED'</span></span><br></pre></td></tr></table></figure>
<h3 id="Better-solution"><a href="#Better-solution" class="headerlink" title="Better solution"></a>Better solution</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connection1</span>:</span></span><br><span class="line">    <span class="comment"># define a class for each state</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.new_state(ClosedConnectionState)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">new_state</span><span class="params">(self, newstate)</span>:</span></span><br><span class="line">        self._state = newstate</span><br><span class="line">        <span class="comment"># Delegate to the state class</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.read(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.write(self, data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.open(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._state.close(self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Connection state base class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConnectionState</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(conn, data)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Implementation of different states</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClosedConnectionState</span><span class="params">(ConnectionState)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(conn, data)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(conn)</span>:</span></span><br><span class="line">        conn.new_state(OpenConnectionState)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Already closed'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OpenConnectionState</span><span class="params">(ConnectionState)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(conn)</span>:</span></span><br><span class="line">        print(<span class="string">'reading'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(conn, data)</span>:</span></span><br><span class="line">        print(<span class="string">'writing'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(conn)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Already open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(conn)</span>:</span></span><br><span class="line">        conn.new_state(ClosedConnectionState)</span><br></pre></td></tr></table></figure>
<p>But what is “staticmethod”?</p>
<p>Here is the document in Python3.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class staticmethod(object)</span><br><span class="line"> |  staticmethod(function) -&gt; method</span><br><span class="line"> |</span><br><span class="line"> |  Convert a function to be a static method.</span><br><span class="line"> |</span><br><span class="line"> |  A static method does not receive an implicit first argument.</span><br><span class="line"> |  To declare a static method, use this idiom:</span><br><span class="line"> |</span><br><span class="line"> |       class C:</span><br><span class="line"> |           @staticmethod</span><br><span class="line"> |           def f(arg1, arg2, ...):</span><br><span class="line"> |               ...</span><br><span class="line"> |</span><br><span class="line"> |  It can be called either on the class (e.g. C.f()) or on an instance</span><br><span class="line"> |  (e.g. C().f()).  The instance is ignored except for its class.</span><br><span class="line"> |</span><br><span class="line"> |  Static methods in Python are similar to those found in Java or C++.</span><br><span class="line"> |  For a more advanced concept, see the classmethod builtin.</span><br></pre></td></tr></table></figure>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = Connection()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c._state</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">ClosedConnectionState</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="class"><span class="title">Traceback</span> <span class="params">(most recent call last)</span>:</span></span><br><span class="line">    File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    File <span class="string">"example.py"</span>, line <span class="number">10</span>, <span class="keyword">in</span> read</span><br><span class="line">        <span class="keyword">return</span> self._state.read(self)</span><br><span class="line">    File <span class="string">"example.py"</span>, line <span class="number">43</span>, <span class="keyword">in</span> read</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'Not open'</span>)</span><br><span class="line">RuntimeError: Not open</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.open()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c._state</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">OpenConnectionState</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="class"><span class="title">reading</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">write</span><span class="params">(<span class="string">'hello'</span>)</span></span></span><br><span class="line"><span class="class"><span class="title">writing</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">close</span><span class="params">()</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">c</span>.<span class="title">_state</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> '<span class="title">__main__</span>.<span class="title">ClosedConnectionState</span>'&gt;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>Reference</strong>: <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener">_David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition_</a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Python3</tag>
        <tag>OOP</tag>
        <tag>Design-pattern</tag>
      </tags>
  </entry>
  <entry>
    <title>Monte-Carlo and Temporal-Difference Learning</title>
    <url>/2020/04/03/mctd-evaluation/</url>
    <content><![CDATA[<p>Model-free prediction (evaluation) is to estimate the value function of an unknown MDP, say $\langle S,A,P,R,\gamma \rangle$. And it includes the Monte-Carlo learning and temporal-difference learning methods.</p>
<h2 id="Monte-Carlo-Reinforcement-learning"><a href="#Monte-Carlo-Reinforcement-learning" class="headerlink" title="Monte-Carlo Reinforcement learning"></a>Monte-Carlo Reinforcement learning</h2><p>As a famous stochastic simulation method, MC methods plays an important role in a wide range of scientific problems. In reinforcement learning, MC-based methods learn directly from episodes of experience, say a whole observing trajectory.</p>
<ul>
<li>Model-free: have no prior knowledge of MDP transitions/ rewards.</li>
<li>Value = average <strong>sample</strong> returns<ul>
<li>i.e., use arithmetic mean to replace the real expectation</li>
</ul>
</li>
<li>Requirement of applying MC: all episodes <strong>must terminate</strong></li>
</ul>
<h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><ul>
<li><p>Goal: learn latent(real) value function $v_\pi$ <strong>from episodes</strong> of experience under policy $\pi$</p>
</li>
<li><p>Originally, the return at time $t$(also, state $s$) is the total discounted $\gamma$ rewards:</p>
<ul>
<li>$G_t = R_{t+1} + \dots + \gamma^{T-t-1}R_T$</li>
</ul>
</li>
<li><p>And corresponding value function is the <strong>expected return</strong>:</p>
</li>
</ul>
<script type="math/tex; mode=display">v_\pi(s) = \mathbb{E}_\pi [G_t|S_t=s]</script><h3 id="Algorithm-First-visit-MC-policy-evaluation"><a href="#Algorithm-First-visit-MC-policy-evaluation" class="headerlink" title="Algorithm: First-visit MC policy evaluation"></a>Algorithm: First-visit MC policy evaluation</h3><p>The MC algorithm for prediction is to evaluate each state $s$ given policy $\pi$. Here, the first-visit MC is introduced, which means we only take account of the <strong>first time-step $t$</strong> that state $s$ is visit in some episode. We simply count the appearing times with its return $S(s)$, where we compute the return(adding dicounted rewards along the trajectory). Then estimate the value by $V(s) = S(s)/N(s)$. By law of large numbers,</p>
<script type="math/tex; mode=display">V(s) \to v_\pi(s) ~ as ~ N(s) \to \infty</script><p><img src="fvmc.png" alt="alt fvmc"></p>
<h3 id="Every-visit-Monte-Carlo-Policy-Evaluation"><a href="#Every-visit-Monte-Carlo-Policy-Evaluation" class="headerlink" title="Every-visit Monte-Carlo Policy Evaluation"></a>Every-visit Monte-Carlo Policy Evaluation</h3><p>Another way to consider is that every time-step t that state s is visited in an episode. We will also obtain the results.</p>
<h3 id="Incremental-Mean"><a href="#Incremental-Mean" class="headerlink" title="Incremental Mean"></a>Incremental Mean</h3><p>If we rewrite the mean as :</p>
<script type="math/tex; mode=display">\mu_k = {1\over k}\sum_{j=1}^k x_j = \mu_{k-1} + {1\over k}(x_k - \mu_{k-1})</script><p>Then the update form can be changed into:</p>
<p>$N(S_t) := N(S_t) + 1$</p>
<p>$V(S_t) := V(S_t) + {1\over N(S_t)}(G_t - V(S_t))$</p>
<p>which is the increment Monte-Carlo Updates.</p>
<p>In non-stationary problems, where episodes may not provide “consistent” information, it can be useful to track a runing mean, i.e. forget old episodes.</p>
<p>$V(S_t) := V(S_t) + \alpha (G_t - V(S_t))$</p>
<h2 id="Temporal-Difference-learning"><a href="#Temporal-Difference-learning" class="headerlink" title="Temporal-Difference learning"></a>Temporal-Difference learning</h2><ul>
<li>Model-free also</li>
<li>Learning from incomplete episodes, by bootstrapping(update involving estimate)</li>
<li>Updates a guess towards a guess!</li>
</ul>
<h3 id="The-difference-between-TD-and-MC"><a href="#The-difference-between-TD-and-MC" class="headerlink" title="The difference between TD and MC"></a>The difference between TD and MC</h3><p>Take Incremental every-visit MC for example,</p>
<p>We update value $V(S_t)$ toward actual return $G_t$ as :</p>
<script type="math/tex; mode=display">V(S_t) := V(S_t) + \alpha (G_t - V(S_t))</script><p>In TD(0) — simplest TD algorithm, we replace the $G_t$ with a guess:</p>
<script type="math/tex; mode=display">V(S_t) := V(S_t) + \alpha (R_{t+1}+\gamma V(S_{t+1}) - V(S_t))</script><p>where $R_{t+1}+\gamma V(S_{t+1}$ is called the <strong>TD target;</strong>R_{t+1}+\gamma V(S_{t+1}) - V(S_t) is called the <strong>TD error</strong>.</p>
<p>And the rest of algorithm is similar.</p>
<h3 id="Algorithm-TD-0-for-estimating-v-pi"><a href="#Algorithm-TD-0-for-estimating-v-pi" class="headerlink" title="Algorithm: TD(0) for estimating $v_\pi$"></a>Algorithm: TD(0) for estimating $v_\pi$</h3><p><img src="td.png" alt="alt td"></p>
<h3 id="Remarks-of-TD"><a href="#Remarks-of-TD" class="headerlink" title="Remarks of TD"></a>Remarks of TD</h3><ul>
<li>return $G_t = R_{t+1} + \dots + \gamma^{T-t-1}R_T$ is <strong>unbiased estimate</strong> of $v_\pi(S_t)$</li>
<li>true TD target (over $v_\pi$), $R_{t+1}+\gamma v_\pi(S_{t+1}$ is also unbiased estimate of $v_\pi(S_t)$</li>
<li>whileTD target (over $v_\pi$), $R_{t+1}+\gamma v_\pi(S_{t+1}$ is also biased estimate</li>
<li>TD target has much lower variance than the return</li>
<li>however, TD target estimate has bias;<ul>
<li>MC has better convergence properties</li>
</ul>
</li>
</ul>
<p>Note that:</p>
<ul>
<li>MC is not very sensitive to initial value</li>
<li>TD is more <strong>sensitive</strong></li>
</ul>
<h2 id="Batch-MC-and-TD"><a href="#Batch-MC-and-TD" class="headerlink" title="Batch MC and TD"></a>Batch MC and TD</h2><p>If we only have finite observation data, say $K$ episodes totally:</p>
<script type="math/tex; mode=display">s_1^1, a_1^1, r_2^1 \dots, s_{T_1}^1</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">s_1^K, a_1^K, r_2^K \dots, s_{T_1}^K</script><p>Then we can <strong>repeatedly sample episode</strong> $k\in [1,K]$ and apply MC or TD(0) to episode k. It still works good!</p>
<p>For certainty equivalence, we can prove that the batch MC or TD:</p>
<ol>
<li>MC converges to solution with <strong>minimum mean-squared error</strong>(MSE), and best fit to the <strong>observed returns</strong>:</li>
</ol>
<script type="math/tex; mode=display">\sum_{k=1}^K \sum_{t=1}^{T_k} (G_t^k - V(s_t^k))^2</script><ol>
<li>TD converges to solution of <strong>maximum likelihood Markov model</strong>, and solution to the MDP model $\langle S,A,P,R,\gamma \rangle$ best fits the observed data, say sequence.( common MLE )</li>
</ol>
<script type="math/tex; mode=display">P^a_{s,s'} = {1\over N(s,a)} \sum_{k=1}^K \sum_{t=1}^{T_k} 1 \{s_t^k,a_t^k,s_{t+1}^k = s,a,s'\}</script><script type="math/tex; mode=display">R_s^a = {1\over N(s,a)}\sum_{k=1}^K \sum_{t=1}^{T_k} 1\{s_t^k,a_t^k = s,a\} r_t^k</script><h2 id="Summary-and-comparison"><a href="#Summary-and-comparison" class="headerlink" title="Summary and comparison"></a>Summary and comparison</h2><p>Generally speaking, TD <strong>exploits Markov property</strong> (one-step determination) and usually more <strong>efficient in Markov env</strong>; while MC does not exploit and is usually used in non-Markov env.</p>
<h3 id="Diagram-explanation"><a href="#Diagram-explanation" class="headerlink" title="Diagram explanation"></a>Diagram explanation</h3><ul>
<li>For Monte-Carlo: $V(S_t) := V(S_t) + \alpha (G_t - V(S_t))$</li>
</ul>
<p><img src="mctree.png" alt="alt mctree"></p>
<ul>
<li>For Temporal-Difference: $V(S_t) := V(S_t) + \alpha (R_{t+1}+\gamma V(S_{t+1}) - V(S_t))$</li>
</ul>
<p><img src="tdtree.png" alt="alt tdtree"></p>
<ul>
<li>Also, here is how Dynamic Programming algorithm works:</li>
</ul>
<p><img src="dptree.png" alt="alt dptree"></p>
<hr>
<p><strong>Reference</strong>: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Reinforcement-learning</tag>
        <tag>Monte-Carlo</tag>
      </tags>
  </entry>
  <entry>
    <title>Generating Function</title>
    <url>/2020/04/01/genefunction/</url>
    <content><![CDATA[<p>A series of numbers shared similar features, can often be described as coefficients of some function expansion. Then we will call such function the <strong>generating function</strong>.</p>
<h2 id="generating-Function"><a href="#generating-Function" class="headerlink" title="generating Function"></a>generating Function</h2><p>In order to obtain a sequence ${a_k: k \geq 0}$, we can represent it by a power series:</p>
<script type="math/tex; mode=display">g(x) = \sum_{i=0}^\infty a_k x^k = a_0 + a_1 x + a_2 x^2 + \dots</script><p>If such representation holds, then the analysis of such sequence( often only for specific term of such sequence ) can now be changed into the analysis of its generating function.</p>
<h3 id="Straightforward-example"><a href="#Straightforward-example" class="headerlink" title="Straightforward example"></a>Straightforward example</h3><ol>
<li>Given $n$, the coefficients of binomial have the form:</li>
</ol>
<script type="math/tex; mode=display">\sum_{i=0}^\infty \binom{n}{k} x^k = (1+x)^n</script><ol>
<li>$k$-ary repeatable combination over $n$-ary set, i.e., to select $k$ elements from $n$ different ones when repetition is allowed; or the number of non-negative integer solutions to equation $x_1+\dots + x_n = k$:</li>
</ol>
<script type="math/tex; mode=display">\left( \binom{n}{k}\right) = \binom{n+k-1}{k}</script><script type="math/tex; mode=display">g(x) = (1-x)^{-n} =\sum_{i=0}^\infty \left( \binom{n}{k}\right) x^k</script><p>Hint: consider the generating process of term $x^k$ and what the number $\left( \binom{n}{k}\right) = \binom{n+k-1}{k}$ describes.</p>
<ol>
<li>Constrained diophantine equation</li>
</ol>
<p>Similar to $case 2$, write the expansion series as polynomial or incomplete series.</p>
<p>In general, to select $k$ elements from $n$-ary set where the number of element $a_i$ is constrained as set $M_i$. The number of such combinations is denoted as $c_k$, then its generating function is:</p>
<script type="math/tex; mode=display">g(x) = (1-x)^{-n} =\sum_{i=0}^\infty c_k x^k = \prod_{i=1}^n \left( \sum_{m\in M_i} x^m \right)</script><p>By detailedly observing the form of the R.H.S. above, it makes total sense as an enumeration.</p>
<p>Many cases from this pattern can construct one-to-one with a lot of counting problems.</p>
<h3 id="Exponential-generating-function"><a href="#Exponential-generating-function" class="headerlink" title="Exponential generating function"></a>Exponential generating function</h3><p>There is a useful type of generating function as exponential generating function. The only difference with general form above is the factorial denominator $k!$.</p>
<p>Exponential generating function is introduced to solve <strong>permutation counting problem.</strong> It has the following general form:</p>
<script type="math/tex; mode=display">\sum_{i=0}^n a_k \frac{x^k}{k!} = a_0 + a_1 x + a_2 x^2 + \dots</script><ol>
<li>The $k$-ary unrepeatable permutations of $n$-ary set is $(n)_k$, and</li>
</ol>
<script type="math/tex; mode=display">\sum_{i=0}^n (n)_k \frac{x^k}{k!} = (1+x)^n</script><ol>
<li>common Exp generatingf function</li>
</ol>
<ul>
<li>$\{a_k=1, k \geq 0\}$ —- $e^x$;</li>
<li>$\{a_k=(-1)^k, k \geq 0\}$ —- $e^{-x}$;</li>
<li>$\{a_k={k!}, k \geq 0\}$ —- $1\over 1-x$;</li>
</ul>
<ol>
<li>To make $k$-ary permutation from $n$-ary set $S=\{ b_1,b_2,\dots, b_n\}$ where the number of element $b_i$ is constrained as set $M_i, m_i\in M_i$. The number of such combinations is denoted as $p_k$, then its generating function is:</li>
</ol>
<script type="math/tex; mode=display">\sum_{i=0}^\infty p_k {x^k\over k!} = \prod_{i=1}^n \left( \sum_{m\in M_i} {x^m\over m!} \right)</script><p>where the coefficient of $x_k\over k!$ is</p>
<script type="math/tex; mode=display">\sum_{m1+\dots+m_n=k, m_i \in M_i} \frac{k!}{m_1!\dots m_n!}</script><p>In particular, if $M_i = \mathbb{N_0}$, then the exponential generating function is $(\sum_{j=0}^\infty \frac{x^j}{j!})^n = e^{nx}$, which is equivalent to that of $n^k$:</p>
<script type="math/tex; mode=display">\sum_{i=0}^\infty n^k {x^k\over k!}</script><h3 id="Allocation-problem"><a href="#Allocation-problem" class="headerlink" title="Allocation problem"></a>Allocation problem</h3><p>In fact, there exists an one-to-one between permutation and allocation: $k$-ary repeatable permutation is equivalent to the allocation of $k$ distinguishable balls into $n$ distinguishable boxes. We may also add some constraints to the two primal problems to make it more complex.</p>
<h3 id="Exp-generating-function-of-Stirling-numbers"><a href="#Exp-generating-function-of-Stirling-numbers" class="headerlink" title="Exp. generating function of Stirling numbers"></a>Exp. generating function of Stirling numbers</h3><p><strong>Prop1</strong><br>The exponential generating function of Stirling numbers of 2nd kind:</p>
<script type="math/tex; mode=display">\sum_{n=0}^\infty S(n,k) \frac{x^n}{n!} = {1\over k!}(e^x -1 )^k</script><p><strong>Corollary1</strong></p>
<script type="math/tex; mode=display">S(n,k) = {1\over k!} \sum_{i=0}^k (-1)^i \binom{k}{i} (k-i)^n</script><p>Correspondingly, we have the form for 1st kind Stirling.</p>
<p><strong>Prop2</strong><br>The exponential generating function of Stirling numbers of 2nd kind:</p>
<script type="math/tex; mode=display">\sum_{n=0}^\infty s(n,k) \frac{x^n}{n!} = {1\over k!}(ln(x+1) )^k</script>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Combinatorics</tag>
        <tag>Generating function</tag>
        <tag>Counting</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of SVM-(4)</title>
    <url>/2020/03/31/svm4/</url>
    <content><![CDATA[<h2 id="Soft-Margin-non-separable-dataset"><a href="#Soft-Margin-non-separable-dataset" class="headerlink" title="Soft Margin: non-separable dataset"></a>Soft Margin: non-separable dataset</h2><p>SVM works with a prerequisite that the given dataset is linearly separable. Even the kernel trick provides an incredible approach to help increase the likelihood that that data is separable, but not always does. Generally, a speck of noise samples, say outliers, will fail the SVM due to its hard requirement $y^{(i)}(w^T x^{(i)} + b) \geq 1$. In order to improve this, we introduce the regularized (slacking) term $\xi_i$, such that the problem has changed into:</p>
<script type="math/tex; mode=display">\min_{w,b} {1\over 2} ||w||^2 + C \sum_{i=1}^m \xi_i</script><script type="math/tex; mode=display">s.t. \; y^{(i)} (w^Tx^{(i)} + b ) \geq 1- \xi_i,</script><script type="math/tex; mode=display">\xi_i\geq 0,  i =1,2,\dots,m</script><p>On top of that, examples are now permitted to have (functional) margin less than $1$. However, the “permission” will add extra penalty to the objective function by $C\xi_i$, which is a trade-off.</p>
<p>We can form the Lagrangian:</p>
<script type="math/tex; mode=display">L(w,b,\xi,\alpha,r) = {1\over 2} ||w||^2 + {C\over 2} \sum_{i=1}^m \xi_i^2 - \sum_{i=1}^m \alpha_i [y^{(i)}(w^T x^{(i)} + b) - 1 + \xi_i] -\sum_{i=1}^m r_i \xi_i</script><p>where $\alpha_i, r_i$ are the Lagrange multipliers.</p>
<p>The dual form of the problem:</p>
<script type="math/tex; mode=display">\max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i  - {1\over 2} \sum_{i=1}^{m} \sum_{j=1}^m \alpha_i \alpha_j y_i y_j K(x_i, x_j)</script><script type="math/tex; mode=display">s.t. \; 0 \leq \alpha_i \leq C, i=1,\dots,m</script><script type="math/tex; mode=display">\sum_{i=1}^m \alpha_iy^{(i)} = 0</script><p>By solving which, we can also obtain the optimal classifier like what we do in the linear separable scenario.</p>
<p><strong>Remarks</strong>:</p>
<ul>
<li>$C$, as the penalty factors, is a hyperparameter in SVM. It indicate the measure of the penalty of wrongly classified samples;</li>
<li>When $C$ increase, the contribution from wrongly classified samples will flood the primal objective, thus “caring the noises” too much, which is inclined to overfit.<ul>
<li>If $C$ goes to $\infty$, then it recovers to the hard margin: now we build the hyperplane <strong>along the contour of the “real” boundary</strong>.</li>
<li>If $C$ approaches $0$, it confines the $\alpha_i$ too hard around $0$ and ignores the outliers.</li>
</ul>
</li>
<li>Now the minimize of objective include:<ul>
<li>maximize the margin (w.r.t $w$)</li>
<li>minimize the number of wrongly classified points</li>
</ul>
</li>
</ul>
<h2 id="SMO-algorithm"><a href="#SMO-algorithm" class="headerlink" title="SMO algorithm"></a>SMO algorithm</h2><h3 id="Coordinate-ascent"><a href="#Coordinate-ascent" class="headerlink" title="Coordinate ascent"></a>Coordinate ascent</h3><p>Introduce by John Platt, SMO (sequential minimal optimization) algorithm gives an efficient way of solving the dual problem. It mainly focuses on how we utilize the samples to update the parameters.</p>
<p>Consider the following unconstrained optimization problem:</p>
<script type="math/tex; mode=display">\max_\alpha W(\alpha_1,\dots,\alpha_m)</script><p>The SMO algorithm uses so-called <strong>coordinate ascent</strong> to finish the update:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Loop until convergence:&#123;</span><br><span class="line">  For i &#x3D; 1,...,m&#123;</span><br><span class="line">    ai :&#x3D; argmax_ai W(a1,...,ai-1,a^i,ai+1,...,am)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In the inner loop, we hold all the variables fixed except for some $\alpha_i$, and re-optimize $W$ w.r.t just the parameters $\alpha_i$.</p>
<p><img src="ca.png" alt="alt ca"></p>
<h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>Coordinate ascent gives a powerful tools for update the parameters, but it fails for most constrained problems like our dual optimization problems due to the equality.</p>
<script type="math/tex; mode=display">\max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i  - {1\over 2} \sum_{i=1}^{m} \sum_{j=1}^m \alpha_i \alpha_j y_i y_j K(x_i, x_j)</script><script type="math/tex; mode=display">s.t. \; 0 \leq \alpha_i \leq C, i=1,\dots,m</script><script type="math/tex; mode=display">\sum_{i=1}^m \alpha_iy^{(i)} = 0</script><p>However, by altering a bit, we can let it work greatly on the SVM problem. In order not to violate the condition $\sum_{i=1}^m \alpha_iy^{(i)} = 0$, we may update two parameters at the same time:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Repeat till convergence &#123;</span><br><span class="line">  1. Select some pair ai and aj to update next (using a</span><br><span class="line">     heuristic that tries to pick the two that will allow us to</span><br><span class="line">      make the biggest progress towards the global maximum).</span><br><span class="line">  2. Reoptimize W(a) with respect to ai and aj, while holding</span><br><span class="line">   all the other ak’s (k ̸&#x3D; i, j) fixed.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Compared to the coordinate ascent, we instead hold $\alpha_3,\dots,\alpha_m$ fixed and re-optimize $W$ w.r.t $\alpha_1, \alpha_2$:</p>
<script type="math/tex; mode=display">\alpha_1 y_1+\alpha_2 y_2 = \zeta</script><script type="math/tex; mode=display">0 \leq \alpha_1 \leq C</script><script type="math/tex; mode=display">0 \leq \alpha_2 \leq C</script><h3 id="Hinge-loss-function"><a href="#Hinge-loss-function" class="headerlink" title="Hinge loss function"></a>Hinge loss function</h3><p>Let’s look back to the primal optimization problem. We can interpret the linear SVM in another way, to minimize the following (loss) objective funtion:</p>
<script type="math/tex; mode=display">\sum_{i=1}^m [1- y^{(i)}(w^T x^{(i)} + b )]_+ \lambda ||w||^2</script><p>where $[z]_+$ is called hinge loss function, which is defined as:</p>
<script type="math/tex; mode=display">[z]_+=
\begin{cases}
z, & \text{ z > 0}\\
0 & \text{otherwise}
\end{cases}</script><p>This is to say, if given sample is correctly classified and has margin larger than $1$, the loss will be zero; on the contrary, it will be $1- y^{(i)}(w^T x^{(i)} + b)$.</p>
<p>The second term $\lambda ||w||^2$ is the $l_2$ regularized term.</p>
<p><img src="lossf.png" alt="alt lossf"></p>
<p>From the figure above, we can see that hinge loss function is one of the continuous upper bound of $0-1$ loss. The logistic loss function decreases after the $1$, which means giving regard to <strong>every samples</strong> even they are far away from the decision boundary.</p>
<p>$0-1$ loss function in fact exhibits the “real loss” but its uncontinuity gives rise to the difficulty for update.</p>
<hr>
<p><strong>Reference</strong>:<br><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener">Stanford CS229-note3</a> by Andrew Ng<br>Statistical Learning methods, by Li Hang (2012)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Support-vector-machine</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Subgroup</title>
    <url>/2020/03/30/subgroup/</url>
    <content><![CDATA[<p>An important concept in the group theory is the <strong>Subgroup</strong>. As part of a given group, its subgroup holds the algebra structure and has a close relation to the original group.</p>
<h2 id="Introduction-to-subgroup-with-relevant-concepts"><a href="#Introduction-to-subgroup-with-relevant-concepts" class="headerlink" title="Introduction to subgroup with relevant concepts"></a>Introduction to subgroup with relevant concepts</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Let $(G,~\cdot)$ be a group and $H$ is a non-empty subset of it. If the $\cdot$ is closed operation in $H$, then $(H,~\cdot)$ also forms a group, and called subgroup of $G$. We denote this relation as $H \leq G$.</p>
<p>Remarks:</p>
<ul>
<li>Group $G$ always conceives subgroups<ul>
<li>at least $G$ itself and $\{e\}$, which are called <strong>trivial subgroup</strong> of $G$</li>
<li>except for $G$, any subgroup of $G$ is called proper subgroup(like proper set)</li>
</ul>
</li>
</ul>
<h3 id="Propsition"><a href="#Propsition" class="headerlink" title="Propsition"></a>Propsition</h3><h4 id="Shared-inverse-and-identity-element"><a href="#Shared-inverse-and-identity-element" class="headerlink" title="Shared inverse and identity element"></a>Shared inverse and identity element</h4><p><strong>Prop1</strong> Any subgroup $H$ shares the identity element $e$ with $G$; any elements $h\in H$ has the same inverse element in $H$ as in $G$.</p>
<h4 id="Equivalent-statement"><a href="#Equivalent-statement" class="headerlink" title="Equivalent statement"></a>Equivalent statement</h4><p><strong>Prop2</strong> $H$, a non-empty subset of group $G$, is subgroup if and only if: $\forall a,b \in H$, then $ab^{-1} \in H$.</p>
<p><strong>Variant</strong> Let $ H \subset G, H \leq G \Leftrightarrow HH^{-1} = H$</p>
<h3 id="Subset-generated-subgroup"><a href="#Subset-generated-subgroup" class="headerlink" title="Subset-generated subgroup"></a>Subset-generated subgroup</h3><p><strong>Def</strong> Let $G$ be group, and $M$ is its subset. The least subgroup containing $M$, called <em>subgroup generated by $M$</em>, is denoted as $\langle M \rangle$; correspondingly, we name $M$ the <em>generating set</em> of $\langle M \rangle$.</p>
<ul>
<li>In particular, when $M$ contains only a single element $m$, then $\langle M \rangle$ is called <strong>cyclic group</strong>.<ul>
<li>For instance, $\mathbb Z$ can be seen as $1$-generated infinitecyclic group; $\mathbb Z_n$, the multiplicative group of integers modulo $n$, is generated by $\bar 1$ with order $n$.</li>
<li>Essentially, any cyclic group is isomorphic with one of them.</li>
</ul>
</li>
</ul>
<h3 id="The-order-of-element-in-group"><a href="#The-order-of-element-in-group" class="headerlink" title="The order of element in group"></a>The order of element in group</h3><p><strong>Def</strong> Let $a\in G$ be an element in group $G$. The least positive number $m$ rendering $a^m = e$ is called the order of $a$, which is denoted as $o(a)$.</p>
<p>Remarks:</p>
<ul>
<li>$o(a) = 1$ if and only if $a=e$</li>
<li>inverse element $a^{-1} =  a^{n-1}$</li>
<li>if $o(a) = \infty$, then $ m \neq s \Leftrightarrow a^m \neq a^s$</li>
</ul>
<h4 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h4><p><strong>Property 1</strong></p>
<script type="math/tex; mode=display">a^n = e \Rightarrow o(a)~|~n</script><p><strong>Property 2</strong></p>
<script type="math/tex; mode=display">o(a)< \infty, ~ o(a) ~ | ~|G|</script><p><strong>Property 3</strong></p>
<script type="math/tex; mode=display">o(a) < \infty, ~ o(a^t) = \frac{o(a)}{(t,o(a))}</script><p><strong>Property 4</strong></p>
<p>If $ o(a) = m &lt;\infty, o(b) = n &lt; \infty$, and $ab=ba$, $gcd(m,n)=1$, then $o(ab)=mn$</p>
<ul>
<li><em>Corollary1</em>: $o(a) = st \Rightarrow o(a^s) = t, (s,t\in \mathbb N_+)$</li>
<li><em>Corollary2</em>: $o(a) = n$, then $o(a^k) =n \Leftrightarrow gcd(k,n)=1$</li>
</ul>
<p>Remarks:</p>
<ul>
<li><p>In order to prove $o(a) = n$, we may prove $o(a)|n$ and $n|o(a)$</p>
</li>
<li><p>When solving divisible problem, we can write any number as $m=qn+r$</p>
</li>
</ul>
<h3 id="Common-notation-Descartes-product-of-subgroup"><a href="#Common-notation-Descartes-product-of-subgroup" class="headerlink" title="Common notation: (Descartes) product of subgroup"></a>Common notation: (Descartes) product of subgroup</h3><p>Let $H,K$ be subsetsof group $G$, then we denote:</p>
<script type="math/tex; mode=display">HK = \{hk,h\in H,k\in k\}</script><script type="math/tex; mode=display">H^{-1}=\{h^{-1}, h\in H\}</script><script type="math/tex; mode=display">aK = \{ak,k\in k\}, a\in G</script><p>Remarks:</p>
<ul>
<li>Not an one-to-one, we only require the existence</li>
<li>If $H$ is a group, then $H=H^{-1}$</li>
</ul>
<h4 id="Propsition-1"><a href="#Propsition-1" class="headerlink" title="Propsition"></a>Propsition</h4><p>Let $H,K$ be subgroups of $G$, then</p>
<script type="math/tex; mode=display">HK \leq G \Leftrightarrow HK = KH</script><h3 id="Common-types-of-subset"><a href="#Common-types-of-subset" class="headerlink" title="Common types of subset"></a>Common types of subset</h3><h4 id="The-Center-of-group"><a href="#The-Center-of-group" class="headerlink" title="The Center of group"></a>The Center of group</h4><p><strong>Def</strong> Let G be group, then the following subgroup</p>
<script type="math/tex; mode=display">C(G) = \{g\in G | gx=xg, \forall x \in G\}</script><p>is called the center of G.</p>
<p>Remarks:</p>
<ul>
<li>$C(G)$ is the collection of those which are exchangable with any elements in $G$</li>
<li>At least $e \in C(G)$</li>
<li>If $C(G) = G$, then $G$ is Abel Group</li>
<li>The order of $C(G)$ gives a measure of $G$’s exchangeability</li>
<li>Any subgroup $H$ of $G$, satisfies $HC(G) = C(G)H$</li>
</ul>
<h4 id="Centralizer"><a href="#Centralizer" class="headerlink" title="Centralizer"></a>Centralizer</h4><p><strong>Def</strong> Let $a\in G$, then subgroup</p>
<script type="math/tex; mode=display">C_G(a) = \{g\in G | ga = ag\}</script><p>is called the <strong>centralizer</strong> of $a$ in $G$, immediately we also have:</p>
<script type="math/tex; mode=display">C(G) = \bigcap_{a\in G} C_G(a)</script><p>(The centralizer of $a$ is a collection of those are exchangable with $a$ in $G$.)</p>
<p><strong>Property</strong></p>
<script type="math/tex; mode=display">|G| = \sum_{a\in C(G)} \frac{|G|}{|C_G(a)|} + \sum_{a\notin C(G)} \frac{|G|}{|C_G(a)|}</script><p>( <em>The first term: if $a\in C(G)$, then $C_G(a) = G$, so that $\sum 1$ -&gt; the order of center</em> )</p>
<h4 id="Normalizer-and-normal-subgroup"><a href="#Normalizer-and-normal-subgroup" class="headerlink" title="Normalizer and normal subgroup"></a>Normalizer and normal subgroup</h4><p><strong>Def</strong> Let subset $A \subset G$, then subgroup</p>
<script type="math/tex; mode=display">N_G(A) = \{g\in G | gA = Ag\}</script><p>is called the <strong>normalizer</strong> of set $A$ in $G$.</p>
<p><strong>Def</strong> Let $G$ be group, $N \leq G$, if:</p>
<script type="math/tex; mode=display">gN=Ng, \forall g \in G</script><p>or equivalently,</p>
<script type="math/tex; mode=display">gNg^{-1} = N, \forall g \in G</script><p>Then $N$ is the normal subgroup of $G$, denoted as  $N \triangleleft G$</p>
<p>(Corresponding to center, here we only require the <strong>existence</strong> of exchange result.)</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Group-theory</tag>
        <tag>Group</tag>
      </tags>
  </entry>
  <entry>
    <title>Policy iteration by dynamic programming</title>
    <url>/2020/03/29/policy-iteration/</url>
    <content><![CDATA[<p>There is a straightforward method to help agent solve the optimal policy and the idea behind is dynamic programming(DP).</p>
<h2 id="What-is-Dynamic-Programming"><a href="#What-is-Dynamic-Programming" class="headerlink" title="What is Dynamic Programming?"></a>What is Dynamic Programming?</h2><ul>
<li>A method for solving complex problem</li>
<li>By breaking down into subproblems<ul>
<li>solve the subproblems bit by bit</li>
<li>combine all solutions to subproblems</li>
</ul>
</li>
</ul>
<p>However, not every best solution can be found by DP, and two properties are required:</p>
<ul>
<li>Optimal substructure<ul>
<li>prerequisite for decomposition</li>
</ul>
</li>
<li>Overlapping subproblems<ul>
<li>subproblems recur many times</li>
<li>solution can be <strong>cached and reused</strong></li>
</ul>
</li>
</ul>
<p>Luckily, the <a href="https://jiaruilu.com/2020/03/20/2020-03-20-mdp" target="_blank" rel="noopener">Markov decision process(MDP)</a> satisfy both properties:</p>
<ul>
<li>Bellman equation give <strong>recursive decomposition</strong></li>
<li>Value function <strong>stores and reuses solutions</strong></li>
</ul>
<p>In the rest of sections, we will talk about how the solution is obtained through iterations. It looks just as simple as fix-point iteration.</p>
<p>Classified by our application purpose, the DP algorithm can basically have two forms:</p>
<ul>
<li>For prediction (policy evalution):<ul>
<li>Input: MDP $\langle S,A,P,R,\gamma \rangle$, policy $\pi$ or MRP $\langle S,P^\pi,R^\pi,\gamma \rangle$</li>
<li>Output: value function given $\pi$, v_\pi</li>
</ul>
</li>
<li>For control(optimization):<ul>
<li><ul>
<li>Input: MDP $\langle S,A,P,R,\gamma \rangle$</li>
</ul>
</li>
<li>Output: optimal value function v_\ast and optimal policy $\pi$</li>
</ul>
</li>
</ul>
<h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><ul>
<li>Problem: evaluate a given policy $\pi$</li>
<li>Solution: iterative application of Bellman expectation backup</li>
<li>Routine: $v_1\to v_2 \to \dots \to v_\pi$(end)</li>
<li>Synchronous backups: $k+1$-stage only use $k$-stage information</li>
<li>Iterative for each state in one update cycle</li>
<li>Converge to $v_\pi$ <em>(can be proved)</em></li>
</ul>
<h3 id="On-the-basis-of…-Bellman-equation"><a href="#On-the-basis-of…-Bellman-equation" class="headerlink" title="On the basis of… Bellman equation"></a>On the basis of… Bellman equation</h3><p><img src="be-diagram.png" alt="alt be"></p>
<h3 id="Algorithm-description"><a href="#Algorithm-description" class="headerlink" title="Algorithm description"></a>Algorithm description</h3><p><img src="policydp-algo1.png" alt="alt algo1"></p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Suppose there is a $4\times4$ grid with two terminals. And we will “help” agent to evaluate the current decision policy, which is described by converged value function.</p>
<p><img src="dp-iter.png" alt="alt iter"></p>
<p>When $k=1$, we will use the value function from $k=0$.</p>
<h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>On top of the policy evalution, which provides us a effective score for updating the policy to be better.</p>
<p>Because the policy, the way agent decides its action, is causative relative to the following value, the policy update is actually a optimization problem.</p>
<p>In short, we can improve the policy by acting greedily:</p>
<script type="math/tex; mode=display">\pi'(s) = arg\max_{a\in A} q_\pi (s,a)</script><p>Subsequently, this improves the value from any state $s$ over one step,</p>
<script type="math/tex; mode=display">v_{\pi'} = q_\pi (s,\pi'(s)) \max_{a\in A} q_\pi (s,a) \geq q_\pi (s,a) = v_\pi (s)</script><p>Improvements <strong>stop</strong> if the equality above holds, which also indicates that the Bellman optimality eqation has been satisfied.</p>
<h3 id="Greedy-selection"><a href="#Greedy-selection" class="headerlink" title="Greedy selection"></a>Greedy selection</h3><ul>
<li>Given a policy $\pi$<ul>
<li>evalution of $\pi$ following the last section</li>
<li>improvement the policy by acting <strong>greedily</strong> w.r.t $v_\pi$</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\pi' = greedy(v_\pi)</script><ul>
<li>In general, improved policy is optimal $\pi’ = \pi^\ast$</li>
<li><p>One-hot strategy</p>
</li>
<li><p>Routine: $\pi_0 \to v_{\pi_0} \to \pi_1 \to v_{\pi_1} \to \dots \to \pi_\ast \to \pi_\ast$</p>
</li>
</ul>
<p><img src="policydp-algo2.png" alt="alt algo2"></p>
<h3 id="Modified-Policy-Iteration"><a href="#Modified-Policy-Iteration" class="headerlink" title="Modified Policy Iteration"></a>Modified Policy Iteration</h3><ul>
<li>policy evaluation DOES not need to converge to $v_pi$<ul>
<li>more iteration cause computation waste</li>
<li>sometimes small iterations can give same results as final one</li>
</ul>
</li>
</ul>
<p><img src="dp-iter2.png" alt="alt iter"></p>
<ul>
<li>instead, set $k=k_0$ as stopping condition</li>
<li>or $\epsilon$-convergence</li>
<li>if we update policy EVERY ITERATION…<ul>
<li>this is equivalent to <strong>value iteration</strong></li>
</ul>
</li>
</ul>
<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><ul>
<li>Problem: find optimal policy $\pi$</li>
<li>Solution: iterative application of Bellman <strong>optimality</strong> backup</li>
<li>Routine: $v_1\to v_2 \to \dots \to v_\pi$(end)</li>
<li>Synchronous backups: $k+1$-stage only use $k$-stage information</li>
<li>Iterative for each state in one update cycle</li>
<li>Converge to $v_\pi$ <em>(can be proved)</em></li>
<li>Unlike policy iteration, there is NO explicit policy<ul>
<li>intermediate value functions may not correspond to any policy!</li>
</ul>
</li>
</ul>
<h3 id="Bellman-Optimality-Backup"><a href="#Bellman-Optimality-Backup" class="headerlink" title="Bellman Optimality Backup"></a>Bellman Optimality Backup</h3><p><img src="boe-diagram.png" alt="alt boe"></p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="policydp-algo3.png" alt="alt algo3"></p>
<h3 id="Comparison-between-Synchronous-Dynamic-Programming-Algorithms"><a href="#Comparison-between-Synchronous-Dynamic-Programming-Algorithms" class="headerlink" title="Comparison between Synchronous Dynamic Programming Algorithms"></a>Comparison between Synchronous Dynamic Programming Algorithms</h3><p><img src="comp.png" alt="alt comp"></p>
<ul>
<li>All algorithms are based on state-value function $v_\pi(s)$ and $v_\ast (s)$</li>
<li>Complexity $O(mn^2)$ per iteration, for $m$ actions and $n$ states</li>
</ul>
<hr>
<p><strong>Reference</strong>: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Dynamic-programming</tag>
        <tag>Reinforcement-learning</tag>
        <tag>Control</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use sed in Linux?</title>
    <url>/2020/03/27/command-sed/</url>
    <content><![CDATA[<p><strong>sed</strong> is a streaming text command-line tool in Linux and Mac OS(a little bit different). It can work perfect with RE to finish the operation of one or more than one text file, including delete, replace, insert and so on.</p>
<h2 id="Patterns"><a href="#Patterns" class="headerlink" title="Patterns"></a>Patterns</h2><h3 id="Command-template"><a href="#Command-template" class="headerlink" title="Command template"></a>Command template</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed [options] <span class="string">'command'</span> file(s)</span><br><span class="line">sed [options] -f scriptfile file(s)</span><br><span class="line">sed -h : to display the helper doc</span><br></pre></td></tr></table></figure>
<h3 id="Input-arguments"><a href="#Input-arguments" class="headerlink" title="Input arguments"></a>Input arguments</h3><p>File: filename(s) in system</p>
<h3 id="Command-parameters"><a href="#Command-parameters" class="headerlink" title="Command parameters"></a>Command parameters</h3><p>(<em>frequently used ones are listed below</em>)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a\ <span class="comment"># insert downward, relatively</span></span><br><span class="line">i\ <span class="comment"># insert upward, relatively</span></span><br><span class="line">d  <span class="comment"># delete</span></span><br><span class="line">s  <span class="comment"># replace</span></span><br><span class="line">h  <span class="comment"># copy the content to buffer</span></span><br><span class="line">g  <span class="comment"># use stuffs in buffer to replace</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Replace-flags"><a href="#Replace-flags" class="headerlink" title="Replace flags"></a>Replace flags</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g <span class="comment"># all-replace in line</span></span><br><span class="line">p <span class="comment"># print the line</span></span><br><span class="line">w <span class="comment"># write the line into a file</span></span><br><span class="line">x <span class="comment"># swap the text with the buffer</span></span><br></pre></td></tr></table></figure>
<h3 id="Metacharacter-set"><a href="#Metacharacter-set" class="headerlink" title="Metacharacter set"></a>Metacharacter set</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">^ <span class="comment"># match the beginning of line</span></span><br><span class="line">$ <span class="comment"># match the ending of line</span></span><br><span class="line">. <span class="comment"># match a non-new-line arbitrary character</span></span><br><span class="line">* <span class="comment"># match 0 or more character</span></span><br><span class="line">[] <span class="comment"># match a range of characters</span></span><br><span class="line">[^] <span class="comment"># match characters beyond the range</span></span><br><span class="line">\(..\) <span class="comment"># match substring and keep</span></span><br><span class="line">&amp; <span class="comment"># keep target character and replace the &amp;</span></span><br><span class="line">\&lt; <span class="comment"># match the beginning of word</span></span><br><span class="line">\&gt; <span class="comment"># match the ending of word</span></span><br></pre></td></tr></table></figure>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/^sed/ <span class="comment"># -&gt; "sedimentation ..."</span></span><br><span class="line">/sed$/ <span class="comment"># -&gt; ".... sed"</span></span><br><span class="line">/s.d/ <span class="comment"># -&gt; "sad"</span></span><br><span class="line">/*sed/ <span class="comment"># -&gt; "    sed"</span></span><br><span class="line">[] <span class="comment"># /[ss]ed/ -&gt; "sed" or "Sed"</span></span><br><span class="line">[^] <span class="comment"># /[^A-RT-Z]ed/ "Sed"</span></span><br><span class="line">\(..\) <span class="comment"># s/\(love\)able/\1rs，loveable -&gt; lovers</span></span><br><span class="line">&amp; <span class="comment"># s/love/ **&amp;** / "love"-&gt; "**love**"</span></span><br><span class="line">\&lt; <span class="comment"># /\&lt;love/ -&gt; "loveable" or "lovelorn"</span></span><br><span class="line">\&gt; <span class="comment"># /love\&gt;/ -&gt; "nolove"</span></span><br></pre></td></tr></table></figure>
<h2 id="Usage-instances"><a href="#Usage-instances" class="headerlink" title="Usage instances"></a>Usage instances</h2><h3 id="replacing-s"><a href="#replacing-s" class="headerlink" title="replacing: s"></a>replacing: s</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'s/book/books/'</span> file</span><br><span class="line"></span><br><span class="line">sed -n <span class="string">'s/test/TEST/p'</span> file <span class="comment"># only matched lines are printed out</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'s/book/books/g'</span> file <span class="comment"># directly editing the files without display</span></span><br></pre></td></tr></table></figure>
<h3 id="all-replacing-g"><a href="#all-replacing-g" class="headerlink" title="all-replacing: g"></a>all-replacing: g</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'s/book/books/g'</span> file</span><br><span class="line"></span><br><span class="line"><span class="comment"># if we want replacing happens from Nth match</span></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/2g'</span></span><br><span class="line">skSKSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/3g'</span></span><br><span class="line">skskSKSKSKSK</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sksksksksksk | sed <span class="string">'s/sk/SK/4g'</span></span><br><span class="line">skskskSKSKSK</span><br></pre></td></tr></table></figure>
<h3 id="delimiter-and-so-on"><a href="#delimiter-and-so-on" class="headerlink" title="delimiter: / : | and so on"></a>delimiter: / : | and so on</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'s/test/TEXT/g'</span></span><br><span class="line">sed <span class="string">'s:test:TEXT:g'</span></span><br><span class="line">sed <span class="string">'s|test|TEXT|g'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># escaping when we need their happening</span></span><br><span class="line">sed <span class="string">'s/\/bin/\/usr\/local\/bin/g'</span></span><br></pre></td></tr></table></figure>
<h3 id="delete-d"><a href="#delete-d" class="headerlink" title="delete: d"></a>delete: d</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'/^$/d'</span> file <span class="comment"># delete empty lines</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'2d'</span> file <span class="comment"># delete 2nd line</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'2,$d'</span> file <span class="comment"># delete 2nd line to end</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'$d'</span> file <span class="comment"># delete end line</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'/^test/'</span>d file <span class="comment"># delete line with 'test' as beginning</span></span><br></pre></td></tr></table></figure>
<h3 id="choosing-target-lines"><a href="#choosing-target-lines" class="headerlink" title="choosing target lines: ,"></a>choosing target lines: ,</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -n <span class="string">'/test/,/check/p'</span> file <span class="comment"># prints lines from test and check</span></span><br><span class="line"></span><br><span class="line">sed -n <span class="string">'5,/^test/p'</span> file <span class="comment"># print from 5th line to the first line with beginning as 'text'</span></span><br></pre></td></tr></table></figure>
<h3 id="multiple-command-e"><a href="#multiple-command-e" class="headerlink" title="multiple command: e"></a>multiple command: e</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -e <span class="string">'1,5d'</span> -e <span class="string">'s/test/check/'</span> file <span class="comment"># perform double operations to each line</span></span><br></pre></td></tr></table></figure>
<h3 id="write-into-file-w"><a href="#write-into-file-w" class="headerlink" title="write into file: w"></a>write into file: w</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -n <span class="string">'/test/w outfile'</span> example</span><br></pre></td></tr></table></figure>
<h3 id="add-downward-a"><a href="#add-downward-a" class="headerlink" title="add(downward): a"></a>add(downward): a</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'/^test/a\this is a test line'</span> file</span><br></pre></td></tr></table></figure>
<h3 id="insert-upward-i"><a href="#insert-upward-i" class="headerlink" title="insert(upward): i"></a>insert(upward): i</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'/^test/i\this is a test line'</span> file <span class="comment"># before the lines with beginning as 'test'</span></span><br></pre></td></tr></table></figure>
<h3 id="Other-common-usage"><a href="#Other-common-usage" class="headerlink" title="Other common usage"></a>Other common usage</h3><h4 id="keep-matched"><a href="#keep-matched" class="headerlink" title="keep matched"></a>keep matched</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> this is a <span class="built_in">test</span> line | sed <span class="string">'s/\w\+/[&amp;]/g'</span> <span class="comment"># match each word and add brackets</span></span><br><span class="line">[this] [is] [a] [<span class="built_in">test</span>] [line]</span><br><span class="line"></span><br><span class="line">sed <span class="string">'s/^192.168.0.1/&amp;localhost/'</span> file</span><br><span class="line">192.168.0.1localhost</span><br></pre></td></tr></table></figure>
<h4 id="substring"><a href="#substring" class="headerlink" title="substring"></a>substring</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> this is digit 7 <span class="keyword">in</span> a number | sed <span class="string">'s/digit \([0-9]\)/\1/'</span></span><br><span class="line">this is 7 <span class="keyword">in</span> a number</span><br></pre></td></tr></table></figure>
<h4 id="combination"><a href="#combination" class="headerlink" title="combination"></a>combination</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'pattern'</span> | sed <span class="string">'pattern'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># also as:</span></span><br><span class="line"></span><br><span class="line">sed <span class="string">'pattern; pattern'</span></span><br></pre></td></tr></table></figure>
<h4 id="quotation"><a href="#quotation" class="headerlink" title="quotation"></a>quotation</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">test</span>=hello</span><br><span class="line"><span class="built_in">echo</span> hello WORLD | sed <span class="string">"s/<span class="variable">$test</span>/HELLO"</span></span><br><span class="line">HELLO WORLD</span><br></pre></td></tr></table></figure>
<h4 id="next"><a href="#next" class="headerlink" title="next"></a>next</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'/test/&#123; n; s/aa/bb/; &#125;'</span> file <span class="comment"># if test is matched then perform over the next line</span></span><br></pre></td></tr></table></figure>
<h4 id="quit-break"><a href="#quit-break" class="headerlink" title="quit(break)"></a>quit(break)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'10q'</span> file <span class="comment"># quit sed after print out line 10</span></span><br></pre></td></tr></table></figure>
<h4 id="print-odd-or-even-lines"><a href="#print-odd-or-even-lines" class="headerlink" title="print odd or even lines"></a>print odd or even lines</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -n <span class="string">'p;n'</span> test.txt  <span class="comment">#odd</span></span><br><span class="line">sed -n <span class="string">'n;p'</span> test.txt  <span class="comment">#even</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"></span><br><span class="line">sed -n <span class="string">'1~2p'</span> test.txt  <span class="comment">#odd</span></span><br><span class="line">sed -n <span class="string">'2~2p'</span> test.txt  <span class="comment">#even</span></span><br></pre></td></tr></table></figure>
<h2 id="Affiliated-tools-under-shell"><a href="#Affiliated-tools-under-shell" class="headerlink" title="Affiliated tools under shell"></a>Affiliated tools under shell</h2><h3 id="Get-all-filenames-as-var-by"><a href="#Get-all-filenames-as-var-by" class="headerlink" title="Get all filenames as var by `"></a>Get all filenames as var by `</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `ls | grep .jpg`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  newfile=`<span class="built_in">echo</span> <span class="variable">$file</span> | sed <span class="string">'s/dtest/cora/g'</span>`</span><br><span class="line">  mv <span class="variable">$file</span> <span class="variable">$newfile</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="String-slice"><a href="#String-slice" class="headerlink" title="String slice"></a>String slice</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="comment">#var&#125; # length of string var</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;alpha:3&#125;</span> <span class="comment"># display rest after first 3 chars</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;alpha:3:4&#125;</span> <span class="comment"># display 4 chars after first 3 chars</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;alpha: -3&#125;</span> <span class="comment"># display the last 3</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Command-line</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title>Counting the number of permutations</title>
    <url>/2020/03/26/2020-03-26-stirling1/</url>
    <content><![CDATA[<p>Permutation group is an important instance in group theory. And each permutation $\pi$ can be uniquely decomposed into the combination of <strong>cycles</strong>. Then the number of such permutation in the known permutation group with specific cycles decompostion is correlated with <strong>Stirling numbers of the first kind</strong>.</p>
<h2 id="Term-definition"><a href="#Term-definition" class="headerlink" title="Term definition:"></a>Term definition:</h2><ul>
<li>finite set with order $n$: $X$, which is also isomorphic with $[n]$</li>
<li>the permutation group over $X$: $S_n$, obviously $\left| S_n \right| = n!$</li>
<li>some permutation in $S_n$:  $\pi=\pi(1)\pi(2)\dots \pi(n)$<ul>
<li>In fact, it is same as full-permutation</li>
</ul>
</li>
<li>the cyclic decompostion of permutation $\pi$: given $\pi \in S_n, i\in [n]$, there must exist a cycle $(i,\pi(i),\pi^2(i),\dots,\pi^{s-1}(i), i)$ (denoted as $C$), where $s$ is the least positive integer constructing such cycle. Then it is obvious that $\forall x \in [n]$ has to belong to one and only one cycle, and thus we have the cyclic decompostion of given permutation:</li>
</ul>
<script type="math/tex; mode=display">\pi = C_1\dots C_k</script><p>which is unique for given $\pi$, thus forming an one-to-one.</p>
<ul>
<li><p>the pattern(type) of permutation: in terms of the decompostion above, if there are totally $\lambda_s$ cycles with length of $s$, then we can write the pattern of such cyclic decompostion: $1^{\lambda_1}2^{\lambda_2}\dots n^{\lambda_n}$. Such as: (147)(25)(3)(6) is $1^2 2^1 3^1$ type.</p>
</li>
<li><p>conjugative relation ( which is also an <strong>equivalence relation</strong>): $\pi - \pi’ ~ if ~ \exists ~\rho \in S_n ~s.t. ~\pi’ = \rho\pi\rho^{-1}$</p>
</li>
<li><strong>[Prop]</strong>  $\pi, \pi’ \in S_n $ are conjugative if and only if they share the same pattern.</li>
</ul>
<h2 id="Stirling-numbers-of-the-first-kind"><a href="#Stirling-numbers-of-the-first-kind" class="headerlink" title="Stirling numbers of the first kind"></a>Stirling numbers of the first kind</h2><p>Here, we first give the definition of such Stirling numbers $s(n,k)$, then talking about its property and relations to the second kind.</p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>Given $n,k \in S_n$, the number of permutations those can be exactly decomposed into $k$ cycles is denoted as $c(n,k)$, then let</p>
<script type="math/tex; mode=display">s(n,k) = (-1)^{n+k} c(n,k)</script><p>which is called <em>Stirling numbers of the first kind</em>.</p>
<p>We appoint that</p>
<ul>
<li>$s(n,0) = c(n,0) = 0 ~(n&gt;0) $</li>
<li>$s(0,0)= c(0,0) = 1$</li>
<li>$s(n,k) = c(n,k) = 0,~ n &lt; k$</li>
</ul>
<h3 id="Recursive-relation"><a href="#Recursive-relation" class="headerlink" title="Recursive relation"></a>Recursive relation</h3><ul>
<li>$c(n,k) = c(n-1,k-1) + (n-1)c(n-1,k)$</li>
<li>$s(n,k) = s(n-1,k-1) - (n-1)s(n-1,k)$</li>
</ul>
<h3 id="Expansion"><a href="#Expansion" class="headerlink" title="Expansion"></a>Expansion</h3><p>Let $x$ be indefinite element, then</p>
<ul>
<li><script type="math/tex; mode=display">(x)_n = \sum_{k=0}^n s(n,k) x^k</script></li>
<li><script type="math/tex; mode=display">(x)^n = \sum_{k=0}^n c(n,k) x^k</script></li>
</ul>
<p>which are respectively $n-$ downward and upward factorial.</p>
<h3 id="Dual-expansion-with-“the-second-kind”"><a href="#Dual-expansion-with-“the-second-kind”" class="headerlink" title="Dual expansion with “the second kind”"></a>Dual expansion with “the second kind”</h3><p>Since have known the following expansion:</p>
<ul>
<li><script type="math/tex; mode=display">(x)_n = \sum_{k=0}^n s(n,k) x^k</script></li>
<li><script type="math/tex; mode=display">x^n = \sum_{k=0}^n S(n,k) (x)_k</script></li>
</ul>
<p>We can spot their dual relations at once.</p>
<p>Moveover, we can deduce more magic result from them two.</p>
<p>Because the $s(k,m) = 0$ if $m &gt; k$, we have:</p>
<script type="math/tex; mode=display">x^n = \sum_{k=0}^n S(n,k) (x)_k = \sum_{k=0}^n S(n,k) \sum_{m=0}^k s(k,m) (x)_m \\
= \sum_{m=0}^k \left(\sum_{k=0}^n S(n,k)s(k,m) \right) x^m</script><p>_(Check it! It holds since we let some terms be zero )_</p>
<p>We immediately have:</p>
<script type="math/tex; mode=display">\sum_{k=0}^n S(n,k)s(k,m)  = \delta_{mn}</script><p>In the form of matrix, we can know that $n+1$-order matrices $S_n^{(2)} = (S(i,j))~(i,j=0,1,\dots)$ and $S_n^{(1)} = (s(i,j))$ are mutually inversive.</p>
<script type="math/tex; mode=display">S_n^{(1)} S_n^{(2)} = S_n^{(2)} S_n^{(1)} = \mathit I</script><hr>
<p><strong>Reference</strong></p>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Stirling_numbers_of_the_first_kind" target="_blank" rel="noopener">Stirling numbers of the first kind</a></p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Combinatorics</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of SVM-(3)</title>
    <url>/2020/03/26/2020-03-26-svm3/</url>
    <content><![CDATA[<p>After magically transforming the optimization problem into a very simple inner products, we continues to introduce the powerful tool, <strong>kernels</strong>. The previous contents: <a href="https://jiaruilu.com/2020/03/24/svm1.html" target="_blank" rel="noopener">SVM-(1)</a>,<a href="https://jiaruilu.com/2020/03/24/svm2.html" target="_blank" rel="noopener">SVM-(2)</a></p>
<h2 id="Kernels"><a href="#Kernels" class="headerlink" title="Kernels"></a>Kernels</h2><h3 id="Terms"><a href="#Terms" class="headerlink" title="Terms:"></a>Terms:</h3><p>Suppose the original input is $x$, for instance, $x$ can be the living area of a house. Then:</p>
<ul>
<li>input attributes of a problem: $x$</li>
<li>input features: $[x,x^2,x^3]^T$ ( any derivative ones )</li>
<li>feature mapping $\phi$: <em>attr.</em> $\to$ <em>feat.</em> (in short it is, such mapping can be dimension-reducting or increasing)</li>
</ul>
<h3 id="Definition-of-Kernel"><a href="#Definition-of-Kernel" class="headerlink" title="Definition of Kernel"></a>Definition of Kernel</h3><p>Remember that we have obtained the final prediction expression in a form of inner products. Then the <em>feature mapping</em> will replace all the $\langle x,z \rangle$ with $\langle \phi(x),\phi(z) \rangle$. Specifically, given a feature mapping $\phi$, we define the corresponding <strong>kernel</strong> to be</p>
<script type="math/tex; mode=display">K(x,z) = \langle \phi(x),\phi(z) \rangle =  \phi(x)^T \phi(z)</script><p>Remarks:</p>
<ul>
<li>the computation of $K(x,z)$ is often inexpensive while that of $\phi(x)$ is the other extreme, because it is such a high-dimensional vector.</li>
<li>For example, consider $K(x,z) = (x^Tz)^2$, where finding $K(x,z)$ takes only $O(n)$ time — linear in the dimension of the input <strong>attributes</strong> whereas it is $O(n^2)$ complexity for $\phi(x)$</li>
<li>$K(x,z)$, just as inner product defined in Euclidean space, is a measure of how similar, say close the $x,z$ are.</li>
</ul>
<p>An important kernel with <strong>infinite dimensional feature mapping</strong> is the Gaussian kernel in the following form:</p>
<script type="math/tex; mode=display">K(x,z) = exp(-\frac{||x-z||^2}{2\sigma^2})</script><p>This is a reasonable measure of $x$ and $z$’s similarity. However, when we try to do some generalization works, we have to check its validity by $K(x,z) = \phi(x)^T \phi(z)$, if there exists some $\phi(\cdot)$ satisfying for all $x,z$?</p>
<p>(Note: we only ask the $\phi$ enabling $K(x,z) = \phi(x)^T \phi(z)$ to hold in terms of $x,z$ in our dataset )</p>
<p>We directly give the following theorem:</p>
<p><strong>Theorem (Mercer)</strong> $K: R^n \times R^n \to R $ is a valid ( Mercer ) kernel if and only if for any ${x^{(i)}} (m &lt; \infty)$, the corresponding kernel matrix $( K(x^{(i)},x^{(j)}) )_{m\times m}$ is symmetric positive semi-definite.</p>
<p>Here is a simple case study: consider input attributes $x$ is a sequence of amino acids, say a string of letters. Let $\phi$ be a feature vector that counts the number of occurrences of each length-k substring in $x$. As for the English letters, there are $26^k$ possible substrings and now $\phi(x)$ is a $26^k$ dimensional vector which is definitely too large to work with. However, by applying DP string matching algorithms, it is efficient to compute $K(x,z)$ directly. We just <strong>implicit</strong> work in such $26^k$-dimensional feature space.</p>
<p>Note that the idea of “kernel tricks” is way broader than the application in SVMs. Any learning function written in the form of inner products can be replaced by $K(x,z)$ thus magically enabling the algorithm to work in the higher-dimensional feature space.</p>
<p>However, as long as the high-dimensional features can be learned, the overfitting problem is followed. As shown above, some kernels offer great insights in the higher space, which equivalently “increase the number of the parameters”. The Gaussian Kernel, say <strong>radial basis function</strong>, is able to mapping the attributes into infinite dimensional space. It incredibly increase the probability to overfit the dataset. On the other hand, without powerful kernels, it is impossible for SVM, a linear model, to outperform LR so much, which exhibits great redundancy to introduce many constructs and conditions without better performance though. It is a trade-off to make decision.</p>
<hr>
<p><strong>Reference</strong>: <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener">Stanford CS229-note3</a> by Andrew Ng</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Support-vector-machine</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of SVM-(2)</title>
    <url>/2020/03/24/2020-03-24-svm2/</url>
    <content><![CDATA[<p>Followed by <a href="https://jiaruilu.com/2020/03/24/svm1.html" target="_blank" rel="noopener">SVM-(1)</a>, our discussion of SVM continues in this post. Here, we will interpret the <em>Lagrange duality</em>, <em>KKT conditions</em>, which are very important to the reinforcement of SVM: the introduction of kernel functions.</p>
<h2 id="Note-of-SVM-2-Lagrange-duality-KKT-conditions-and-kernels"><a href="#Note-of-SVM-2-Lagrange-duality-KKT-conditions-and-kernels" class="headerlink" title="Note of SVM-(2): Lagrange duality, KKT conditions and kernels"></a>Note of SVM-(2): Lagrange duality, KKT conditions and kernels</h2><p><em>Review:</em> According to the last post, we state that building up hyperplane is to solve the optimization problem:</p>
<script type="math/tex; mode=display">\min_{w,b} {1\over 2} ||w||^2</script><script type="math/tex; mode=display">s.t. ~ y^{(i)}(w^T x^{(i)} + b) \geq 1</script><p>Before we move on, we branch out to talk about the <strong>Lagrange duality</strong> of such optimization. Consider the following optimization problem with both equality and inequality constraints ( <em>called primal problem ,relatively</em> ) :</p>
<p>$\min_{w} f(w)$<br>$s.t. g_i(w) \leq 0, i=1,\dots,k$<br>$s.t. h_i(w) = 0   ,i = 1,\dots,l$</p>
<p>And then we define the (generalized) <strong>Lagrangian</strong> of it:</p>
<script type="math/tex; mode=display">L(w,\alpha,\beta) = f(w) + \sum_{i=1}^k a_i g_i{w} + \sum_{i=1}^l \beta_i h_i(w)</script><p>where $\alpha$’s and $\beta$’s are the Lagrange multipliers. Then we define the following quantity:</p>
<script type="math/tex; mode=display">\theta_p(w) = \max_{\alpha,\beta,\alpha \geq 0} L(w,\alpha,\beta)</script><p>Actually, the $\theta_p(w)$ is equal to $f(w)$ if $w$ satisfies all the primal constraints; otherwise, it goes up to $\infty$.</p>
<p>Please keep that in mind, we immediately derive:</p>
<script type="math/tex; mode=display">\min_w \theta_p(w) = \min_w \max_{\alpha,\beta,\alpha \geq 0} L(w,\alpha,\beta)</script><p>which is also equivalent to $\min_w f(w)$ under all of the constraints.</p>
<p>We denote $\min_w \theta_p(w)$ as $p^\ast$, called the <strong>value</strong> of the primal problem.</p>
<p>Correspondingly, we define the <strong>dual</strong> as:</p>
<script type="math/tex; mode=display">\theta_D(w) = \min_{\alpha,\beta,\alpha \geq 0} L(w,\alpha,\beta)</script><p>also with the dual optimization problem:</p>
<script type="math/tex; mode=display">\max_w \theta_D(w) = \max_w \min_{\alpha,\beta,\alpha \geq 0} L(w,\alpha,\beta)</script><p><em>(where $\max_w \theta_D(w)$ is denoted by $d^\ast$ similarly)</em></p>
<p>Without proof, we claim that the “max min” of a function is always less than or equal to the “min max”, i.e.,</p>
<script type="math/tex; mode=display">d^\ast = \max_w \min_{\alpha,\beta,\alpha \geq 0} L(w,\alpha,\beta) \leq \min_w \max_{\alpha,\beta,\alpha \geq 0} L(w,\alpha,\beta) = p^\ast</script><p>But what we really care is the condition rendering:</p>
<script type="math/tex; mode=display">d^\ast = p^\ast</script><p>In fact, there must exist $w^\ast, \alpha^\ast, \beta^\ast$ such that $w^\ast$ is the solution to the <em>primal problem</em>; $\alpha^\ast, \beta^\ast$ are the solution to the <em>dual problem</em>, and moreover $d^\ast = p^\ast = L(w^\ast, \alpha^\ast, \beta^\ast)$. Furthermore, $w^\ast, \alpha^\ast, \beta^\ast$ satisfy the following conditions.</p>
<h3 id="Karush-Kuhn-Tucker-KKT-conditions"><a href="#Karush-Kuhn-Tucker-KKT-conditions" class="headerlink" title="Karush-Kuhn-Tucker (KKT) conditions"></a>Karush-Kuhn-Tucker (KKT) conditions</h3><script type="math/tex; mode=display">{\partial \over \partial w_i} L(w^\ast, \alpha^\ast, \beta^\ast) = 0</script><script type="math/tex; mode=display">{\partial \over \partial \beta_i} L(w^\ast, \alpha^\ast, \beta^\ast)</script><script type="math/tex; mode=display">\alpha^\ast_i g_i(w^\ast) = 0</script><script type="math/tex; mode=display">g_i(w^\ast) \leq 0</script><script type="math/tex; mode=display">\alpha \geq 0</script><p>What’s more important, is that if some $w^\ast, \alpha^\ast, \beta^\ast$ satisfy the KKT conditions, then it is also a solution to the primal and dual problems.</p>
<p><strong>Remarks:</strong></p>
<p>Note that the third equation $\alpha^\ast_i g_i(w^\ast) = 0$, which is called the KKT dual complementarity condition, implies that if $\alpha^\ast_i &gt; 0$, then $g_i(w^\ast) = 0$.</p>
<h3 id="Optimal-margin-classifiers"><a href="#Optimal-margin-classifiers" class="headerlink" title="Optimal margin classifiers"></a>Optimal margin classifiers</h3><p>Get back to our original problem:</p>
<script type="math/tex; mode=display">\min_{w,b} \frac{1}{2} ||w||^2</script><script type="math/tex; mode=display">s.t. y^{(i)}(w^T x^{(i)} + b) \geq 1</script><p>We can write the constraints as</p>
<script type="math/tex; mode=display">g_i(w) = -y^{(i)}(w^T x^{(i)} + b) + 1 \leq 0</script><p>Note that we have one such constraints for each training example and according to _KKT dual complementarity condition_, we will have $\alpha &gt; 0$ only for the training example those having functional margin exactly equal to one ( $g_w(w) = 0 $ ).</p>
<p><img src="/images/svm2-1.png" alt="alt img1"></p>
<p>The points with the smallest margins are the ones closest to the decision boundary( on dash lines as shown in the figure).<br>These are called the <strong>support vectors</strong>, the number of which is far less than the whole dataset.</p>
<p>When we construct the lagrangian for our optimization problem we have:</p>
<script type="math/tex; mode=display">L(w,b,\alpha) = {1\over 2} ||w||^2 - \sum_{i=1}^m \alpha_i [y^{(i)}(w^T x^{(i)} + b) - 1]</script><p>where the $\beta$ disappear because we only have inequality constraints. By setting the derivatives( may say gradient ) of $L$ w.r.t $w,b$ to zero, the expression of $w,b$ is obtained. Plugging them back to the dual problem, we have:</p>
<script type="math/tex; mode=display">L(w,b,\alpha) = \sum_{i=1}^m \alpha_i+ {1\over 2}  \sum_{i,j=1}^m y^{(i)} y^{(j)}\alpha_i \alpha_j (x^{(i)})^T x^{(i)}</script><p>Then we have the optimization problem as ( inner product form ):</p>
<p>$\max_\alpha W(\alpha) = \sum_{i=1}^m \alpha_i + {1\over 2} \sum_{i,j=1}^m y^{(i)} y^{(j)}\alpha_i \alpha_j \langle x^{(i)} x^{(i)} \rangle$</p>
<p>$s.t. ~~ \alpha_i \geq 0$</p>
<p>$\sum_{i=1}^m \alpha_i y^{(i)} = 0$</p>
<p>However, we should also be able to verify that the KKT conditions and $d^\ast = p^\ast$ are satisfied so that we can solve this dual in lieu of primal problem.</p>
<p>If some methods can be applied to solve the optimal $\alpha_i, ~i =1,\dots,m$, then we are able to subsequently compute the:</p>
<ul>
<li>$w(\alpha, y,x)$</li>
<li>$b(w,y,x)$</li>
</ul>
<p>The final form for our predicition task is obtained when we applied the optimal $w,b$ to compute the $\hat y$ with the new data $x$:</p>
<script type="math/tex; mode=display">w^T + b = (\sum_{i=1}^m \alpha^i y^{(i)}x^{(i)} )^T x + b\\
= \sum_{i=1}^m \alpha^i y^{(i)} \langle x^{(i)},  x \rangle + b</script><p>Just like the <strong>KNN</strong>, we use the training samples to “directly” finish the prediction in the form of inner products. But however, we do not need to exhaust the whole dataset, which is time-consuming. Instead, according to the KKT conditions, non-zero $\alpha_i$ arises only for a small number of support vectors! Such that:</p>
<script type="math/tex; mode=display">w^T + b = \sum_{support ~ vector ~\{i\}} \alpha^i y^{(i)} \langle x^{(i)},  x \rangle + b</script><p>Note that by examining the dual form of the optimization problem, we gained significant insight into the structure of the problem, thus writing the algorithm as inner products, which plays an important role in kernel trick.</p>
<p><strong>In summary</strong>, we introduce the dual form of the primal problem and transform the optimization “$\min \max$” into “$\max \min$”. By taking derivatives and let them be zero, we replaced the $w(\alpha,x,y),b(\alpha,x,y)$ by $\alpha$; after solving the optimal $\alpha$, we plug tem in obtaining the parameters of SVM classifier. In the final step, we only use those samples which belong to $\alpha_i &gt; 0$ and greatly reduce the computation cost.</p>
<hr>
<p><strong>Reference</strong>: <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener">Stanford CS229-note3</a> by Andrew Ng</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Support-vector-machine</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of SVM-(1)</title>
    <url>/2020/03/24/2020-03-24-svm1/</url>
    <content><![CDATA[<p>In the following posts, we will move on to the discussion about one important basic method in machine learning: Support Vector Machines(SVM). To be honest, SVM is a family of classifier which is based on the optimization tasks. First things first, some important concepts will be introduced as prerequisites.</p>
<h2 id="Note-of-SVM-1-Margins-confidence-and-classification"><a href="#Note-of-SVM-1-Margins-confidence-and-classification" class="headerlink" title="Note of SVM-(1): Margins, confidence and classification"></a>Note of SVM-(1): Margins, confidence and classification</h2><p>It is definitely not a very strange topic when it comes to classification. There is already simple but powerful tool, Logistic regression, serving for an effective model of classification. It provides straightforward rules for discrimination. However, SVM is a little bit different from LR.</p>
<p>Consider the following two clusters of points as data samples on a plane.</p>
<p><img src="/images/svm1-1.png" alt="alt img1"></p>
<p>Our task is to find a proper line, or more generally, <strong>separating hyperplane</strong>, as our discriminative rule for classification. Instead of parameter estimation of $\theta$ by MLE with regard to the probability of dataset, we rather maximize the <strong>“confidence”</strong> of our predictions.</p>
<p>The confidence is high if the prediction point is located way more distant from the separating hyperplane. In LR case, it happens when the $abs(\theta^T x)$ is large. Thus, we will be sure enough that our prediction is good. This seems to be a nice goal to aim for, and after formalize this idea, we obtain the basic idea of SVM.</p>
<h3 id="Functional-Margins"><a href="#Functional-Margins" class="headerlink" title="Functional Margins"></a>Functional Margins</h3><p>As the $\theta^T x = 0$ gives a decision boundary between the positive training examples and negative ones, we define such a measure of “how good” the dataset being separated, called functional margins.</p>
<script type="math/tex; mode=display">\hat{\gamma}^{(i)} = y^{(i)}(w^T x^{(i)} + b)</script><p>where $y^{(i)} \in {-1,1}$ holds the sign to be non-negative for convenience, $\{(x^{(i)},y^{(i)})\}$ as the dataset, and thus our hypothesis is</p>
<script type="math/tex; mode=display">h_{w,b} = g(w^T x + b); ~g(z)=1( z\geq 0), ~g(z)= -1 (otherwise)</script><p>According to the “best separating” idea, we definitely would like to maximize such measure, but not for all! We can know better by thinking it over: in fact only the “closest” point is useful for determining our hyperplane. So among the whole dataset, we need firstly find such “closest” point, also called <strong>support vector</strong>:</p>
<script type="math/tex; mode=display">\hat{\gamma}= \min_i \hat{\gamma}^{(i)}</script><p><img src="/images/svm1-2.png" alt="alt img2"></p>
<p>Before we go further, we can add a little trick to the $\gamma$ stuff by geometric insights. Actually, we can do some normalization operation without changing the meaning of our goal:</p>
<script type="math/tex; mode=display">{\gamma}^{(i)} = y^{(i)}((\frac{w}{||w||})^T x^{(i)} + \frac{b}{||w||})</script><p>So far, we have applied a proposition that arbitrary scaling for $w \to w/4 ~and~ b\to b/4$ does no effect on the support vector selection in the context of “margin”.</p>
<h3 id="Optimal-classifier"><a href="#Optimal-classifier" class="headerlink" title="Optimal classifier"></a>Optimal classifier</h3><p>Since the minimal $\gamma$ provides the critical points in the plane for determining our decision boundary, say separating hyperplane, we now state our goal formally in terms of optimization.</p>
<p>$ \max_{\gamma,w,b} \gamma$</p>
<p>$<del>~</del> s.t. <del>y^{(i)}(w^T x^{(i)} + b) \geq \gamma$<br>$</del>~~~||w|| = 1$</p>
<p>However, this lies a non-convex constraint “$||w|| =1 $””, we now change it into:</p>
<p>$ \max_{\hat\gamma,w,b} {\hat\gamma \over ||w||}$</p>
<p>$<del>~</del> s.t. ~~y^{(i)}(w^T x^{(i)} + b) \geq \hat\gamma$</p>
<p>But still, the non-convex scenario appears again on the objective function. We then set $\hat\gamma =1$ and transform the task into minimizing $w$:</p>
<p>$\min_{w,b} {1\over 2} ||w||^2$</p>
<p>$<del>~</del> s.t. ~~y^{(i)}(w^T x^{(i)} + b) \geq 1$</p>
<p>The solution of above directly gives us the <strong>optimal margin classifier</strong>. And such optimization problem can be solved easily using quadratic programming (QP) algorithm.</p>
<p><em>Remarks</em>:</p>
<ul>
<li>We assume that the dataset $\{(x^{(i)},y^{(i)})\}$ is linear separable</li>
<li>It is still a simple classifier which may not outperform the LR model. In the next post, some improvement will be introduced to reinforce it.</li>
</ul>
<hr>
<p><strong>Reference</strong>: <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener">Stanford CS229-note3</a> by Andrew Ng</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Support-vector-machine</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Structure decomposition - two main methods</title>
    <url>/2020/03/23/2020-03-23-nlu-note3/</url>
    <content><![CDATA[<p>From a view of structural learning, we may review the classification task in machine learning. It is clear that most of NLP tasks are not of typical classification since it is nearly impossible or extremely time-consuming to perform such learning process, due to the large amount of labels, say different patterns to be predicted. However, we can make our model proper to assume classfier-like behavior by formularizing both form of input and output representatiion.</p>
<h2 id="Structural-decomposition"><a href="#Structural-decomposition" class="headerlink" title="Structural decomposition"></a>Structural decomposition</h2><p>No structural learning can be straightforwardly and effectively solved until the convertion task is performed. It is called structure decomposition (encoding), which converts the graphs into low-class classification subtasks. Correspondingly, there also exists a re-building step before structure prediction, called <strong>decoding</strong> to give us the formularized prediction results.</p>
<p>There are two types of decomposition strategies: contemporary decomposition and diachronic decomposition, which will be discussed in the following sections. Another thing needed to mention is that the input can always have a form of linear sequence, for convenience.</p>
<h3 id="Contemporary-decomposition"><a href="#Contemporary-decomposition" class="headerlink" title="Contemporary decomposition"></a>Contemporary decomposition</h3><ul>
<li>a graph model (space-dimension decomposition)</li>
<li>polynomial time complexity $O(L^{f(n)})$for decoding in the SAME pattern</li>
<li>accurately done</li>
</ul>
<blockquote>
<p>Structure is decomposed into pieces, decoding is done by searching the space of all possible structure candidates.</p>
</blockquote>
<p>For a structure learning sample $\{X,G\}$, where $X$ is input structure, $G$ is structure for prediction which is represented as a graph decomposed into sub-graphs $G_1,G_2,\dots,G_n$</p>
<p>where</p>
<script type="math/tex; mode=display">G=\bigcup_{j=1}^n G_j</script><ul>
<li>W.L.O.G, decomposition of input structure $X$ is ignored.</li>
<li>The union is not necessarily disjoint( the overlapping may exist between sub-graphs)</li>
</ul>
<h4 id="The-training-of-graph-model"><a href="#The-training-of-graph-model" class="headerlink" title="The training of graph model"></a>The training of graph model</h4><p>We firstly define a score function _score($G_j$)_ to evaluate each <strong>sub-graph</strong>, such that</p>
<script type="math/tex; mode=display">G = arg\max_{\{G_j\}} \sum_{j} score(X,G_j)</script><p>which means that the optimal decomposition strategy (optimal partition of $G$: $\{G_j\}_\ast$) gives the best sumup scores for each sub-graph. _Search algorithm is involved._</p>
<ul>
<li>the training is to derive proper score function</li>
<li>decoding is the maximizing of the scores’ sum-up</li>
</ul>
<h4 id="The-most-simple-case-linear-to-linear-sequence"><a href="#The-most-simple-case-linear-to-linear-sequence" class="headerlink" title="The most simple case: linear to linear sequence"></a>The most simple case: linear to linear sequence</h4><ul>
<li>The number of edges from maximum decomposed sub-graph is called order of such graph model</li>
<li>For the linear decomposition, if every sub-graphs are in the same pattern, then <strong>Viterbi decoding</strong> can be applied for seeking a analytic solution.</li>
</ul>
<p><img src="/images/nlu3-1.png" alt="alt img1"></p>
<ul>
<li>For the linear decomposition, if not every sub-graphs are in the same pattern, then only beam search decoding can be applied for <strong>approximate solution.</strong></li>
</ul>
<p><img src="/images/nlu3-2.png" alt="alt img2"></p>
<p>Now suppose a linear sequence for prediction is always decomposed into linear piece with $n+1$ nodes (namely, $n$ edges), <strong>step-wise overlapping</strong> among all these neighbored decomposed pieces.</p>
<p>_(the filter move step is only $1$, overlapping range is $n$)_</p>
<p>Formally, a sequence $x_1x_2\dotsx_m$ is decomposed into</p>
<script type="math/tex; mode=display">x_1x_2\dots x_{n+1}, ~x_2x_3\dots x_{n+2}, ~\dots , ~x_{m-n}x_{m-n+1}\dots x_m</script><p>We say this is an $n$-order hidden Markov model (HMM), where the score function is defined as logarithmic probability of each pieces.</p>
<h4 id="General-model-over-sub-graphs"><a href="#General-model-over-sub-graphs" class="headerlink" title="General model over sub-graphs"></a>General model over sub-graphs</h4><p><img src="/images/nlu3-3.png" alt="alt img3"></p>
<p>Generally speaking, the classifier should be determined heavily. Given a structure learning sample $\{X,G\}$, $X$ is decomposed into $\{X_i\}$, $G$ into $\{G_i\}$. Then $score(X_i,G_i)$ should be learned by the resulting classifier. Sometimes, the mapping from $X_i$ to $G_i$ is non-trivial.</p>
<p><img src="/images/nlu3-4.png" alt="alt img4"></p>
<h3 id="Diachronic-decomposition"><a href="#Diachronic-decomposition" class="headerlink" title="Diachronic decomposition"></a>Diachronic decomposition</h3><ul>
<li>a transition model (time-dimension decomposition)</li>
<li>linear time performance of greedy procedure</li>
<li>not necessarily finding global optimal decision</li>
<li>beam search with polynomial time complexity to improve</li>
</ul>
<blockquote>
<p>Structure is built step by step, decoding is to make decision about the best building operation each time.</p>
</blockquote>
<p><img src="/images/nlu3-5.png" alt="alt img5"></p>
<p>Given a structure learning sample $\{X,G\}$, $G$’s formation is performed by a series of operation $\{b_1,b_2,\dots\}$.</p>
<p>We then instead use $score(X, G_C,b_i)$ to evaluate every building operation of $G$. The whole $G$ (say all the edges) is established step-wise during the <strong>decoding</strong>: we <strong>greedily</strong> follow the chosen graph building operation according to the classifier prediction with highest scoring.</p>
<p>Every time, after a building operation is performed, the graph building status (context) should be updated, $G_C = G_C^\ast$ ; Repeatedly, the last $G_C^\ast$ will be the graph predicted by the model,</p>
<script type="math/tex; mode=display">G_C^\ast = arg\max_{b_i} score(X, G_C, b_i)</script><blockquote>
<p>Sometimes, we may incorporate transition and graph models. There comes a model like step-wise re-ranking which achieve this, for example, the <strong>Easy-first dependency parsing</strong>. (Ref: _Yoav Goldberg, Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. NAACL 2010._)</p>
</blockquote>
<h2 id="Two-variants-of-graph-model"><a href="#Two-variants-of-graph-model" class="headerlink" title="Two variants of graph model"></a>Two variants of graph model</h2><p>Given a structure learning sample $\{X, G\}$, based on decomposition of input structure $X$ and output structure $G$, there may comes two variants of graph model:</p>
<h3 id="Scoring-sub-graphs"><a href="#Scoring-sub-graphs" class="headerlink" title="Scoring sub-graphs"></a>Scoring sub-graphs</h3><p>Scoring is done on combination of aligned input and output sub-graphs (inside each dotted line frame).</p>
<ul>
<li>$x_i$ is node of <strong>input structure</strong> $X$, and $y_i$ is node of <strong>output structure</strong> $G$.</li>
</ul>
<p><img src="/images/nlu3-6.png" alt="alt img6"></p>
<ul>
<li>Easy and straightforward structure decomposition</li>
<li>Need a <strong>source-target alignment</strong> for candidate sub-graph generation, _sometimes hard_!</li>
<li>Easy-eg. POS tagging task from seq to seq</li>
<li>Haaaard-eg. statistical machine translation ( the mapping is not pair-wise following sequence )</li>
</ul>
<h3 id="Labeling-sub-graphs-tagging"><a href="#Labeling-sub-graphs-tagging" class="headerlink" title="Labeling sub-graphs(tagging)"></a>Labeling sub-graphs(tagging)</h3><p>This demonstrates <strong>learning a label</strong> for each output node $y_i$, and taking $x_i$ as mapping focus from input structure (automatically aligned), $x_{i-1}$ and $x_{i+1}$ as input context, $y_{i-1}$ as output context.</p>
<p><img src="/images/nlu3-7.png" alt="alt img7"></p>
<ul>
<li>Limit to the tag set size, cannot support too many classes for classifying</li>
<li>Usually no need of alignment</li>
</ul>
<hr>
<p><strong>Reference</strong>: Tutorial materials of Zhao Hai(Shanghai Jiao Tong Univ.);</p>
]]></content>
      <tags>
        <tag>NLU</tag>
        <tag>Structural-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hidden Markov model and Viterbi algorithm</title>
    <url>/2020/03/23/2020-03-23-viterbi/</url>
    <content><![CDATA[<p>The <strong>Viterbi algorithm</strong> is a <strong>dynamic programming(DP)</strong> algorithm for finding the most likely sequence of hidden states, called the Viterbi path, which results in a sequence of observed events, especially in the context of Markov information sources and <strong>hidden Markov models(HMM)</strong>.</p>
<blockquote>
<p>Markov property indicates the independency of history information.</p>
</blockquote>
<p>Viterbi algorithm exhibits the very same idea shared with Markov by integrating all the past information to the current state(memorization), thus <strong>decomposing the problem</strong> into sub-problems.</p>
<p>Let us consider a simple case: to find the shortest weighted path (say transition probabilities in the context of HMM) between two node in the graph.</p>
<p><img src="/images/viterbi-1.gif" alt="alt img1"></p>
<p>The central idea of Viterbi is straightforward: in order to reduce the price of searching process, we cut off useless paths(for shortest optimization) immediately when we know their “uselessness”. In the meantime, the best path to a specific node in the current layer $t$ (say time) is definite, which means that we can throw paths otherwise away. The memorization is done such that the total computing price is reduces.</p>
<p>(Remarks: It is like beam search to trim(or called prune) in the space domain, while Viterbi is the trimming in the time domain(since we consider from the perspective of State Transition graph).</p>
<h2 id="Viterbi-algorithm-Pseudocode"><a href="#Viterbi-algorithm-Pseudocode" class="headerlink" title="Viterbi algorithm: Pseudocode"></a>Viterbi algorithm: Pseudocode</h2><p>We firstly define the notion as follows and then narrate the strict definition of such algorithm:</p>
<ul>
<li>Observation space $O = {o_1,o_2,\dots,o_N}$</li>
<li>The state space $S={s_1,s_2,\dot,s_K}$</li>
<li>Algorithm optimized path $X=(x_1,x_2,\dots,x_T)$ as a sequence of states $x_n \in S$</li>
<li>Observation trajectory $Y=(y_1,y_2,\dots,y_T)$ with $y_n \in O$$ which is generated by $X$</li>
<li>Array of initial probabilities $\Pi=(\pi_1,\pi_2,\dots,_pi_K)$ such that $\pi_i$ stores the probability that $x_1==s_i$</li>
<li>Sequence of observations $Y=(y_1,y_2,\dots,y_T)$ such that $y_t == i$ if the observations at time $t$ is $o_i$</li>
<li>transition matrix $A$ of size $K\times K$ such that A_{ij} stores the transition probability of transition from state $s_i$ to state $s_j$</li>
<li>emission matrix $B$ of size $K\times N$ such that $B_{ij}$ stores the probability of observing $o_j$ from state $s_j$</li>
</ul>
<p>Two 2-dimensional matrixs of size $K\times T$ are thus constructed:</p>
<ul>
<li><p>each element $T_1[i,j]$ of $T_1$ stores the probability of the most likely path so far $\hat{X} = (\hat{x_1},\hat{x_2},\dots,\hat{x_T})$ with $\hat{x_j} = s_j$ that generates $Y=(y_1,y_2,\dots,y_T)$</p>
</li>
<li><p>each element $T_2[i,j]$ of $T_2$ stores $\hat{x_{j-1}}$ of the most likely path so far $\hat{X} = (\hat{x_1},\hat{x_2},\dots,\hat{x_{j-1}},\hat{x_{j}}=s_i) ~\forall j, 2 \leq j \leq T$</p>
</li>
</ul>
<p>The table entries $T_1[i,j]$,$T_2[i,j]$ are filled by increasing order of $K\cdot j+i$</p>
<script type="math/tex; mode=display">T_1[i,j] = \max_k (T_1[k,j-1] \cdot A_{ki} \cdot B_{i y_j} )</script><p>and</p>
<script type="math/tex; mode=display">T_2[i,j] = arg\max_k(T_1[k,j-1]\cdot A_{ki})</script><p>The Pseudocode of Viterbi:</p>
<p><img src="/images/viterbi-2.png" alt="alt img2"></p>
<h3 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h3><p>If given HMM with its essential elements: state space $S$, initial probabilities $\pi_i$ and transition probabilities $a_{i,j}$, and our observation trajectory $(y_1,y_2,\dots,y_T)$, we then compute the most likely sequence $(x_1,x_2,\dots,x_T)$ the produce the observations.</p>
<blockquote>
<p>We compute only the possible combinations between current two layers.</p>
</blockquote>
<h4 id="Why-bother-discussing-probability"><a href="#Why-bother-discussing-probability" class="headerlink" title="Why bother discussing probability?"></a>Why bother discussing probability?</h4><p>In fact, it is way more powerful than that of optimal path search algorithm, when it comes to stochastic process. Hidden Markov model describes that the system being modeled is assumed to be a Markov process with unobservable (i.e. hidden) states.</p>
<p>In summary, therefore, using DP to solve the prediction in HMM is Viterbi algorithm. Firstly we compute each probabilities(Markov) for transition between the sequence. Then we trace back to collect the nodes thus obtain the optimal path.</p>
<h2 id="What-does-“Hidden”-mean"><a href="#What-does-“Hidden”-mean" class="headerlink" title="What does “Hidden” mean?"></a>What does “Hidden” mean?</h2><p>Consider the elements of HMM, the initial probabilities and state transition probabilities are, however causative, beyond the reach of our observation.</p>
<p>There are two main assumption behind:</p>
<ul>
<li>homogeneous Markov property</li>
<li>independent observation: observance only depends on the current state of Markov chain</li>
</ul>
<p>Observation probability distribution provides “how to” generate observation at a time given state.</p>
<hr>
<p><strong>Reference</strong>:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="noopener">Wikipedia: Viterbi algorithm</a></li>
<li>Statistical Learning methods, by Li Hang (2012)</li>
</ul>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Dynamic-programming</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Markov Decision Process</title>
    <url>/2020/03/20/2020-03-20-mdp/</url>
    <content><![CDATA[<p><strong>Markov Process(MP)</strong> is an important model in reinforcement learning. It provides the framework for description of agent’s state-reward-action chain. MP can be subsequently divided into <strong>Markov Reward Process(MRP)</strong> and <strong>Markov Decision Process(MDP)</strong>, respectively based on different measures.</p>
<h2 id="Elements-of-RL"><a href="#Elements-of-RL" class="headerlink" title="Elements of RL"></a>Elements of RL</h2><p>In the first section, we need to point out the definition, which is essential to understand MRP and MDP better.</p>
<p>An RL agent may include one or more of these components:</p>
<ul>
<li><p>Policy: agent’s behaviour function</p>
<ul>
<li>it is a <strong>map</strong> from state to action</li>
<li>Deterministic policy: $a=\pi(s)$</li>
<li>Stochastic policy:<script type="math/tex; mode=display">\pi(a|s) = P[A_t=a|S_t=s]</script></li>
</ul>
</li>
<li><p>Value function: how good is each state and/or action</p>
<ul>
<li>a prediction of future reward</li>
<li>used to evaluate the goodness/ badness of <strong>states</strong> and therefore select between actions<script type="math/tex; mode=display">v_\pi(s) = E_\pi [R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3}+ \dots|S_t =s ]</script></li>
</ul>
</li>
<li>Model: agent’s representation of the environment<ul>
<li>predict what the environment will do next</li>
<li>transitions: $p$ predicts the next state<script type="math/tex; mode=display">p^a_{ss'} = P[S_{t+1} = s'| S_t=s.A_t = a]</script></li>
<li>Rewards: R predicts the next(immediate) reward,e.g.<script type="math/tex; mode=display">R_{s}^a = E[R_{t+1} | S_t =s A_t = a]</script></li>
</ul>
</li>
</ul>
<p>With all of these building blocks, we can go on talking about the Markov Process with its variants.</p>
<h2 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h2><p>The central idea of MP is the <strong>Memoryless</strong>: “the future is independent of the past given the present”, i.e., once the current state is known, the history may be thrown away since the state itself is a sufficient statistic of the future.</p>
<p>_[Def]_ A state $S_t$ is Markov if and only if</p>
<script type="math/tex; mode=display">P[S_{t+1}|S_t] = P[S_{t+1}|S_1,\dots,S_t]</script><p><strong>State transition probability</strong> is defined by</p>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s]</script><p>Collection of transition probabilities from all states $s$ to all successor $s’$ is denoted by ST matrix p:</p>
<script type="math/tex; mode=display">p =
\begin{bmatrix}
p_{11} & \dots & p_{1n} \\
\vdots &  \vdots & \vdots \\
p_{n1} & \dots & p_{1n}
\end{bmatrix}</script><p><strong>where each row of the matrix sums to $1$</strong>.</p>
<p>A Markov Process (or Markov Chain) is a tuple $<S, p >$, i.e., a sequence of random states $S_1,S_2,\dots$ with the Markov property.</p>
<ul>
<li>$S$ is a (finite) set of states</li>
<li>$p$ is a state transition probability matrix</li>
</ul>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s]</script><h2 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h2><p>![alt img1]</p>
<p>The goal of agent is, informally speaking, to maximize _the total amount of reward it received._ Not only the immediate reward $R_{t+1}$, but in a long run.</p>
<p>A Markov reward process(MRP) is a Markov chain with <strong>values</strong>, as such tuple $<S,p,R,\gamma>$</p>
<ul>
<li>$S$ is a (finite) set of states</li>
<li>$p$ is a state transition probability matrix</li>
</ul>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s]</script><ul>
<li>$R$ is a reward function(of state $s$)</li>
</ul>
<script type="math/tex; mode=display">R_s = E[R_{t+1}|S_t=s]</script><ul>
<li>$\gamma$ is a discount factor, $\gamma \in [0,1]$</li>
</ul>
<p>Thus, we have designed a manner to give reward “label” to each state $s$.</p>
<h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><p>The <strong>Return</strong> $G_t$ is the total discounted reward from time-step $t$</p>
<script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^\infin \gamma^k R_{t+k+1}</script><ul>
<li>The discount $\gamma \in [0,1]$ is the <strong>present value</strong> of future rewards(_decay weighted_)</li>
<li>The value of receiving reward $R$ after $k+1$ time-steps is $\gamma^k R$</li>
<li>This values immediate reward above delayed reward.<ul>
<li>$\gamma \to 0$ leads to “myopic” evaluation</li>
<li>$\gamma \to 1$ leads to “far-sighted” evaluation</li>
</ul>
</li>
</ul>
<p><strong>Remarks:</strong></p>
<ul>
<li>Avoids infinite returns in cyclic Markov processes<ul>
<li>If the task is continuous, the final time step would be $T = \infin$</li>
</ul>
</li>
<li>Uncertainty about the future may not be fully represented</li>
<li>It is sometimes possible to use undiscounted Markov reward processes (i.e. $\gamma = 1$), e.g. if all sequences terminate, which we call episodic</li>
<li>$(*)$ If the reward is financial, immediate rewards may earn more interest than delayed rewards</li>
<li>$(*)$ Animal/human behaviour shows preference for immediate reward<br>task.</li>
</ul>
<h3 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h3><p>The value function $v(s)$ gives the long-term value of state $s$, i.e., an estimation of <strong>how good it is</strong> for the agent to be in the given state.</p>
<p>The state value function $v(s)$ of an MRP is the <strong>expected return</strong> starting from state $s$</p>
<script type="math/tex; mode=display">v(s) = E[G_t|S_t = s ]</script><p>If we expand $G_t$ in the R.H.S. term, we have the <strong>Bellman Equation</strong>:</p>
<script type="math/tex; mode=display">v(s) =E[G_t|S_t = s ]= E[R_{t+1} + \gamma R_{t+2} + \dots |S_t = s ] = E[R_{t+1} + \gamma v(S_{t+1} | S_t = s)]</script><p>![alt img2]<br>It can be interpreted as iterative decompsition into two parts:</p>
<ul>
<li>immediate reward $R_{t+1}$</li>
<li>discounted <strong>value</strong> of successor state $\gamma v(S_{t+1})$</li>
</ul>
<p>![alt img3]</p>
<script type="math/tex; mode=display">v(s) = R_s + \gamma \sum_{s' in S} p_{ss'}v(s')</script><p>Write Bellman Equation as matrix form:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
v(1) \\
\vdots  \\
v(n)
\end{bmatrix} =
\begin{bmatrix}
R_1 \\
\vdots  \\
R_n
\end{bmatrix}+ \gamma
\begin{bmatrix}
p_{11} & \dots & p_{1n} \\
\vdots &  \vdots & \vdots \\
p_{n1} & \dots & p_{1n}
\end{bmatrix}+
\begin{bmatrix}
v(1) \\
\vdots  \\
v(n)
\end{bmatrix}</script><script type="math/tex; mode=display">v = R + \gamma p v</script><p>It is such a linear equation that can be solved directly:</p>
<script type="math/tex; mode=display">(1- \gamma p)v = R</script><script type="math/tex; mode=display">v = (1- \gamma p)^{-1} R</script><p>However, computational complexity is $O(n^3)$ for $n$ states, and thus direct solution is only possible for small MRPs.</p>
<p>We may try <strong>iterative methods</strong> like Dynamic programming, Monte-Carlo evaluation and so on.</p>
<h2 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h2><p>A Markov Decision Process(MDP) is a Markov reward process with <strong>decisions</strong>. It is an environment in which all states are Markov, represented as a tuple $<S,A,p,R,\gamma>$</p>
<ul>
<li>$S$ is a finite set of states</li>
<li>$A$ is a finite set of actions</li>
<li>$p$ is a state transition probability matrix</li>
</ul>
<script type="math/tex; mode=display">p_{ss'} = P[S_{t+1} = s'|S_t=s,A_t=a]</script><ul>
<li>$R$ is a reward function(of state $s$)</li>
</ul>
<script type="math/tex; mode=display">R_s = E[R_{t+1}|S_t=s,A_t = a]</script><ul>
<li>$\gamma$ is a discount factor, $\gamma \in [0,1]$</li>
</ul>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>A policy $\pi$ is a ditribution over actions given states, say <strong>weights of each transition</strong> at specific state.</p>
<script type="math/tex; mode=display">\pi (a|s) = P[A_t = a| S_t = s]</script><ul>
<li>a policy fully defined the behavior of an agent</li>
<li>MDP policies depend on the current state( not history!)</li>
<li>policies are stationary (time-independent)</li>
</ul>
<script type="math/tex; mode=display">A_t ~ \pi(\cdot|S_t), \forall t > 0</script><p>![alt img4]</p>
<p>Remarks:</p>
<ul>
<li>The state sequence $S_1,S_2,\dots$ is a Markov process $<S,p^\pi>$</li>
<li>THe state and reward sequence $S_1,R_2,S_2,\dots$ is a Markov reward process $<S,p^\pi,R^\pi,\gamma>$</li>
</ul>
<p>where</p>
<script type="math/tex; mode=display">p^\pi_{ss'} = \sum_{a \in A} \pi(a|s)p^a_{ss'}</script><script type="math/tex; mode=display">R^\pi_{s} = \sum_{a \in A} \pi(a|s)p^a_{s}</script><p>The <strong>state-value function</strong> $v_\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\pi$</p>
<script type="math/tex; mode=display">v_\pi(s) = E_\pi [G_t|S_t=s]</script><p>The <strong>action-value function</strong> $q_\pi(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$</p>
<script type="math/tex; mode=display">q_\pi(s,a) = E_\pi [G_t|S_t=s,A_t=a]</script><p>![alt img5]</p>
<p>In fact, these two functions can also be decomposed by following Bellman Equation.</p>
<p>![alt img6]</p>
<p>![alt img7]</p>
<h2 id="Optimal-value-function-trained-result"><a href="#Optimal-value-function-trained-result" class="headerlink" title="Optimal value function (trained result)"></a>Optimal value function (trained result)</h2><p>At last, we introduce the optimal value function, i.e., the maximum ( state-, action- )value function over all policies. Like:</p>
<script type="math/tex; mode=display">v_\ast (s) = max_{\pi} v_ast (s)</script><ul>
<li>the optimal value functiion specifies the best possible performance in the MDP</li>
<li>MDP is already <strong>“solved”</strong> when we know the optimal value</li>
</ul>
<p>Under this goal, we can define a partial ordering over policies:</p>
<script type="math/tex; mode=display">\pi \geq \pi' ~ if ~ v_\pi(s) \geq v_{\pi_\ast}(s), \forall s</script><p><strong>Th.</strong></p>
<ul>
<li>There exists an optimal policy $\pi$, which is a recommended way to make action</li>
<li>All <strong>optimal policies achieve</strong> the optimal value function</li>
<li>All <strong>optimal policies achieve</strong> the optimal action-value function</li>
</ul>
<p>An <strong>optimal policy</strong> can be found by maximizing over $q_\ast(s,a)$</p>
<script type="math/tex; mode=display">\pi_ast (a|s) = 1, if a = argmax_{a \in A} q_\ast (s,a) ~( 0 otherwise )</script><p>_( There is always a deterministic optimal policy for any MDP. )_</p>
<p>Remarks:</p>
<ul>
<li>Bellman Optimality Equation is non-linear</li>
<li>No closed form solution ( in general )</li>
<li>iterative solution methods include<ul>
<li>Value Iteration</li>
<li>Policy Iteration</li>
<li>Q-Learning</li>
<li>Sarsa</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Reference</strong>:</p>
<ul>
<li>Wikipedia-<a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener">reinforcement Learning</a>;</li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides</a> of Prof. David Silver at UCL</li>
</ul>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Reinforcement-leraning</tag>
        <tag>MDP</tag>
      </tags>
  </entry>
  <entry>
    <title>Unsupervised structural learning for NLU</title>
    <url>/2020/03/18/2020-03-18-nlu-note1/</url>
    <content><![CDATA[<p>If we regard the NLP task as structural learning, we are in the position of unsupervised learning. And this post will give a brief introduction to such structural learning tasks.</p>
<h2 id="Unsupervised-structural-learning-for-NLU"><a href="#Unsupervised-structural-learning-for-NLU" class="headerlink" title="Unsupervised structural learning for NLU"></a>Unsupervised structural learning for NLU</h2><p>We might as well firstly stress an important concept: <strong>Probability distribution</strong>. In machine learning algorithm, probability-based methods occupy a incredible fraction such as Naive Bayes. In NLP, similarly, what we actully do in the way of direct statistics, is constructing the probability distribution of each gram, or human-defined language unit.</p>
<p>Probability distribution naturally <strong>depict the data characteristics</strong> from a statistical way. Sometimes, we extend it to a <strong>general scoring scheme</strong> over the UNIT for our concern, namely, say _gram_.</p>
<p>Typically, we have n-gram language model with the <em>maximum likelihood estimation:</em></p>
<script type="math/tex; mode=display">P(n-gram) = \frac{Count(n-gram)}{Count( (n-1)-gram )}</script><p>and we also adopt the <strong>smoothing</strong> strategies for those outside the training corpus.</p>
<p>Now let us start from the simplest part of structural learning.</p>
<h4 id="Segmentation-over-sequence"><a href="#Segmentation-over-sequence" class="headerlink" title="Segmentation over sequence"></a>Segmentation over sequence</h4><p>This is a special tokenization task and the difficulty varys extremely as for different language. Such as:</p>
<ul>
<li>Chinese word segmentation(CWS)</li>
<li>English <strong>subword</strong> segmentation</li>
</ul>
<h2 id="Segmentation-over-some-scoring-framework"><a href="#Segmentation-over-some-scoring-framework" class="headerlink" title="Segmentation over some scoring framework"></a>Segmentation over some scoring framework</h2><p>A word candidate (n-gram) list, i.e., a <strong>lexicon</strong>:</p>
<script type="math/tex; mode=display">W={w_i, g(w_i), i=1,2,\dots,n}</script><p>where $w_i$ is a n-gram, $g(w_i)$ is its corresponding goodness score.</p>
<p>There are two segmentation schemes:</p>
<ol>
<li>Viterbi decoding: The best segmentation manner $S^\ast$ for a full text $T$ as:</li>
</ol>
<script type="math/tex; mode=display">S^\ast = arg\max_S \sum_{i=1}^m g(w_i)</script><ol>
<li>Greedy Maximum Matching decoding: works on $T$ to output the best current word $w^\ast$ <strong>repeatedly</strong> with $T= t^\ast$ for the next:</li>
</ol>
<script type="math/tex; mode=display">{w^\ast,t^\ast} = arg\max_T g(w)</script><p><em>(each tuple for one time)</em></p>
<h2 id="The-goodness-Measure-from-statistics"><a href="#The-goodness-Measure-from-statistics" class="headerlink" title="The goodness Measure from statistics"></a>The goodness Measure from statistics</h2><p>Goodness score is defined as a statistical measure to indicate how <strong>independent an n-gram is</strong>. For what:</p>
<ul>
<li>ranking collocation which is stable, non-tribial n-gram.</li>
<li>word segmentation as for wordhood likelihood</li>
<li>subword like segmentation for alphabetical languages( eg. English )</li>
<li>CWS task</li>
</ul>
<p>Theoretically, the higher goodness score for a candidate, the more <strong>possible</strong> it is to be a <strong>true word</strong>. Remember, different GMs will often give their <strong>own meaningful ranking</strong>.</p>
<h3 id="Frequency"><a href="#Frequency" class="headerlink" title="Frequency"></a>Frequency</h3><p>Just counting for every n-grams.</p>
<script type="math/tex; mode=display">g_f(n-gram) = count (n-gram)</script><p>Two improvements:</p>
<ul>
<li>The longest wins<ul>
<li>if two partially overlapped n-grams have the <strong>same frequency</strong> in the input corpus, then the shorter one is discarded as a redundant word candidate</li>
<li>eg. if <em>Count(New York City) == Count(York City)</em> -&gt; the former adopted</li>
</ul>
</li>
<li>Filtering<ul>
<li>Positive, keep those with POS pattern templates<ul>
<li>Noun, adjective, …</li>
</ul>
</li>
<li>Negaive, remove stop words<ul>
<li>meaningless</li>
</ul>
</li>
</ul>
</li>
<li>Extract any bigrams for collocation detection ranking</li>
</ul>
<h3 id="Accessor-Variety-AV"><a href="#Accessor-Variety-AV" class="headerlink" title="Accessor Variety (AV)"></a>Accessor Variety (AV)</h3><p>The AV of an n-gram $w_i$ is</p>
<script type="math/tex; mode=display">AV(w_i) = min \{L_av(w_i), R_av(w_i) \}</script><p>where the left and right accessor variety are, respectively, the number of <strong>distinct predecessor and successor grams</strong> (defined unit, can be word).</p>
<ul>
<li>Logarithmic value of AV is actually used as the goodness score.</li>
</ul>
<p>For example:<br>$T$ = {<br>  this is a student<br>  it is a book<br>  he is working<br>}</p>
<p>Then</p>
<p>$AV(‘is’)$ = min{<br>$L_{av} = 3$<br>$R_{av} = 2$<br>}</p>
<p>similarly,</p>
<p>$AV(‘is’) = 2$<br>$AV(‘is a’) = 2$</p>
<h3 id="Branch-entropy-BE"><a href="#Branch-entropy-BE" class="headerlink" title="Branch entropy (BE)"></a>Branch entropy (BE)</h3><p>The BE of a n-gram $w_i$ is:</p>
<script type="math/tex; mode=display">BE(w_i) = min\{L_{be}(w_i), R_{be}(w_i)\}</script><p>where $L_{be}$, $R_{be}$ are respectively the left and right be w.r.t distinct predecessor and successor grams. And the local entropy can be computed by:</p>
<script type="math/tex; mode=display">H(w) = -\sum_{x\in V} p(x|w) log p(x|w)</script><p>It indicates the <strong>average uncertainty</strong> after (or before)  substring in the text, where</p>
<script type="math/tex; mode=display">p(x|w)</script><p>is the co-occurrence probability for $x$ and $w$.</p>
<p>_Remarks: $AV(w)$ is such an extreme value for $BE(w)$, where each grams have a <strong>uniform distribution</strong> over different types of grams._</p>
<h3 id="Description-length-gain-DLG"><a href="#Description-length-gain-DLG" class="headerlink" title="Description length gain (DLG)"></a>Description length gain (DLG)</h3><p>DLG score for $w=x_{i,j}$ is computed from the <strong>entire text</strong> $X$:</p>
<script type="math/tex; mode=display">DLG(w) = LG(X) - LG(X[w\to x_{i,j}] + w)</script><p>(1) $X[w\to x_{i,j}]$ represents all $x_{i,j}\in X$ are replaced by the notation $w$, and $+ w $ means concatenation operation.</p>
<p>(2) $LG(X)$ can be computed by(over entire text $X$):</p>
<script type="math/tex; mode=display">LG(X) = = - |X| \sum_{x\in V} p(x) log p(x)</script><p>where $V$ is vocabulary of $X$, and $p(x)$ is $x$’s frequency in $X$.</p>
<h3 id="Point-wise-Mutual-information-PMI"><a href="#Point-wise-Mutual-information-PMI" class="headerlink" title="Point-wise Mutual information (PMI)"></a>Point-wise Mutual information (PMI)</h3><p>PMI of n-gram $w=x_{i,j}$ can be computed by</p>
<script type="math/tex; mode=display">PMI(w) = log(p(w)) - \sum_{k=i}^j log p(x_i) = log \frac{p(w)}{p(x_i) \dots p(x_j)}</script><p>This type of mutual information is roughly a measure of how much one word tells us about the other.</p>
<h3 id="Student-T-test"><a href="#Student-T-test" class="headerlink" title="(Student)T-test"></a>(Student)T-test</h3><p>Just like the $t$-distribution in statistics, the T-test score of n-gram $w=x{i.j}$ is computed by:</p>
<script type="math/tex; mode=display">t(w) = \frac{logp(w) - \sum_{k=i}^j log p(x_i) }{\sqrt{ \frac{p(w)(1-p(w)) }{|X| }}}</script><p>where $p(x)$ indicates frequency or probability $x$ in the entire text $X$.</p>
<p>In fact, it is in a form as PMI over another distribution: Bernoulli.</p>
<h3 id="Pruning-the-word-candidate-list-preprocessing"><a href="#Pruning-the-word-candidate-list-preprocessing" class="headerlink" title="Pruning the word candidate list: preprocessing"></a>Pruning the word candidate list: preprocessing</h3><p>Goodness scores provide an idea for pruning: Except for setting the maximal $n$(n-gram), we may directly discard all word candidates whose goodness scores are below a predefined threshold (not limited to unigrams).</p>
<p>Remarks: A decoding algorithm may require the goodness score of all single-character (unigram) candidates. However, most Goodness scores for unigram are $0$, or even $&lt;0$, and even worse, goodness function does not support computation of unigram.<br>We can:</p>
<ul>
<li>Still computed by the goodness measure, which is applicable only if the measure allows;</li>
<li>Set to 0 as default value for all unigrams.</li>
</ul>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>Here is a standard segmentation corpora for evaluation as SIGHAN Bakeoff multi-standard corpora. And the <a href="http://sighan.cs.uchicago.edu/bakeoff2005/" target="_blank" rel="noopener">SIGHAN Bakeoff-2</a> is commonly used at present and open-sourced.</p>
<h3 id="Some-insights-why-subword-segmentation"><a href="#Some-insights-why-subword-segmentation" class="headerlink" title="Some insights: why subword segmentation?"></a>Some insights: why subword segmentation?</h3><p>Deep learning for NLP relies on word embedding such a vector representation, nevertheless</p>
<ul>
<li>Word embedding is insufficient<ul>
<li>Out-of-vocabulary (OOV), rare words?</li>
</ul>
</li>
<li>Word embedding is inaccurate<ul>
<li>Introducing character embedding? It is functional often, have a try! But subword is better !</li>
</ul>
</li>
<li>GPU memory is not enough to support too large vocabulary<ul>
<li>NMT training in 12G GPU memory only supports 50K vocabulary at most. GPU memory limited!!</li>
</ul>
</li>
</ul>
<p>eg:</p>
<ul>
<li>is</li>
<li>a</li>
<li>for</li>
<li>cor @@ ners</li>
<li>fire @@ places</li>
<li>protec @@ ts (slightly bad)</li>
<li>announc @@ ement (looks unreasonable)</li>
</ul>
<hr>
<p><strong>Reference</strong>: Tutorial materials of Zhao Hai(Shanghai Jiao Tong Univ.);</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>NLU</tag>
        <tag>Structural-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Supervised learning for NLU - Basics</title>
    <url>/2020/03/18/2020-03-18-nlu-notes2/</url>
    <content><![CDATA[<p>In this post, we will talk about the general case of structual learning, from the perspective of machine learning. First things first, we have to determine the unit for such machine learning task.</p>
<h2 id="Granularity"><a href="#Granularity" class="headerlink" title="Granularity"></a>Granularity</h2><p>We have discussed about the language model, n-gram, where a specific gram is needed to point out. In fact, there are several typical layers of the gram selection:</p>
<ul>
<li>Text (discourse)</li>
<li>Sentence</li>
<li>Word</li>
<li>Character</li>
</ul>
<p>Or you may consider another way such as:</p>
<ul>
<li>subword between word and character, such as <strong>prefix, suffix, stem</strong></li>
<li>paragraph between text and sentence</li>
</ul>
<p><img src="/images/nlunote2-1.png" alt="alt img1"></p>
<p>The granularity selection is like a trade-off task since discourse can give rise to large set of candidate while text is a little too rough for data mining.</p>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>For most of the NLP tasks, it is necessary to perform the <strong>structual learning</strong>, unlike typical classification in machine learning.</p>
<p>Take sentimental analysis as an example:</p>
<p>…</p>
<ul>
<li>Disliking watercraft is not really my thing. _(Negation, inverted word order)_</li>
<li>I’d really truly love going out in this weather! _(Possibly sarcastic)_</li>
<li>Sometimes I really hate RIBs. _(Adverbial modifies the sentiment)_</li>
<li>The results are given below. _(Neutral)_<br>…</li>
</ul>
<p><strong>Label (tag) set:</strong></p>
<ul>
<li>positive</li>
<li>negative</li>
<li>neutral</li>
</ul>
<p><strong>Input units:</strong></p>
<ul>
<li>Sentences</li>
</ul>
<p>And another example called Named entity recognition:</p>
<p>…</p>
<ul>
<li>Unlike <strong>Robert, John Briggs Jr</strong> contacted <strong>Wonderful Stockbrockers Inc</strong> in <strong>New York</strong> and instructed them to sell all his share in <strong>Acme</strong>.<br>…</li>
</ul>
<p><strong>Label (tag) set</strong></p>
<ul>
<li>NE and non-NE</li>
</ul>
<p><strong>Input units</strong></p>
<ul>
<li>Words in sentences</li>
</ul>
<p>What has been showned above is two typical tasks in NLP with different granularity.</p>
<p><em>Remarks</em>:</p>
<ul>
<li>Text is written in multi-layer, i.e., multiple granularities</li>
<li>The processing unit (processing granularity) is not necessarily related to the linguistic granularity.</li>
<li>Processing granularity is adopted catering for abstract modeling requirements.</li>
</ul>
<h2 id="The-essential-of-structual-learning"><a href="#The-essential-of-structual-learning" class="headerlink" title="The essential of structual learning"></a>The essential of structual learning</h2><p>NLP is generally about structual learning over the units as nodes in the graph. In fact, the most challenge of NLP learning is two-fold:</p>
<ol>
<li>How to make structure decomposition</li>
</ol>
<p>This is not straightforward task in terms of the typical classification of ML. Simple In/Out system will often practically fail for most NLP problems.</p>
<p>Why? Our goal of classification may ask to give a ton of targeted classes to be learned while the data provided is rather limited. This will inevitably cause the data sparseness issue and paucity of generalization ability.</p>
<p>However, a few NLP task may be quickly converted as <strong>effective classification</strong></p>
<ol>
<li>How to make unit selection (or called segmentation) and representation</li>
</ol>
<p>The second challenge is aroused due to the <strong>continuous sequence</strong> as a carrier of some language, though in its core natural language is confined in multi-layer structure.</p>
<p>All this is to say, that structual learning is important and required for general NLP tasks(non-classification type like sentimental analysis). We may need to re-formulize, say do structure decomposition, for original data.</p>
<h3 id="Sequence-to-sequence-seq2seq"><a href="#Sequence-to-sequence-seq2seq" class="headerlink" title="Sequence to sequence( seq2seq )"></a>Sequence to sequence( seq2seq )</h3><p>Sequence to sequence is a class of NLP task, which is defined as:</p>
<ul>
<li>Input: linear sequence</li>
<li>Output: linear sequence</li>
</ul>
<p>_(No other constraint)_</p>
<p><strong>Task instances of seq2seq:</strong></p>
<ul>
<li>Sequence labeling ( eg. POS tagging: _noun verb adjective_…)<ul>
<li>Source: language sequence; target: sequence <strong>with same size</strong></li>
</ul>
</li>
<li>Machine translation<ul>
<li>Source: language sequence; target: sequence</li>
</ul>
</li>
<li>Dialogue response generation<ul>
<li>Source: question; target: answers</li>
</ul>
</li>
<li>Summarization<ul>
<li>Source: input paragraph; target: relatively shortened one</li>
</ul>
</li>
</ul>
<p>Even though both input and output are linear in form, they have latent nonlinear structures. To handle such challenge, <strong>tree alignment</strong>( syntactic parse tree ) for traditional machine translation can be applied to improve reordering. <strong>Neural models</strong> can also help a lot, which nevertheless has no convincible explanation.</p>
<p><img src="/images/nlunote2-2.png" alt="alt img2"></p>
<hr>
<p><strong>Reference</strong>: Tutorial materials of Zhao Hai(Shanghai Jiao Tong Univ.);</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>NLU</tag>
        <tag>Structural-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Note of Naive Bayes</title>
    <url>/2020/03/17/2020-03-17-bayes/</url>
    <content><![CDATA[<p>There are some algorithms that try to learn mappings directly from the space of inputs $X$ to the labels $Y={0,1}$, which are called <strong>discriminative learning algorithms</strong>; while the <strong>generative learning algorithms</strong> are those trying to model</p>
<script type="math/tex; mode=display">p(x|y),p(y)$$.

For instance, logistic regression modeled
$$p(y|x;\theta) ~as~h_\theta(x) = g(\theta^T x)</script><p>where $g$ is the sigmoid function.</p>
<p>(Remarks: we can explain it as probability model or non-probability one, and we do not discriminate them here. <strong>Bayes classifier</strong>, which will be covered in the following section, belongs to the former.)</p>
<p>For generative case, if $y$ indicates whether an exapmple is a dog(0) or an elephant(1), then</p>
<script type="math/tex; mode=display">p(x|y=0)</script><p>models the distribution of dogs’ _feature_, and</p>
<script type="math/tex; mode=display">p(x|y=1)</script><p>models the distribution of elephants’ _feature_.</p>
<p>We claim the terminology as follows:</p>
<ul>
<li>prior: “expert knowledge” indicating the general distribution</li>
</ul>
<script type="math/tex; mode=display">p(y)</script><ul>
<li>posteriori: such a conditional probability based on some known facts(_Bayes rule_)</li>
</ul>
<script type="math/tex; mode=display">p(y|x) = \frac{p(x|y) p(y)}{p(x)}</script><p>where the denominator is given by <strong>Law of total probability</strong>：</p>
<script type="math/tex; mode=display">p(x) = \sum_{y_i \in Y} p(x|y=y_i)p(y=y_i) = p(x|y=y_0)p(y=y_0) +p(x|y=y_1)p(y=y_1)</script><p>In order to fit the real distribution of data, we make the following optimization:</p>
<script type="math/tex; mode=display">arg \max_y p(y|x) = arg \max_y \frac{p(x|y) p(y)}{p(x)} = arg \max_y p(x|y) p(y)</script><p>_(Remark: where $x$ is vector, indicating the joint probability)_<br>Here, the last equality holds due to the constant of $p(x)$. The R.H.S. of the equality is our goal. Such method is called Maximum a posteriori (MAP) instead of original MLE, in which the goal is</p>
<script type="math/tex; mode=display">arg\max_\theta P(D|\theta)</script><p>It means to optimize the parameters $\theta$ to maximize the probability of <strong>the whole dataset</strong> $D$.</p>
<h2 id="Gaussian-discriminant-analysis-GDA"><a href="#Gaussian-discriminant-analysis-GDA" class="headerlink" title="Gaussian discriminant analysis (GDA)"></a>Gaussian discriminant analysis (GDA)</h2><p>GDA is an instance of generative learning algorithms. In this model, some features need to be pointed out for better understanding it:</p>
<ul>
<li>continuous classification</li>
<li>multivariate normal distribution is involved with its mean vector $\mu$ and covariance matrix $\Sigma$ (symmetric and positive semi-definite)</li>
<li>we assume the posteriori is distributed according to a $N(\mu, \Sigma)$</li>
</ul>
<p>The model is:</p>
<script type="math/tex; mode=display">y ~\sim ~ Bernoulli(\phi)</script><script type="math/tex; mode=display">x|y=0 ~\sim~ N(\mu_0, Sigma)</script><script type="math/tex; mode=display">x|y=1 ~\sim~ N(\mu_1, Sigma)</script><p>where we may assume the identical covariance matrix for each case.</p>
<p>So far we can write out the distribution, this is:</p>
<script type="math/tex; mode=display">p(y) = \phi^y (1-\phi)^{1-y}, ~y \in \{0,1\}</script><script type="math/tex; mode=display">p(x|y=0) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(- \frac {1}{2}(x-\mu_0)^T \Sigma^{-1}(x-\mu_0))</script><script type="math/tex; mode=display">p(x|y=1) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(- \frac {1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1))</script><p>And in order to obtain the best fitting parameters, we consider the log-likelihood of the data, which is given by</p>
<script type="math/tex; mode=display">l(\phi,\mu_0,\mu_1,\Sigma) = log \prod_{i=1}^m p(x^{(i)},y^{(i)};~\phi,\mu_0,\mu_1,\Sigma )=log \prod_{i=1}^m p(x^{(i)}|y^{(i)};~\phi,\mu_0,\mu_1,\Sigma ) ~ p(y^{(i)};~\phi)</script><p>_(Remarks: likelihood is definitely bound with DATA when discussing, because it indicates the fitting degree of present parameters to data; moreover, log is introduced to decouple the terms; normal parameters is relevant to $x$, thus canceled in the prior)_</p>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><ul>
<li>If have obtained the “trained” parameters, we can generate new instance $x^{n+1}$ through the distribution</li>
<li>Because of the property of exponential family, if viewing the quantity</li>
</ul>
<script type="math/tex; mode=display">p(y=1| x;\phi, \mu_0, \mu_1, \Sigma)</script><p>as a function of x. Then it can be expressed in the form (_logsitic regression_)</p>
<script type="math/tex; mode=display">p(y=1| x;\phi, \mu_0, \mu_1, \Sigma) = \frac{1}{1+exp(-\theta^Tx)}</script><p>where $\theta$ is some appropriate function of $\phi, \mu_0, \mu_1, \Sigma$.</p>
<ul>
<li><p>GDA makes strong assumption: the distribution of posteriori to be multivariate gaussian. That leads to some consequences if the modeling assumptions are <strong>wrong</strong>; on the contrary, the GDA is <strong>asymptotically efficient</strong> is data is indeed gaussian.</p>
</li>
<li><p>According to law of large number, GDA is best given very large training sets(large $m$)</p>
</li>
<li><p>In a similar way, we can deduce logistic regression is more robust and less sensitive to incorrect modeling assumptions due to the weaker assumption making</p>
</li>
</ul>
<h2 id="Naive-Bayes-NB"><a href="#Naive-Bayes-NB" class="headerlink" title="Naive Bayes (NB)"></a>Naive Bayes (NB)</h2><p>Let us talk about the conjugative problem: the discrete case (in terms of $x^j$). First things first, _NB_ is based on an important assumption: Naive Bayes assumption</p>
<p>_We assume that the $x_j$’s are conditionally independent given y._</p>
<p>or formally,</p>
<script type="math/tex; mode=display">p(x_1,x_2,\dots|y) = \prod_j p(x_j|y)</script><p>Under the knowledge of $y$, the conditional probability is dividable. The purpose of the assumption is to reduce the dimensionality of parameters (shown as follows).</p>
<p>We use $j$ is one of the discrete (one-hot vector) $n$ features of instance, and our model is parameterized by probabilities:</p>
<script type="math/tex; mode=display">\phi_{j|y=1} = p(x_j = 1|y=1), ~\phi_{j|y=0} = p(x_j = 1|y=0), \phi_{y} = p(y=1)</script><p>Given the $m$ instances and training dataset $T={(x^{(i)},y^{(i)})}, i\in \{1,\dots,m\}$, we write down the <strong>joint likelihood of the data</strong>:</p>
<script type="math/tex; mode=display">L(\phi_{j|y=1},\phi_{j|y=0},\phi_{y}) = \prod_{i=1}^m p(x^{(i)},y^{(i)})</script><p>Maximizing this with respect to $\phi_{j|y=1},\phi_{j|y=0},\phi_{y}$ gives the maximum likelihood estimate(MLE).</p>
<p>It is easy to deduce by take the derivative of $L$ w.r.t. each parameter, in order to make it clear we write in natural language (sample $(x^{(i)},y^{(i)})$ from $T$ ):</p>
<script type="math/tex; mode=display">\phi_{j|y=1} = \frac{Counting(all ~ the ~ sample ~ with ~ x_j =1 ~ \&  ~ y=1)}{Counting(all ~ the ~ sample ~ with ~ y=1) }</script><script type="math/tex; mode=display">\phi_{j|y=0} = \frac{Counting(all ~ the ~ sample ~ with ~ x_j =1 ~ \&  ~ y=0)}{Counting(all ~ the ~ sample ~ with ~ y=0) }</script><script type="math/tex; mode=display">\phi_{y} = \frac{Counting(all ~ the ~  sample ~ with ~ y=1)}{m}</script><p>On top of that, to make a prediction on a new example with features $x$.<br>According to Naive Bayes, we simply calculate (the parameters is what machine learned):</p>
<script type="math/tex; mode=display">p(y=1|x) = \frac{p(x|y=1) p(y=1)}{p(x)} = \frac{(\prod_{j=1}^{n} p(x_j|y=1)) p(y=1) }{ (\prod_{j=1}^{n} p(x_j|y=1)) p(y=1)+(\prod_{j=1}^{n} p(x_j|y=0)) p(y=0) }</script><p>where n is the number of features for an instance $x$.</p>
<h3 id="Laplace-smoothing-add-k"><a href="#Laplace-smoothing-add-k" class="headerlink" title="Laplace smoothing: add-k"></a>Laplace smoothing: add-k</h3><p>There exists a latent problem when applying Bayes classifier to real-life cases: if the probability (parameters) estimated $\phi$ is zero due to the paucity of data, we will yield a $\frac{0}{0}$ situation in the final computation! In order to avoid zero-probability, we can simply add $1$ ( the simplest case) to the numerator and add $k$ to the denominator. Such that</p>
<script type="math/tex; mode=display">\phi_j = \frac{\sum_{i=1}^m ~1\{z^{(i)} = j + 1\} } {m+k}</script><p>where k is the number of sample such that $\sum_{j=1}^k \phi_j = 1$ still holds.</p>
<p>With that tool involved, the Bayes classifier can work on wider range of problems.</p>
<hr>
<p><strong>Reference</strong>: <a href="http://cs229.stanford.edu/notes/cs229-notes2.pdf" target="_blank" rel="noopener">Stanford CS229-note2</a> by Andrew Ng</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Bayes-classifier</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title>Complement of Linear Model</title>
    <url>/2020/03/17/2020-03-17-glm/</url>
    <content><![CDATA[<p>_This post is as complement content of <a href="https://jiaruilu.com/2020/03/10/linear-reg.html" target="_blank" rel="noopener">Linear Regression</a>. We mainly talk about the <strong>generalized linear models(GLM)</strong>._</p>
<p>In the world of statistical learning, the first thing we do to obtain a optimal hypothesis from its space, is to give a framework (model prototype) where the parameters lie in. We will show a broader family of models, called GLMs. Models other than Bernoulli($\phi$) and normal $N(\mu, \sigma^2)$, can be also applied to classification and regression problems.</p>
<h2 id="The-exponential-family"><a href="#The-exponential-family" class="headerlink" title="The exponential family"></a>The exponential family</h2><p>_[Def]_ We say that <strong>a class of distribution</strong> is in the exponential family if it can be written in the form</p>
<script type="math/tex; mode=display">p(y;\eta) = b(y) exp(n^T T(y) - a(\eta))</script><ul>
<li>$\eta $ is called the <strong>natural parameter</strong>( also called the <strong>canonical parameter</strong>) of the distribution</li>
<li>$T(y)$ is the sufficient statistic (it will often be the case that $T(y) =y$)</li>
<li>$a(\eta)$ is the <strong>log partition function</strong></li>
<li>the quantity $e^{-a(\eta)}$ plays the role of a normalization constant</li>
</ul>
<p>We will show that Bernoulli distribution and Gaussian distribution are both in the exponential family.</p>
<p>(1) Bernoulli distribution:</p>
<script type="math/tex; mode=display">p(y;\phi) = \phi^y (1-\phi)^{1-y} = exp(ylog\phi +(1-y)log(1-\phi)) = exp((log(\frac{\phi}{1-\phi}))y + log(1-\phi))</script><p>where:</p>
<ul>
<li>natural parameter is given by $\eta = \log(\phi / (1-\phi ))$</li>
<li>$T(y)=y$</li>
<li>$a(\eta) = -log(1-\phi) = log(1+e^\eta)$</li>
<li>$b(y) = 1$</li>
</ul>
<p><em>Remarks</em>: The interesting thing appears that, if we invert this definition for $\eta$ by solving for $\phi$ in terms of $\eta$, we obtain $\phi = \frac{1}{1+e^{-\eta}}$, which is the <strong>sigmoid function</strong>.</p>
<p>(2) Gaussian function</p>
<script type="math/tex; mode=display">p(y;\mu) = \frac{1}{\sqrt{2\pi}} exp(-\frac{1}{2} (y-\mu)^2) = \frac{1}{\sqrt{2\pi}} exp(-\frac{1}{2}y^2 ) \cdot exp(\mu y-\frac{1}{2} \mu^2)</script><p>which is parameterized as:</p>
<ul>
<li>$\eta = \mu$</li>
<li>$T(y)=y$</li>
<li>$a(\eta) = \mu^2/2 = \eta^2 /2$</li>
<li>$b(y) = exp(-\frac{1}{2}y^2 ) $</li>
</ul>
<h2 id="Constructing-GLMs-how-to"><a href="#Constructing-GLMs-how-to" class="headerlink" title="Constructing GLMs: how to"></a>Constructing GLMs: how to</h2><p>Suppose we want to apply a GLM to Poisson distribution(which is new to our classification/regression problems so far), how to construct a model for that?</p>
<p>More generally, consider a classification or regression problem where we would like to predict the value of $r.v.~y$ as a function of $x$. To derive a GLM for this problem, we will make three assumptions first about the conditional distribution of $y$ given $x$ and our model:</p>
<script type="math/tex; mode=display">~ y|x;\theta \sim Exponentialfamily(\eta)</script><ol>
<li><p>Given $x$, our goal is to predict the expected value of $T(y)$ given $x$. If $T(y)=y$, this means we would like the prediction $h(x)$ output by learned hypothesis $h$ to satisfy $h(x) = E[y|x]$.</p>
</li>
<li><p>The natural parameter $\eta$ and the input $x$ are related linearly: $\eta = \theta^T x$.</p>
</li>
</ol>
<p>The reason why we set the rules for our model is that such design choices enable our model to be equipped with good properties</p>
<p>In the last part of our discussion, we will talk about two examples of derived from GLMs: Logistic Regression and Softmax Regression.</p>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>Here we are interested in binary classification, so $y \in \{0,1\}$. Given that, it is natural to turn to Bernoulli family of distributions to model the conditional distribution of $y$ given $x$. In the formulation of Bernoulli distribution as an exponential family distribution, we had $\phi = 1/(1+e^{-\eta})$. And we note that:</p>
<script type="math/tex; mode=display">y|x;\theta \sim Bernoulli(\phi) \to E[y|x;\theta] = \phi</script><p>So we get hypothesis as:</p>
<script type="math/tex; mode=display">h_\theta (x) = E[y|x;\theta] = \phii = 1/(1+e^{-\eta}) = 1/(1+e^{-\theta^T x})</script><p>Now we know: once we assume that $y$ conditioned on $x$ is Bernoulli, it arises as a consequence of the definition of GLMs and exponential family distribution.</p>
<p>(<strong>Remarks*: the function $g$ giving the distribution’s mean as a function of $\eta$ is called the </strong>canonical response function<strong>($g^{-1}$ is called the </strong>canonical link function**). Thus, the canonical response function for the Gaussian family is just the _identify function_; and the canonical response function for the Bernoulli is the _logistic function_.</p>
<h3 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h3><p>Consider a classification problem in which the response variable $y$ is multivariate: $y \in \{1,2,\dots,k\}$. We will thus model it as distributed according to a <strong>multinomial distribution</strong>.</p>
<p>Similarly, we have: (the definition of notation is left out, sorry about that since it is also understandable)</p>
<script type="math/tex; mode=display">p(y;\phi)  \\
= \phi^{1\{y=1\}} \dots \phi^{1\{y=k\}} \\
= \phi^{1 \{ y=1 \} }  \dots \phi^{ 1 - \sum_{i=1}^{k-1} 1 \{ y=i \} }\\
= exp((T(y))_1 log(\phi_1) + \dots + (1- \sum_{i=1}^{k-1} (T(y))_i\} ) log(\phi_k))\\
= exp((T(y))_1 log(\phi_1 /\phi_k) + \dots + (T(y) )_{k-1} log(\phi_{k-1}/ \phi_k)  + log(\phi_k))\\
= b(y) exp(\eta^T T(y) - a(\eta))</script><p>( * $1- \sum_{i=1}^{k-1} 1\{y=i\}$ due to the sum of total probabilities gives $1$, they are not independent actually)</p>
<p>where</p>
<ul>
<li>$\eta = [log(\phi_1 /\phi_k),\dots,log(\phi_k-1 /\phi_k)]^T$</li>
<li>$a(\eta) = -log(\phi_k)$</li>
<li>$b(y) = 1$</li>
</ul>
<p>Thus, the link function is given by:</p>
<script type="math/tex; mode=display">\eta_i = log\frac{\phi_i}{\phi_k}</script><p>in particular, we also defined $\eta_k = log(\frac{\phi}{\phi}) = 0$</p>
<p>And then we have:</p>
<script type="math/tex; mode=display">e^{\eta_i} = \frac{\eta_i}{\eta_k}</script><script type="math/tex; mode=display">\phi_k e^{\eta_i} = \phi_i</script><script type="math/tex; mode=display">\phi_k \sum_{i=1}^k e^{\eta_i} = \sum_{i=1}^k \phi_i =1</script><p>This implies that $\phi_k = 1/\sum_{i=1}^k e^{\eta_i} $, which can be further written as:</p>
<script type="math/tex; mode=display">\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}</script><p>This functions mapping from the $\eta$’s to the $\phi$’s is called the <strong>Softmax function</strong>.</p>
<p>By applying our Assumption-3, the $\eta$’s are linearly related to the $x$’s. So we have $\eta_i = \theta_i^T x $ for $i=1,\dots,k-1$. For notational convenience, we can also define $\theta_k = 0$, such that $\eta_k = \theta^T_k x = 0$.</p>
<p>Hence, the Softmax Regression is given by:</p>
<script type="math/tex; mode=display">p(y=i|x;\theta) = \phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}} = \frac{e^{\theta^T_i x}}{\sum_{j=1}^k \theta^T_j x}</script><p>which construct the output vector as $h_\theta(x)$.</p>
<p>( * $\phi$ is usually referred to the probability of some special output value of $y$)</p>
<hr>
<p><strong>Reference</strong>: <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">Stanford CS229-note1</a> by Andrew Ng</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>Tutorial on how to read a paper</title>
    <url>/2020/03/16/2020-03-16-paper-reading/</url>
    <content><![CDATA[<p>This is a note summarizing the core content of the paper “How to read a paper” written by Professor S. Keshav. To avoid redundancy, I construct the central idea of the <strong>Three-Pass Approach</strong> introduced by the paper.</p>
<h2 id="Three-Pass-Approach"><a href="#Three-Pass-Approach" class="headerlink" title="Three-Pass Approach"></a>Three-Pass Approach</h2><h3 id="1-The-first-Pass"><a href="#1-The-first-Pass" class="headerlink" title="1 The first Pass"></a>1 The first Pass</h3><p>_As quick scan to get a bird’s-eye view of the paper._</p>
<p><strong>Purpose</strong>: get the general idea about the paper</p>
<p><strong>Time cost</strong>: five to ten minutes</p>
<p><strong>Steps</strong>:</p>
<ul>
<li>Carefully read the <strong>title, abstract, and introduction</strong></li>
<li>Read the section and sub-section <strong>headings</strong>, but ignore<br>everything else</li>
<li>Read the <strong>conclusions</strong></li>
<li>Glance over the references, mentally ticking off the<br>ones you’ve already read</li>
</ul>
<p><strong>Five Cs to answer</strong>:</p>
<ul>
<li>Category: What type of paper is this? A measurement paper? An analysis of an existing system? A<br>description of a research prototype?</li>
<li>Context: Which other papers is it related to? Which<br>theoretical bases were used to analyze the problem?</li>
<li>Correctness: Do the assumptions appear to be valid?</li>
<li>Contributions: What are the paper’s main contributions?</li>
<li>Clarity: Is the paper well written?</li>
</ul>
<h3 id="2-The-second-pass"><a href="#2-The-second-pass" class="headerlink" title="2 The second pass"></a>2 The second pass</h3><p>_With greater care, ignore details such as proofs. It helps to jot down the key points, or to make comments in the margins._</p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>should be able to grasp the content of the paper</li>
<li>should be able to summarize the main thrust of the paper, with supporting evidence, to someone else</li>
<li>this level of detail is proper for a paper not lying in your speciality</li>
</ul>
<p><strong>Time cost</strong>: up to one hour</p>
<p><strong>Steps</strong>:</p>
<ul>
<li>Look carefully at the <strong>figures, diagrams and other illustrations</strong> in the paper. Pay special attention to graphs.</li>
</ul>
<p>(Remarks: _Are the axes properly labeled? Are results shown with error bars, so that conclusions are statistically significant?_ Common mistakes like these will separate rushed, shoddy work from the truly excellent.</p>
<ul>
<li>Mark relevant unread references for further reading (a good way to learn more about the background of the paper)</li>
</ul>
<p><strong>You may not UNDERSTAND a paper even after the second pass,</strong> because:</p>
<ul>
<li>the subject matter is new to you, with unfamiliar terminology and acronyms</li>
<li>authers may use a proof or experimental technique that you don’t understand such that the bulk of the paper is incomprehensible</li>
<li>the paper may be poorly written with unsubstantiated assertions and numerous forward references</li>
<li>maybe you are tired…</li>
</ul>
<p>Then you can now choose to: (a) set the paper aside, hoping you don’t need to understand the material to be successful in your career, (b) return to the paper later, perhaps after reading background material or (c) persevere and go on to the third pass.</p>
<h3 id="3-The-third-pass"><a href="#3-The-third-pass" class="headerlink" title="3 The third pass"></a>3 The third pass</h3><p>_Key: Attempt to virtually re-implement the paper: that is, making the same assumptions as the authors, re-create the work._</p>
<p><strong>Purpose</strong>:</p>
<ul>
<li>fully understand a paper</li>
<li>identify a paper’s innovations and hidden failings and assumptions</li>
<li>in the end be able to reconstruct the entire structure of the paper from memory as well as identify its pros and cons</li>
</ul>
<p><strong>Time cost</strong>: one hour to five hours (based on fluency)</p>
<p><strong>Steps</strong>:</p>
<ul>
<li>identify and challenge every assumption in every statement</li>
<li>think about how yourself would present a particular idea</li>
</ul>
<p>(Remarks:This comparison of the actual with the virtual lends a sharp insight into the proof and presentation techniques in the paper and you can very likely add this to your repertoire of tools.)</p>
<ul>
<li>also jot down ideas for future work</li>
<li>_(Advanced)_ if allowed, pinpoint implicit assumptions and potential issues with experimental or analytical techniques</li>
</ul>
<hr>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf" target="_blank" rel="noopener">How to Read a Paper</a> by S. Keshav at University of Waterloo.</p>
]]></content>
      <categories>
        <category>Reading</category>
      </categories>
      <tags>
        <tag>Paper-reading</tag>
      </tags>
  </entry>
  <entry>
    <title>EM algorithm from scratch</title>
    <url>/2020/03/15/2020-03-15-em/</url>
    <content><![CDATA[<p><strong>EM</strong> (_Expectation Maximization_) algorithm is an iterative algorithm of wide-range application. It provides a effective method parameter estimation in probability models encompassing <strong>latent variables</strong>. In terms of classic probability models, in which all the variables are observable, the maximum likelihood estimation or Bayes estimation can be functional tools. If one model involves latent variables, we have to turn to a variant of maximum likelihood, the EM algorithm.</p>
<p>The triad elements of EM algorithm:</p>
<ul>
<li>observable variable $Y = (Y_1,Y_2,\dots,Y_n)$, with its observation record $y=(y_1,y_2,\dots,y_n)$ ($n$ is the number of observing times)</li>
<li>latent variable $Z = (Z_1,Z_2,\dots,Z_n)$</li>
<li>parameter of model $\theta$</li>
</ul>
<p>In the EM algorithm, due to our paucity of knowledge for $Z$, the original likelihood function</p>
<script type="math/tex; mode=display">P(Y,Z|\theta)</script><p>fails. Then we instead consider the likelihood function of observable variable $Y$ :</p>
<script type="math/tex; mode=display">P(Y|\theta) = \sum_{Z} P(Z|\theta) P(Y | Z,\theta)</script><p>then our goal is to solve the maximum likelihood for parameter $\theta$:</p>
<script type="math/tex; mode=display">\hat{\theta} = arg \max_\theta log P(Y|\theta)</script><p>Unfortunately, there is no analytic solution for $\hat{\theta}$, and thus we turn our eyes on the iterative method, which is the EM algorithm.</p>
<h3 id="EM-algorithm"><a href="#EM-algorithm" class="headerlink" title="EM algorithm"></a>EM algorithm</h3><p>In: $Y,Z$, joint probability distribution</p>
<script type="math/tex; mode=display">P(Y,Z|\theta)</script><p>conditional distribution</p>
<script type="math/tex; mode=display">P(Z|Y,\theta)</script><p>Out: optimal parameter $\theta$.</p>
<p>Step1: Initialization: $\theta^{(0)}$</p>
<p>Step2:</p>
<p><strong>E step</strong>: The estimate of $\theta$ in the stage $i$ is denoted as $\theta^{(i)}$, in the next stage $i+1$, compute <strong>$Q$ function</strong>:</p>
<script type="math/tex; mode=display">Q(\theta,\theta^{(i)}) = E_z [logP(Y,Z | \theta) | Y,\theta^{(i)}]= \sum_Z logP(Y,Z  | \theta) P(Z | Y,\theta^{(i)})</script><p><strong>M step</strong>: Optimization. solve $\theta$ which maxmizes the $Q$ function, and apply it to the next stage:</p>
<script type="math/tex; mode=display">\theta^{(i+1)} = arg\max_\theta Q(\theta,\theta{(i)})</script><p>Step3: Repeat step2 until convergence.</p>
<h4 id="Remarks"><a href="#Remarks" class="headerlink" title="Remarks:"></a>Remarks:</h4><p>$Q$ function - $Q(\theta,\theta{(i)})$:</p>
<p>The logarithmic likelihood function of complete data is denoted as</p>
<script type="math/tex; mode=display">log P(Y,Z | \theta)</script><p>Its conditional <strong>expectation</strong> with regard to $Y$ under present parameter $\theta^{(i)}$ for $Z$ is called $Q$ function:</p>
<script type="math/tex; mode=display">Q(\theta,\theta{(i)}) = E_z [logP(Y,Z|\theta) | Y,\theta^{(i)}]</script><p>The core of the algorithm is the maximization of $Q$-function.</p>
<h4 id="Features-of-EM-iteration"><a href="#Features-of-EM-iteration" class="headerlink" title="Features of EM iteration:"></a>Features of EM iteration:</h4><ul>
<li>EM algorithm is sentitive to initial value $\theta^{(0)}$;</li>
<li>The likelihood function will increase or achieve local maxima after each stage;</li>
<li><strong>(Th)</strong> If there exist a upper bound for<script type="math/tex; mode=display">P(Y | \theta)</script>then<script type="math/tex; mode=display">L(\theta^{(i)}) = logP(Y|\theta^{(i)})</script>converges into some value $L^{*}$;</li>
<li><strong>(Th)</strong> Under specific conditions for $Q(\theta,\theta{(i)})$ and $L(\theta)$, the convergence point $\theta^*$ of parameter series $\theta^{(i)}$ is stable point of $L(\theta)$;</li>
<li><a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">Jensen’s inequality</a> is applied during the deduction of EM;</li>
<li>The idea of EM is: applying expectation instead of observed values for $Z$ to estimate the $\theta$;</li>
<li>Fitting a mixture of Gaussians is an important application of EM.</li>
</ul>
<hr>
<p><strong>Reference</strong>: Statistical Learning methods, by Li Hang (2012)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>EM</tag>
        <tag>Parameter-estimation</tag>
      </tags>
  </entry>
  <entry>
    <title>Boosting learning - A brief introduction</title>
    <url>/2020/03/14/2020-03-14-boosting/</url>
    <content><![CDATA[<p>Boosting is a common type of ensembling method in machine learning, known as its effective and wide range of application.</p>
<p>The central idea of boosting is that several relatively simple hypotheses are learned by changing the weight (importance) of sample items, thus combining them together to handle more complex tasks, classification and regression. One feature of boosting is that the sub-machines are trained <strong>sequentially</strong>, say one after another.</p>
<p><img src="/images/boosting-1.png" alt="alt img1"></p>
<p><img src="/images/boosting-2.png" alt="alt img2"></p>
<p>According to _Schapire_, if confined in PAC(probably approximately correct) learning framework, a concept is <strong>strongly learnable</strong> (with high accuracy) if and only if the concept is <strong>weakly learnable</strong> (with mediocore accuracy). _Learnable_ concept means that there exists a algorithm in a form of polynomial that can learn it. Meanwhile, finding a weakly learnable algorithm is way easier that find a strongly learnable one, after boosting, however, the former one may outperform the latter. Many approaches are proposed to boost a “easy algorithm” into a “hard algorithm”, and AdaBoost(Adaptive Boosting algorithm) is one of them.</p>
<p>In short, we state the essential elements in boosting classification as follows:</p>
<ul>
<li>a given training dataset (assume all the samples are from identical distribution)</li>
<li>set a rough rule for classification such that base classifier is determined</li>
<li>iteratively find a new base classifier in order to lower the loss function, i.e. errors</li>
<li>output the linear combination of a series of base classifiers</li>
</ul>
<p>However, there are questions remained to answer:</p>
<ul>
<li>How to yield a <strong>new</strong> classifier in each iteration? Because we are always using the same samples<ul>
<li>To change the probability distribution, say weight for each sample after each stage</li>
</ul>
</li>
<li>Then how to change the weight of the data in each stage?<ul>
<li>According to the discrepancy between the output from classifiers in the last stage and real data</li>
</ul>
</li>
<li>How is the combination of base classifiers going on?<ul>
<li>Weighted voting. We assign the classifiers with different weights by the accuracy in their own stages, and then sum up all of them.</li>
</ul>
</li>
</ul>
<h3 id="The-algorithm-AdaBoost"><a href="#The-algorithm-AdaBoost" class="headerlink" title="The algorithm: AdaBoost"></a>The algorithm: AdaBoost</h3><p>We start with the simple case: binary classification.</p>
<p>(1) Suppose we have a series of tuples containing terms of instance-label, say training data $T$:</p>
<script type="math/tex; mode=display">T = {(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)}</script><p>where $x_i \in X \subset R^n$ as set of instance and $y_i \in Y =<br>\{1,-1\}$ as set of label.</p>
<p>(2) Set a rule(framework) to learn the weakly learnable hypotheses $G_m:~ X\to Y, ~~m = 1,2,\dots, M$, which is successively learned in the iteration. For example, if $x_i \in R$, it can be $G_1 = sign(x_i)$.</p>
<p>( $M$ is the total number of base classifiers we want )</p>
<p>(3) Initialize the weight distribution of the whole training data:</p>
<script type="math/tex; mode=display">D_1 = (w_{11}, w_{12}, \dots, w_{1N}), ~w_{1i} = \frac{1}{N}, ~i = 1,2,\dots, N</script><p>(4) For $m=1,2,\dots,M$, repeat:</p>
<p>(4-1) Using $D_m$ for training, we obtain the present base classifier:</p>
<script type="math/tex; mode=display">G_m(x): X\to Y=\{-1,1\}</script><p>(4-2) Compute the error probability of classification on $T$:</p>
<script type="math/tex; mode=display">e_m = P(G_m(x_i) \neq y_i ) = \sum_{i=1}^{N} w_{mi} I (G_m(x_i) \neq y_i)</script><p>(4-3) Compute the coefficient for $G_m(x)$ as follow:</p>
<script type="math/tex; mode=display">\alpha_m = \frac{1}{2} log \frac{1-e_m}{e_m}</script><p>(4-4) Update the weight distribution of training data:</p>
<script type="math/tex; mode=display">D_{m+1} = (w_{m+1,1},w_{m+1,2}, \dots, w_{m+1,N})</script><script type="math/tex; mode=display">w_{m+1} = \frac{w_{mi}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), ~i=1,2,\dots,N</script><p>where $Z_m= \sum_{i=1}^N exp(-\alpha_m y_i G_m(x_i))$ is the normalizing factor, which enables the $D_{m+1}$ to become a probability distribution.</p>
<p>(5) Constructing the linear combination of base classifiers</p>
<script type="math/tex; mode=display">f(x) = \sum_{m=1}^M \alpha_m G_m(x)</script><p>along with the final classifier:</p>
<script type="math/tex; mode=display">G(x) = sign(f(x)= sign(\sum_{m=1}^M \alpha_m G_m(x))</script><p><strong>Remarks</strong>:</p>
<ul>
<li>Initially we have made an assumption that the data have average weighted distribution, i.e., all the data is sampled from the same distribution.</li>
<li>In (4-2), the term $w_{mi} I (G_m(x_i)$ gives the contribution to error $e_m$ from sample $i$, such that weight $w_mi$ shows the <strong>influence</strong> in the stage $m$.</li>
<li>The coefficient $\alpha_m$, which means the importance of $G_m(x)$ in the final combination. When error $e_m$ is small, $\alpha_m$ has relatively large value, engendering $G_m(x)$ an <strong>influential component</strong>.</li>
<li>We utilize the very same data with different weight distribution for iteration.</li>
<li>Because it is we who determine the classifier framework, so new classifier yielded in each stage is in nature the tuple of <strong>parameters</strong>.</li>
</ul>
<h4 id="Explanation-of-AdaBoost-from-modeling-perspective"><a href="#Explanation-of-AdaBoost-from-modeling-perspective" class="headerlink" title="Explanation of AdaBoost: from modeling perspective"></a>Explanation of AdaBoost: from modeling perspective</h4><p>The AdaBoost essentially is the integration of <strong>additve model, exponential loss function, forward stagewise algorithm</strong>, applied for binary classification problem.</p>
<p>Further explanations:</p>
<ul>
<li>Additive model: the final classifier comes from the linear combination, i.e., simple addition of base classifiers.</li>
<li>Forward stagewise algorithm: the components of final classifier are yielded bit by bit and the <strong>sum up</strong> is gradually approaching the real model.</li>
</ul>
<h3 id="Boosting-tree"><a href="#Boosting-tree" class="headerlink" title="Boosting tree"></a>Boosting tree</h3><p>If we alter the AdaBoost algorithm, by changing the framework of base classifier into Decision Tree(DT), then the Boosting tree method is formed. In terms of specific tasks, we can use _Binary classification tree_ or _Binary regression tree_.</p>
<p>In particular, the simple sign function or its variant can be seen as decision stump, which only has two leaf nodes directly connected by root.</p>
<p>Model description:</p>
<script type="math/tex; mode=display">f_M(x)=\sum_{m=1}^M = T(x;\Theta_m)</script><p>The core of such algorithm is similar to the AdaBoost algorithm.</p>
<h3 id="Gradient-boosting-GB"><a href="#Gradient-boosting-GB" class="headerlink" title="Gradient boosting (GB)"></a>Gradient boosting (GB)</h3><p>AdaBoost is slightly limited because of the loss function is confined in the exponential type. However, if we would like to generalise the loss function into a broad field, giving the corresponding boosting method, say how to update in each stage, is absoluately impossible.</p>
<p>For this, gradient boosting is proposed by _Freidman_ to provide the boosting strategy in terms of general loss functions, like the gradient descent(GD). The core is the negative gradient of the loss function at the point of present model:</p>
<script type="math/tex; mode=display">-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><p>which serves as the approximate residual in the boosting regression tree. (<em>remark: In the regression task, the residual $r_{mi}$ of each sample is computed in order to <em>*help yield the following base decision tree</em></em>, which is expected to fit the residuals well.)</p>
<p>Here we give the algorithm of GB:</p>
<p>In: traning set $T = {(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)}$, and $x_i \in X \subset R^n ,y_i \in Y \subset R$; Loss function $L(y,f(x))$;</p>
<p>Out: regression tree $\hat{f} (x)$;</p>
<p>Step1: Initialization</p>
<script type="math/tex; mode=display">f_0(x) = arg\min_c \sum_{i=1}^N L(y_i,c)</script><p>Step2: For each $m = 1,2,\dots, M$, repeat:</p>
<p>(a) for i=1,2,\dots,N, compute</p>
<script type="math/tex; mode=display">r_{mi} = -[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><p>(b) fit a regression tree by $r_{mi}$, and obtain the partition of regions $R_{mj}$ corresponding leaf nodes in tree $m$, $j=1,2,\dots,J$</p>
<p>(c) for $j = 1,2,\dots, J$, compute</p>
<script type="math/tex; mode=display">c_{mj} = arg\min_c \sum_{x_i\in R_{mj}} L(y_i,f_{m-1} (x_i) + c)</script><p>(d) update <script type="math/tex">f_m(x) = f_{m-1}(x) + \sum_{j=1}^J c_{mj} I(x\in R_{mj})</script></p>
<p>Step3: The regression tree obtained</p>
<p><script type="math/tex">\hat{f}(x) = f_M(x) = \sum_{m=1}^{M} \sum_{j=1}^J c_{mj} I(x\in R_{mj})</script>.</p>
<p><strong>Remark</strong>: the negative gradient is used to approach the real residual, which is hard to obtain in general loss function case. But it provides us with the “correct direction” to update the tree, just like what happens in the GD algorithm.</p>
<hr>
<p><strong>Reference</strong>:</p>
<ul>
<li>Statistical Learning methods, by Li Hang (2012)</li>
<li>IMG from <a href="https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/" target="_blank" rel="noopener">https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/</a></li>
</ul>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Boosting Machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>How to organize an academic paper</title>
    <url>/2020/03/14/2020-03-14-paper-writing/</url>
    <content><![CDATA[<p>After introducing some key points of academic writing, there is a integrated writing guide towards the whole paper followed by. _Here! Now._</p>
<h3 id="Purposes"><a href="#Purposes" class="headerlink" title="Purposes"></a>Purposes</h3><ul>
<li>provide you with examples of some of the phraseological ‘nuts and bolts’ of writing</li>
<li>we all need to report their research work</li>
</ul>
<h3 id="Why-do-I-need-them"><a href="#Why-do-I-need-them" class="headerlink" title="Why do I need them"></a>Why do I need them</h3><ul>
<li>to assist you in thinking about the content and organisation of your own writing</li>
<li>appropriate words/phrases of neutral and generic attribute in nature</li>
<li>this does not constitute plagiarism</li>
</ul>
<p>In the rest of this post, several sections will be covered: <strong>Introduction, Reference, Methods, Results, Discussion and Conclusion</strong>, corresponding to the sections of academic articles or essays. Okay! Let us start up the journey!</p>
<h3 id="Introducing-Work"><a href="#Introducing-Work" class="headerlink" title="Introducing Work"></a>Introducing Work</h3><h4 id="Why-Intro-Essential-Elements"><a href="#Why-Intro-Essential-Elements" class="headerlink" title="Why Intro? Essential Elements"></a>Why Intro? Essential Elements</h4><ul>
<li>Common elements include:<ul>
<li>establishing the context, background and/or importance of the topic</li>
<li>giving a brief review of the relevant academic literature</li>
<li>identifying a problem, controversy or a knowledge gap in the field of study</li>
<li>stating the aim(s) of the research and the research questions or hypotheses</li>
<li>providing a synopsis of the research design and method(s)</li>
<li>explaining the significance or value of the study</li>
<li>defining certain key terms</li>
<li>providing an overview of the dissertation or report structure</li>
</ul>
</li>
</ul>
<h4 id="What-is-it"><a href="#What-is-it" class="headerlink" title="What is it?"></a>What is it?</h4><ul>
<li>Slightly less complex, simply inform the reader:<ul>
<li>what the topic is</li>
<li>why it is important</li>
<li>how the rest of writing is organised</li>
</ul>
</li>
<li>Be careful not to commence by stating the purpose of writing</li>
</ul>
<p>CARS model — one of the best known patterns, given by John Swales (1990)</p>
<ul>
<li>Establishing the territory (establishing importance of the topic, reviewing previous work)</li>
<li>Identifying a niche (indicating a gap in knowledge)</li>
<li>Occupying the niche (listing purpose of new research, listing questions, stating the value the work, indicating the structure of the writing)</li>
</ul>
<h4 id="How-to-write-Examples"><a href="#How-to-write-Examples" class="headerlink" title="How to write? Examples"></a>How to write? Examples</h4><ul>
<li><p>Establishing the importance of the topic for the world or society</p>
<ul>
<li>X is fundamental to …</li>
<li>Evidence suggests that X is among the most important factors for …</li>
</ul>
</li>
<li><p>Establishing the importance of the topic as a problem to be addressed</p>
<ul>
<li>The main challenge faced by many researchers is the …</li>
</ul>
</li>
<li><p>Referring to previous work to establish what is already known</p>
<ul>
<li>Previous research has established that …</li>
<li>Several attempts have been made to …</li>
</ul>
</li>
<li><p>Explaining the inadequacies of previous studies</p>
<ul>
<li>However, few writers have been able to draw on any systematic research into …</li>
<li>The research to date has tended to focus on X rather than Y.</li>
<li>Whilst some research has been carried out on X, no studies have been found which …</li>
</ul>
</li>
<li><p>Stating the focus, aim, or argument of a short paper</p>
<ul>
<li>The aim of this essay is to explore the relationship between …</li>
</ul>
</li>
<li><p>Outlining the structure of the paper or dissertation</p>
<ul>
<li>The remaining part of the paper proceeds as follows: …</li>
</ul>
</li>
</ul>
<h3 id="Referring-to-Sources"><a href="#Referring-to-Sources" class="headerlink" title="Referring to Sources"></a>Referring to Sources</h3><h4 id="Why-Refer"><a href="#Why-Refer" class="headerlink" title="Why Refer?"></a>Why Refer?</h4><ul>
<li>it is informed by what is already known, what work has been done before, and/or what ideas and models have already been developed(distinguishing feature of academic writing)</li>
<li>writers frequently make reference to other studies and to the work of other authors</li>
<li>writers guide their readers through this literature</li>
</ul>
<h4 id="What-is-it-1"><a href="#What-is-it-1" class="headerlink" title="What is it?"></a>What is it?</h4><ul>
<li>literature review:<ul>
<li>what is already known about the research topic as a whole</li>
<li>outline the key ideas and theories that help us to understand this</li>
<li>be evaluative and critical of the studies or ideas which are relevant to the current work</li>
</ul>
</li>
</ul>
<p>(For example, you may think a particular study did not investigate some important aspect of the area you are researching, that the author(s) failed to notice a weakness in their methods, or that their conclusion is not well-supported (refer to Being Critical).)</p>
<ul>
<li>verb tenses:<ul>
<li><strong>[General reference]</strong> the present perfect tense (have/has + verb participle) tends to be used</li>
<li><strong>[To specific studies]</strong> the simple past tense is most commonly used</li>
<li><strong>[To the words or ideas of writers]</strong> the present tense is often used if the ideas are still relevant (even if the author is no longer alive)</li>
</ul>
</li>
</ul>
<h4 id="How-to-write"><a href="#How-to-write" class="headerlink" title="How to write?"></a>How to write?</h4><ul>
<li>Previous research:<ul>
<li>methodological approaches taken<ul>
<li>Most research on X has been carried out in …</li>
<li>Much of the X research has focused on identifying and evaluating the …</li>
</ul>
</li>
<li>what has been established or proposed<ul>
<li>Traditionally, it has been argued that … (e.g. Smith, 1960; Jones, 1972).</li>
<li>Surveys such as that conducted by Smith (1988) have shown that …</li>
<li>It has been suggested that levels of X are independent of the size of the Y (Smith et al., 1995).</li>
</ul>
</li>
</ul>
</li>
<li>Reference to a previous investigation:<ul>
<li>researcher prominent<ul>
<li>Jones et al. (2001)    showed that reducing X to 190oC decreased … (see Figure 2).</li>
<li>performed a similar series of experiments in the 1960s to show that …</li>
</ul>
</li>
<li>time prominent<ul>
<li>In 1990, Al-Masry et al. reported a new and convenient synthetic procedure to obtain …</li>
</ul>
</li>
<li>investigation prominent<ul>
<li>A significant analysis and discussion on the subject was presented by Smith (1988).</li>
<li>In a recent cross-sectional study, Smith (2016) investigated whether …</li>
</ul>
</li>
<li>topic prominent<ul>
<li>To determine the effects of X, Jones et al. (2005) compared …</li>
</ul>
</li>
</ul>
</li>
<li>Reference to what other writers do in their text<ul>
<li>Smith (2000) discusses the challenges and strategies for facilitating and promoting …</li>
<li>In her analysis of …, Caroline (2012) identifies five characteristics of …</li>
</ul>
</li>
<li>Reference to another writer’s idea or position<ul>
<li>Smith (2013)    argues that preventative medicine is far more cost effective, and therefore better adapted to the developing world.</li>
<li>Smith argues that …. Likewise, Wang (2012) holds the view that …</li>
<li>Zhao (2002) notes that …. Other researchers, however, who have looked at X, have found … Jones (2010), for example, …</li>
</ul>
</li>
<li>Summarising the review or parts of the review<ul>
<li>Together, these studies indicate that …</li>
<li>Overall, there seems to be some evidence to indicate that …</li>
</ul>
</li>
</ul>
<h3 id="Describing-Methods"><a href="#Describing-Methods" class="headerlink" title="Describing Methods"></a>Describing Methods</h3><h4 id="Why-Methods"><a href="#Why-Methods" class="headerlink" title="Why Methods?"></a>Why Methods?</h4><ul>
<li>writers give an account of how they carried out their research</li>
<li>should be clear and detailed enough for another experienced person to repeat the research and reproduce the results</li>
</ul>
<h4 id="What-is-it-2"><a href="#What-is-it-2" class="headerlink" title="What is it?"></a>What is it?</h4><ul>
<li>where the methods chosen are new, unfamiliar or perhaps even controversial</li>
<li>where the intended audience is from many disciplines</li>
<li>tend to be much more extensive</li>
<li>for most the verbs are written in the simple past tense</li>
</ul>
<h4 id="How-to-write-1"><a href="#How-to-write-1" class="headerlink" title="How to write?"></a>How to write?</h4><ul>
<li>Giving reasons why a method was adopted or rejected<ul>
<li>The semi-structured approach was chosen to obtain further in-depth information on the …</li>
<li>The study uses qualitative analysis in order to gain insights into …</li>
</ul>
</li>
<li>Indicating the use of an established method<ul>
<li>Samples were analysed for X as previously reported by Smith et al. (2012).</li>
<li>This compound was prepared by adapting the procedure used by Jones et al. (1990)…</li>
</ul>
</li>
<li>Describing the characteristics of the sample<ul>
<li>The participants were divided into two groups based on their performance on …</li>
</ul>
</li>
<li>Describing the process:<ul>
<li>infinitive of purpose<ul>
<li>To determine whether …, the cells were incubated for …</li>
<li>In order to understand how X regulates Y, a series of transfections was performed.</li>
</ul>
</li>
<li>expressing purpose with ‘for’<ul>
<li>For the purpose of analysis, two segments were extracted from each …</li>
</ul>
</li>
<li>verbs used in the passive<ul>
<li>The experiments were run using custom software written in…</li>
<li>Data management and analysis were performed using SPSS 16.0 (2010).</li>
</ul>
</li>
<li>sequence words<ul>
<li>Prior to analysing the interview data, the transcripts were checked for …</li>
<li>Once the samples were extracted, it was first necessary to …</li>
<li>When dividing X, care was taken to …</li>
</ul>
</li>
<li>adverbs of manner<ul>
<li>The tubes were accurately reweighed to six decimal places using …</li>
</ul>
</li>
<li>‘using’ + instruments<ul>
<li>Data were collected using two high spectral resolution Xs.</li>
<li>Statistical significance was analysed using analysis of variance and t-tests as appropriate.</li>
</ul>
</li>
<li>statistical procedures<ul>
<li>The mean score for the two trials was subjected to multivariate analysis of variance to …</li>
<li>A p value Descriptive data were generated for all variables.</li>
</ul>
</li>
<li>Indicating methodological problems or limitations<ul>
<li>The small size of the dataset meant that it was not possible to …</li>
<li>In this investigation there are several sources for error. The main error is …</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Reporting-Results"><a href="#Reporting-Results" class="headerlink" title="Reporting Results"></a>Reporting Results</h3><h4 id="Results-what-you-find"><a href="#Results-what-you-find" class="headerlink" title="Results: what you find"></a>Results: what you find</h4><ul>
<li>describe in a systematic and detailed way</li>
<li>more elaborate commentary on the results is normally restricted to the Discussion section</li>
<li>combination may appear: Results and Discussion</li>
</ul>
<h4 id="What-is-it-3"><a href="#What-is-it-3" class="headerlink" title="What is it?"></a>What is it?</h4><ul>
<li>when reporting qualitative results<ul>
<li>highlight and comment on the themes that emerge from the analysis</li>
<li>comments be illustrated with excerpts from the raw data.</li>
</ul>
</li>
<li>in quantitative studies<ul>
<li>consist of tables and figures, comment on the significant data shown in these ( which identifies the table or figure and indicates its content )</li>
<li>often takes the form of the location or summary statement, and a highlighting statement or statements( which point out and describe the relevant or significant data )</li>
</ul>
</li>
<li>all figures and tables should be numbered and given a title</li>
</ul>
<h4 id="How-to-write-2"><a href="#How-to-write-2" class="headerlink" title="How to write?"></a>How to write?</h4><ul>
<li>Referring back to the research aims or procedures<ul>
<li>To compare the difference between …</li>
<li>The average scores of X and Y were compared in order to …</li>
</ul>
</li>
<li>Referring to data in a table or chart *<ul>
<li>Table/Figure 1 shows/compares/presents/provides    an overview of …</li>
<li>The table below illustrates some of the main characteristics of the …</li>
<li>As can be seen from the table (above), …</li>
<li>From the graph above we can see that, …</li>
<li>The results obtained from the preliminary analysis of X    are set out/ presented in Table 1.</li>
</ul>
</li>
<li>Highlighting significant data in a table or chart<ul>
<li>What stands out in the table is …</li>
<li>From this data, we can see that Study 2 resulted in the lowest value of …</li>
<li>In Fig.10 there is a clear trend of decreasing …</li>
<li>As Table III shows, there is a significant difference (t = -2.15, p = 0.03) between the two groups.</li>
</ul>
</li>
<li>Stating a positive result<ul>
<li>On average, Xs were shown to have …</li>
<li>The difference between the X and Y groups was significant.</li>
</ul>
</li>
<li>Stating a negative result<ul>
<li>No increase in X was detected.</li>
<li>No evidence was found for non-linear associations between X and Y.</li>
<li>None of these differences were statistically significant.</li>
</ul>
</li>
<li>Transition: moving to the next result<ul>
<li>The next section of the survey was concerned with …</li>
<li>A comparison of the two results reveals …</li>
<li>Turning now to the experimental evidence on …</li>
</ul>
</li>
<li>Summarising the results section<ul>
<li>In summary, these results show that …</li>
<li>Together these results provide important insights into …</li>
<li>The results in this chapter indicate that … The next chapter, therefore, moves on to discuss the …</li>
</ul>
</li>
</ul>
<h3 id="Discussing-Findings"><a href="#Discussing-Findings" class="headerlink" title="Discussing Findings"></a>Discussing Findings</h3><h4 id="Why-discuss"><a href="#Why-discuss" class="headerlink" title="Why discuss?"></a>Why discuss?</h4><ul>
<li>considering both sides of an issue, or question before reaching a conclusion</li>
<li>considering the results of research and the implications of these</li>
<li>the most complex section</li>
</ul>
<h4 id="What-is-it-4"><a href="#What-is-it-4" class="headerlink" title="What is it?"></a>What is it?</h4><ul>
<li>normally centre around a “statement of result” or an important “finding”</li>
<li>structured into a series of discussion cycles</li>
<li>when offering explanations and suggesting implications the language used is <strong>very tentative or cautious</strong></li>
</ul>
<h4 id="Examples-Elements-involved"><a href="#Examples-Elements-involved" class="headerlink" title="Examples (Elements involved)"></a>Examples (Elements involved)</h4><ul>
<li>Providing background information:<ul>
<li>reference to the literature<ul>
<li>Several reports have shown that …</li>
</ul>
</li>
<li>reference to the question<ul>
<li>The present study was designed to determine the effect of …</li>
<li>This study set out with the aim of assessing the importance of X in …</li>
</ul>
</li>
</ul>
</li>
<li>Restating the result or one of several results<ul>
<li>One interesting finding is …</li>
<li>In this study, Xs were found to cause …</li>
</ul>
</li>
<li>Indicating an unexpected outcome<ul>
<li>This finding was unexpected and suggests that ….</li>
<li>However, the observed difference between X and Y in this study was not significant.</li>
</ul>
</li>
<li>Comparing the result:<ul>
<li>supporting previous findings<ul>
<li>These results further support the idea of …</li>
<li>These findings are in agreement with Smith’s (1999) findings which showed …</li>
</ul>
</li>
<li>contraditing previous findings<ul>
<li>However, this result has not previously been described.</li>
<li>These results differ from X’s 2003 estimate of Y, but they are broadly consistent with earlier …</li>
</ul>
</li>
</ul>
</li>
<li>Offering an explanation for the findings<ul>
<li>A possible explanation for this might be that …</li>
<li>There are, however, other possible explanations.</li>
<li>These factors may explain the relatively good correlation between X and Y.</li>
<li>This discrepancy could be attributed to …</li>
<li>The reason for this is not clear but it may have something to do with …</li>
</ul>
</li>
<li>Advising cautious interpretation of the findings<ul>
<li>However, with a small sample size, caution must be applied, as the findings might not be …</li>
<li>Although exclusion of X did not …, these results should be interpreted with caution.</li>
</ul>
</li>
<li>Suggesting general hypotheses<ul>
<li>It is possible, therefore, that …</li>
<li>These results provide further support for the hypothesis that …</li>
<li>These findings suggest that …</li>
</ul>
</li>
<li>Noting implications of the findings<ul>
<li>It can therefore be assumed that the …</li>
<li>One of the issues that emerges from these findings is …</li>
</ul>
</li>
<li>Commenting on the findings<ul>
<li>However, these results were not very encouraging.</li>
<li>The test was successful as it was able to identify students who …</li>
</ul>
</li>
<li>Giving suggestion for future work<ul>
<li>This is an important issue for future research.</li>
<li>Further work is required to establish the viability of…</li>
<li>Several questions remain unanswered at present.</li>
<li>Despite these promising results, questions remain.</li>
<li>To develop a full picture of X, additional studies will be needed that …</li>
</ul>
</li>
</ul>
<h3 id="Writing-Conclusion"><a href="#Writing-Conclusion" class="headerlink" title="Writing Conclusion"></a>Writing Conclusion</h3><h4 id="Why-conclusion"><a href="#Why-conclusion" class="headerlink" title="Why conclusion?"></a>Why conclusion?</h4><ul>
<li>summarise and bring together the main area covered in the writing: “looking back”</li>
<li>give a final comment or judgement on this</li>
<li>comments include making suggestions for improvement and speculating on future directions.</li>
</ul>
<h4 id="What-is-it-5"><a href="#What-is-it-5" class="headerlink" title="What is it?"></a>What is it?</h4><ul>
<li>shorter section but often complex</li>
<li>include the significance of the findings and recommendations for future work</li>
</ul>
<p>_Remarks: Such conclusion is similar to the last elements in the Discussion section_</p>
<ul>
<li>For research articles conclusions may be convered in the Discussion section</li>
<li>For dissertations and essays, however, they are usually expected</li>
</ul>
<h4 id="How-to-write-3"><a href="#How-to-write-3" class="headerlink" title="How to write?"></a>How to write?</h4><ul>
<li>Restating the aims of the study<ul>
<li>This paper has argued that …</li>
<li>The main goal of the current study was to determine …</li>
<li>This project was undertaken to design … and evaluate …</li>
</ul>
</li>
<li>Summarising main research findings<ul>
<li>This study has shown that …</li>
<li>This study has found that generally …</li>
<li>One of the more significant findings to emerge from this study is that …</li>
</ul>
</li>
<li>Suggesting implications for the field of knowledge<ul>
<li>In general, therefore, it seems that …</li>
<li>The results of this study indicate that …</li>
<li>An implication of this is the possibility that …</li>
<li>The evidence from this study suggests that …</li>
<li>The findings of this research provide insights for …</li>
</ul>
</li>
<li>Explaining the significance of the findings or contribution of the study<ul>
<li>The contribution of this study has been to confirm …</li>
<li>The findings reported here shed new light on …</li>
<li>The present study lays the groundwork for future research into …</li>
<li>This study has gone some way towards enhancing our understanding of …</li>
</ul>
</li>
<li>Recognising the limitations of the current study<ul>
<li>The study is limited by the lack of information on …</li>
<li>The main weakness of this study was the paucity of …</li>
</ul>
</li>
<li>Acknowledging limitation(s) whilst stating a finding or contribution *<ul>
<li>Notwithstanding these limitations, the study suggests that …</li>
<li>In spite of its limitations, the study certainly adds to our understanding of the …</li>
<li>Although the current study is based on a small sample of participants, the findings suggest …</li>
</ul>
</li>
<li>Making recommendations for further research work<ul>
<li>Further    work needs to be done to establish whether …</li>
<li>Further research is required to determine whether …</li>
<li>Large randomised controlled trials could provide more definitive evidence.</li>
</ul>
</li>
<li>Setting out recommendations for practice or policy<ul>
<li>A reasonable approach to tackle this issue could be to …</li>
<li>This study suggests that X should be avoided by people who are prone to …</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Reference</strong>: _mainly from_ <a href="http://www.phrasebank.manchester.ac.uk/" target="_blank" rel="noopener">Academic Phrasebank</a></p>
]]></content>
      <categories>
        <category>Writing</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Partition of a set</title>
    <url>/2020/03/12/2020-03-12-partition-of-a-set/</url>
    <content><![CDATA[<p>An important part in combinatorics is the Partition of a set, the study of which provides some interesting conclusions. Let us come straight to the point.</p>
<h3 id="Partition-of-a-set-a-definition"><a href="#Partition-of-a-set-a-definition" class="headerlink" title="Partition of a set: a definition"></a>Partition of a set: a definition</h3><p>Suppose we have a set $A$, then a partition of $A$ is a grouping, say “pattern”, of its elements into <strong>non-empty subsets</strong> such that every element us included in <strong>one and only one</strong> subset. And $A$ has to be a disjoint union of the subsets. The result of grouping gives us a specific family of sets $P$.</p>
<p>_<strong>[Def]</strong>_</p>
<p>$Partition~P=\{A_1,A_2,\dots, A_k \}$ is a $k$-ary partition, if and only if the following conditions hold:</p>
<ul>
<li>The family $P$ does not contain the empty set: $\emptyset \notin P$.</li>
<li>The union of the sets in $P$ is equal to $X$: $\bigcup_{A\in P}A=X$. The sets in $P$ are said to cover $X$.</li>
<li>The intersection of any two distinct sets in P is empty: $\forall A,B\in P) \; A\neq B \implies A\cap B=\emptyset $. The elements of P are said to be pairwise disjoint.</li>
</ul>
<p>Then the sets in $P$ are called the <strong>blocks, parts or cells</strong> of the partition. These blocks are <strong>not</strong> arranged in sequence. _(A combination problem, right?)_</p>
<h3 id="Now-the-Counting-Stirling-numbers-of-the-second-kind"><a href="#Now-the-Counting-Stirling-numbers-of-the-second-kind" class="headerlink" title="Now the Counting! Stirling numbers of the second kind"></a>Now the Counting! Stirling numbers of the second kind</h3><p>The total number of the possible partitions of a $n$-ary set into $k$ non-empty subsets is denoted by $S(n,k)$, called _<strong>Stirling numbers of the second kind</strong>_.</p>
<h4 id="Properties"><a href="#Properties" class="headerlink" title="Properties:"></a>Properties:</h4><ul>
<li>$S(n,n) = S(1,1) = 1$</li>
<li>$S(n,k) = 0$, if $1 \leq n &lt; k$</li>
<li>$S(n,0) = S(0,k) = 0$, if $1 \leq n$; in particular $S(0,0)=1$</li>
</ul>
<p>On top of that, the domain of $S(n,k)$ will be on $n,k \in N_0$.</p>
<h4 id="Recurrence-relation"><a href="#Recurrence-relation" class="headerlink" title="Recurrence relation"></a>Recurrence relation</h4><p>Stirling numbers of the second kind obey the recurrence relation</p>
<script type="math/tex; mode=display">S(n+1,k) = S(n,k-1)+ kS(n,k)</script><p>for $k &gt; 0$ with initial conditions.</p>
<p>Take the formula carefully! The form of it exhibits one of the ideas in combinatorics: the first term on the R.H.S can be interpreted that $\{a_1\}$ serves as a independent block of the partition $P$ while the second term means that $a_1$ has shared the block with any other $n$ element. The former case just reduces the scale of the problem such that it provides a <strong>recursive method</strong> for counting.</p>
<p>With recurrence relation, we can also prove the following relation:</p>
<script type="math/tex; mode=display">S(n+r,n) = \sum_{1 \leq k_1 \leq \dots \leq k_r} k_1k_2\dots k_r</script><p>_Hint_: Construct the function $F(r,n) = \sum_{1 \leq k_1 \leq \dots \leq k_r} k_1k_2\dots k_r$, and deduce the relation: $F(r,n) = F(r,n-1 + nF(r-1,n)$. If two functions share the recursive condition and their boundry condition are identical, then they are equal.</p>
<h4 id="Analytic-definition-generating-functions"><a href="#Analytic-definition-generating-functions" class="headerlink" title="Analytic definition: generating functions"></a>Analytic definition: generating functions</h4><p>Let $k\in N, (x)_k = x(x-1)\dots(x-k+1), (x)_0=1$ ( $(x)_k$ as falling $k$-order factorial ), then:</p>
<script type="math/tex; mode=display">x^n = \sum_{k=0}^n S(n,k) (x)_k</script><p>and</p>
<script type="math/tex; mode=display">\sum_{k=0}^n S(n,k)x^k = T_n(x)</script><p>where $T_n(x)$ are <a href="https://en.wikipedia.org/wiki/Touchard_polynomials" target="_blank" rel="noopener">Touchard polynomials</a>.</p>
<p>Proof is left out. But some ideas are necessary to be pointed out:</p>
<ul>
<li>The number of surjection from a $n$-ary set to a $k$-ary set is $k!S(n,k)$</li>
<li>The set of the map from a $n$-ary set to a $m$-ary set is equal to the <strong>union</strong> of the map from a $n$-ary set to a $k$-ary set for each $k=1,2,\dots,m$</li>
</ul>
<h4 id="Explicit-formula"><a href="#Explicit-formula" class="headerlink" title="Explicit formula"></a>Explicit formula</h4><p>The Stirling numbers of the second kind are given by the explicit formula:</p>
<script type="math/tex; mode=display">S(n,k) = \frac{1}{k!} \sum_{i=0}^k (-1)^k \binom{k}{i} (k-i)^n</script><h4 id="Asymptotic-approximation"><a href="#Asymptotic-approximation" class="headerlink" title="Asymptotic approximation"></a>Asymptotic approximation</h4><p>For fixed value of $k$, the asymptotic value of the Stirling numbers of the second kind as $n\rightarrow \infty$  is given by:</p>
<p>$S(n,k) \sim \frac{k^n}{k!}$</p>
<hr>
<p><strong>Reference</strong>:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Partition_of_a_set" target="_blank" rel="noopener">Wikipedia: Partition of a set</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stirling_numbers_of_the_second_kind" target="_blank" rel="noopener">Wikipedia: Stirling_numbers of the second kind</a></li>
</ul>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Combinatorics</tag>
      </tags>
  </entry>
  <entry>
    <title>Basics of NLU and Smoothing strategies</title>
    <url>/2020/03/11/2020-03-11-basics-NLU/</url>
    <content><![CDATA[<p>Today we will focus on one of the branches of Artificial Intelligence: Natural language Understanding(NLU) or say processing(NLP). Whatever it is called, the source can trace back to almost a hundred year ago, before machine learning(ML). To avoid redundancy, we leave out the brilliant development history of this subject, which has been first called Computational Linguistics for a long time, but instead, go to the very part of the ideas and methods of processing a “language”.</p>
<h2 id="Basics-of-NLU-and-Smoothing-strategies"><a href="#Basics-of-NLU-and-Smoothing-strategies" class="headerlink" title="Basics of NLU and Smoothing strategies"></a>Basics of NLU and Smoothing strategies</h2><h4 id="What-is-natural-language"><a href="#What-is-natural-language" class="headerlink" title="What is natural language?"></a>What is natural language?</h4><p>It is meant to be told apart from format language, like programming language. NL is, to be more specific, the huamn language, including written and spoken forms, text and speech materials respectively.</p>
<h4 id="What-is-NLU-for"><a href="#What-is-NLU-for" class="headerlink" title="What is NLU for?"></a>What is NLU for?</h4><p>Similar to other tasks or applications in ML, it works on understanding of a text that is formulated in some natural languages, moreover, <strong>mining some knowledge from its carrier.</strong></p>
<p>There are two types of knowledge:</p>
<ul>
<li><em>common sense</em>: named entities in NL relate to the common sense knowledge</li>
<li><em>linguistic knowledge</em>: part of seech,syntax, formal semantics are about the linguistic knowledge</li>
</ul>
<p>The latter is more difficult and has been studyed for a long time without a effective solution.</p>
<h5 id="Investigation-Goals-for-NLU-engineering-goal"><a href="#Investigation-Goals-for-NLU-engineering-goal" class="headerlink" title="Investigation Goals for NLU (engineering goal)"></a>Investigation Goals for NLU (engineering goal)</h5><ul>
<li>the development of pratical, useful language processing or understanding systems</li>
<li>a better understanding of language (also the intelligence</li>
</ul>
<h4 id="Analysis-Levels"><a href="#Analysis-Levels" class="headerlink" title="Analysis Levels"></a>Analysis Levels</h4><p>There is a clear hierarchy lying in the analysis of NL:</p>
<ul>
<li>morphological analysis(lexical analysis)</li>
<li>syntactic analysis ( deep &amp;&amp; shallow <strong>parsing</strong> )</li>
<li>semantic analysis  (meaning of some tokens)</li>
<li>pragmatic analysis   (the usage of one expression)</li>
<li>discourse analysis(text analysis</li>
<li>whole world knowledge analysis(<em>Final topic of intelligence!</em>)</li>
</ul>
<p>In the rest of discussion, we will talk about some stuffs more <strong>technically</strong>. Yep!</p>
<h4 id="Language-model"><a href="#Language-model" class="headerlink" title="Language model"></a>Language model</h4><blockquote>
<p>A language model is a probability distribution [ $0\sim1$, normalized ] over word sequences, like:</p>
</blockquote>
<ul>
<li>P(“Amazing”) = 0.001</li>
<li>P(“Ball”) = 0.01</li>
</ul>
<p>In this post, the topic is around one of the most important ones, a basic model: <strong><em><a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="noopener">n-gram</a></em></strong>.</p>
<h5 id="You-may-ask-me-What’s-n-gram-for"><a href="#You-may-ask-me-What’s-n-gram-for" class="headerlink" title="You may ask me: What’s n-gram for?"></a>You may ask me: What’s n-gram for?</h5><ul>
<li>speech recognition</li>
<li>handwriting recognition</li>
<li>spelling correction</li>
<li>optical character recognition</li>
<li>machine translating</li>
<li>…</li>
</ul>
<p><img src="/images/n-gram.png" alt="alt img1"></p>
<p><em>( <strong>Notice</strong>: It is naive because anyone can do statistical modeling, without the requirement for enough understanding of some specific language)</em></p>
<p><em>Hey! You’d better have a little knowledge of <strong>Probability and Statistics</strong> for our following discussion, but do not worry, it is easy to understand.</em></p>
<h5 id="Some-definitions-of-probability"><a href="#Some-definitions-of-probability" class="headerlink" title="Some definitions of probability"></a>Some definitions of probability</h5><ul>
<li>probability P(X) -&gt; of X true</li>
<li>joint probability P(X,Y)  -&gt; of X and Y both true</li>
<li>conditional probability<script type="math/tex; mode=display">P(X|Y)</script></li>
</ul>
<p>-&gt; of X true when Y already known true ( unsymmetric -&gt; Bayes! )</p>
<ul>
<li>Defined as<script type="math/tex; mode=display">P(X|Y) = P(X,Y) / P(Y)</script></li>
</ul>
<p><em>Remarks</em>: in NLU, the conditional probability $ P(X|Y)$ is described as: <strong>After knowing the existence of token $Y$, what is the probability of appearance of token $X$</strong>?</p>
<p>[Bayes law, <strong>symmetric</strong>]</p>
<script type="math/tex; mode=display">P(X|Y) \times P(Y) = P(Y|X) \times P(X)</script><p>or in general form,</p>
<script type="math/tex; mode=display">P(X|Y)  = \frac{P(Y|X) \times P(X)}{ P(Y)}</script><p>All in all, probability with its variants describe a specific state of some distribution.</p>
<h5 id="Decoding-a-process-of-generating-a-“sentence”"><a href="#Decoding-a-process-of-generating-a-“sentence”" class="headerlink" title="Decoding: a process of generating a “sentence”"></a>Decoding: a process of generating a “sentence”</h5><p>Our goal is :</p>
<script type="math/tex; mode=display">arg\max_{wordseq} P(wordseq| acoustics) \\
=  arg\max_{wordseq} \frac{P( acoustics|wordseq) \times P(wordseq)}{P(acoutics)}  \\
= arg\max_{wordseq} P(wordseq| acoustics) \times P(wordseq)</script><p><em>The last equality holds because of the same situation, as P(acoustics) is constant after all.</em></p>
<p>The very L.H.S. term in the formula above is hard to obtain, by using Bayes Law, we turn to the R.H.S.</p>
<p><strong>Applications</strong> eg.:</p>
<p>acoustics: another language -&gt; this language (wordseq)<br>acoustics: sound -&gt; print wordseq</p>
<h5 id="How’s-it-work"><a href="#How’s-it-work" class="headerlink" title="How’s it work"></a>How’s it work</h5><ul>
<li>step1: decompose the probabilities</li>
</ul>
<script type="math/tex; mode=display">P("this~is ~a ~long ~model") = P("this") \times P("is") \times ...</script><ul>
<li>step2: make <strong>Markov Independence Assumptions</strong> by using <strong>n-gram</strong> approximation (i.e. finite regression</li>
</ul>
<script type="math/tex; mode=display">P("the"| "... whole ~truth ~and ~nothing ~but") \approx P("the"|"nothing ~but")</script><script type="math/tex; mode=display">P("the"| "... whole~truth~and~nothing~but~the") \approx P("the"|"but~the")</script><h5 id="Instance-Trigram-n-3"><a href="#Instance-Trigram-n-3" class="headerlink" title="Instance: Trigram $(n=3)$"></a>Instance: Trigram $(n=3)$</h5><blockquote>
<p>Assume each word depends only on the previous two words, say 3 words totally for concern<br>Generally speaking, the gram is <strong>unit</strong> defined by human.</p>
</blockquote>
<h5 id="So-to-do-the-counting"><a href="#So-to-do-the-counting" class="headerlink" title="So to do the counting!"></a>So to do the counting!</h5><ul>
<li>how do we find probability?<ul>
<li>counting !</li>
</ul>
</li>
<li>for the Trigram example,<script type="math/tex; mode=display">P("the"|"nothing~but") \approx C("nothing~but ~the") / C("nothing~but")</script></li>
</ul>
<p>There is a big question remaining: Denominator can be non-zero, sometimes which is caused by small corpus.</p>
<h3 id="Evaluation-on-the-chosen-application"><a href="#Evaluation-on-the-chosen-application" class="headerlink" title="Evaluation on the chosen application"></a>Evaluation on the chosen application</h3><p>Like the cost function in ML, we have to decide the baseline for good or bad prediction (say learning result).</p>
<ul>
<li>How to tell a good language model from bad one?</li>
<li>like, run a speech recognizer, calculate word error rate, but:<ul>
<li>slow</li>
<li>specific to your recognizer</li>
</ul>
</li>
</ul>
<h5 id="Evaluation-index-Perplexity-PPL"><a href="#Evaluation-index-Perplexity-PPL" class="headerlink" title="Evaluation index: Perplexity (PPL)"></a>Evaluation index: Perplexity (PPL)</h5><p>When we say perplexity, we mean “model on test”</p>
<ul>
<li>ask a speech recognizer to recognize digits: “0 1 2 3 4 5 6 7 8 9” -&gt; easy! perplexity=10</li>
<li>…. recognize names at MicroSoft -&gt; hard! ~30,000 perplexity=30,000</li>
<li>…. recognize “A … Z” perplexity=26</li>
<li>…. recognize “alpha …. zulu “ perplexity=26</li>
</ul>
<p><em>Remarks</em>:</p>
<ul>
<li>Perplexity is weighted <strong>equivalent branching</strong>.</li>
<li>Perplexity measures language model difficulty, not that of acoutics.</li>
</ul>
<p>In math, perplexity is <strong>geometric average</strong> inverse probability:</p>
<script type="math/tex; mode=display">PPL =  \sqrt[n] {\prod_{i=1}^n \frac{1}{ P (w_i | w_{ 1\dots i-1} ) }}</script><p><em>Remarks</em>: <strong>The true model for data has lowest possible perplexity</strong>( Occam’s Razor principle). This is to say, lower the perplexity, the closer we are to true model.</p>
<p>Typically, perplexity correlates well with application-specific measure such as speech recognition word error rate.</p>
<ul>
<li>Correlates better when both models are trained on same data</li>
<li>Doesn’t correlate well when training data changes</li>
</ul>
<h5 id="Evaluation-index-Cross-Entropy"><a href="#Evaluation-index-Cross-Entropy" class="headerlink" title="Evaluation index: Cross Entropy"></a>Evaluation index: Cross Entropy</h5><p><em>Definition</em>:</p>
<script type="math/tex; mode=display">Entropy = log_2 PPL = log_2 \sqrt[n] {\prod_{i=1}^n \frac{1}{ P (w_i | w_{ 1\dots i-1} ) }}</script><p><em>Remarks</em>: entropy is average number of bits per word required to encode test data using this probability model, and an optimal coder. Called bits.</p>
<h3 id="Smoothing-the-standard-techniques"><a href="#Smoothing-the-standard-techniques" class="headerlink" title="Smoothing: the standard techniques"></a>Smoothing: the standard techniques</h3><p>Smoothing methods are proposed to solve the problem of overfitting, which is caused by the fact that <strong>noises are treated as sensitive effect near decision boundary</strong>.</p>
<p>Like $n$-order polynomial ==&gt; $m$-order polynomial ($m&lt;n$).</p>
<h4 id="Smoothing-None"><a href="#Smoothing-None" class="headerlink" title="Smoothing: None"></a>Smoothing: None</h4><script type="math/tex; mode=display">P(z|xy) \approx \frac{C(xyz)}{\sum_w C(xyw)} = \frac{C(xyz)}{C(xy)}</script><ul>
<li>called Maximum Likelihood estimate</li>
<li>lowest perplexity trigram on training data</li>
<li><strong>terrible</strong> on test data: if no occurrences of C(xyz), zero probability</li>
</ul>
<h4 id="Smoothing-Add-One"><a href="#Smoothing-Add-One" class="headerlink" title="Smoothing: Add-One"></a>Smoothing: Add-One</h4><p>In order to solve the zero probability on the numerator:</p>
<script type="math/tex; mode=display">P(z|xy) \approx \frac{C(xyz)+1}{C(xy)+V}</script><p>or add-$\delta$:</p>
<script type="math/tex; mode=display">P(z|xy) \approx \frac{C(xyz)+\delta}{C(xy)+V\delta}</script><p>However, they:</p>
<ul>
<li>Works very badly</li>
<li>DO NOT DO THIS</li>
</ul>
<h4 id="Smoothing-Single-Interpolation"><a href="#Smoothing-Single-Interpolation" class="headerlink" title="Smoothing: Single Interpolation"></a>Smoothing: Single Interpolation</h4><script type="math/tex; mode=display">P(z|xy) \approx \lambda \frac{C(xyz)}{C(xy)} + \mu\frac{C(yz)}{C(y)} + (1-\lambda -\mu)\frac{C(z)}{C(*)}</script><ul>
<li>trigram is very context specific, very noisy</li>
<li>Unigram is context-independent, smooth</li>
<li>Interpolate Trigram, Bigram, Unigram for best combination</li>
<li>Find $0&lt; \lambda ,\mu &lt; 1$ by optimizing on “held-out” data</li>
<li>Almost <strong>good</strong> enough</li>
</ul>
<h5 id="How-to-find-parameter-values-lambda-mu"><a href="#How-to-find-parameter-values-lambda-mu" class="headerlink" title="How to find parameter values $ \lambda ~\mu$?"></a>How to find parameter values $ \lambda ~\mu$?</h5><p>Steps:</p>
<ul>
<li>Split data into training, “held out” (or development, dev), test sets</li>
<li>Try lots of different values for $ \lambda ~\mu$ on dev data, pick best</li>
<li>Test on test data</li>
<li>Use tricks like “EM” (estimation maximization) to find values</li>
</ul>
<p><strong>EM algorithm:</strong></p>
<ul>
<li>Initialization: Pick arbitrary/random values for $\lambda_1, \lambda_2, \lambda_3$</li>
<li>Step 1: Calculate the following quantities:</li>
</ul>
<p><img src="/images/EM-c3.png" alt="alt img2"></p>
<ul>
<li>Step 2: Re-estimate  $\lambda_i = \frac{c_i}{\sum_{j=1}^3 c_j}$</li>
<li>Step 3: If $\lambda_i$  have not converged, go to Step 1.</li>
</ul>
<h5 id="The-law-of-splitting-data"><a href="#The-law-of-splitting-data" class="headerlink" title="The law of splitting data"></a>The law of splitting data</h5><p>How much data for training, heldout, test?</p>
<ul>
<li>Some people say things like “1/3, 1/3, 1/3” or “80%, 10%, 10%”<ul>
<li>They may be WRONG</li>
</ul>
</li>
</ul>
<p>Answer:</p>
<ul>
<li>Heldout should have (at least) 100-1000 words per parameter.</li>
<li>Enough test data to be statistically significant. (1000s of words perhaps)</li>
</ul>
<p>_<strong>heldout and test</strong> go FIRST !_</p>
<h4 id="Smoothing-Jelinek-Mercer"><a href="#Smoothing-Jelinek-Mercer" class="headerlink" title="Smoothing: Jelinek-Mercer"></a>Smoothing: Jelinek-Mercer</h4><ul>
<li>Single Interpolate:</li>
</ul>
<script type="math/tex; mode=display">P_{smooth}(z|xy) = \lambda \frac{C(xyz)}{C(xy)} +  (1-\lambda)P_{smooth}(z|y)</script><ul>
<li>Better: smooth a little after “The Dow”(“Jones” probably follow), lots after “Adobe required”.</li>
</ul>
<script type="math/tex; mode=display">P_{smooth}(z|xy) = \lambda(C(xy)) \frac{C(xyz)}{C(xy)} +  (1-\lambda(C(xy)))P_{smooth}(z|y)</script><ul>
<li>Find $\lambda$ by cross-validation on held-out data</li>
<li>called “deleted-interpolation”</li>
</ul>
<h4 id="Smoothing-Good-Turing"><a href="#Smoothing-Good-Turing" class="headerlink" title="Smoothing: Good-Turing"></a>Smoothing: Good-Turing</h4><p>Steps:</p>
<ul>
<li>Define $n_r$ = number of elements $x$ types for which $Count(x) = r$.</li>
<li>modified count for any x with Count(x) = r and r &gt; 0;<br>  $r \to (r+1)n_{r+1}/n_{r}$</li>
<li>leads to the following estimae of “missing mass”(count=0):<br>  $0 \to n_1/N,$</li>
</ul>
<p>where N is the size of the sample. This is the estimate of the prob of seeing a new element on the (N+1)-th draw !</p>
<p>For _example_, imagine you are fishing:</p>
<p>You have caught:<br>_10 Carp, 3 Cod, 2 tuna, 1 trout, 1 salmon, 1 eel._</p>
<ul>
<li>How likely is it that next species is new?<ul>
<li>3/18</li>
</ul>
</li>
<li><p>How likely is it that next is tuna?</p>
<ul>
<li>Less than 2/18</li>
</ul>
</li>
<li><p>What you caught (data)</p>
<ul>
<li>10 Carp, 3 Cod, 2 tuna, 1 trout, 1 salmon, 1 eel.</li>
</ul>
</li>
<li>How likely is new data ($p_0$ )<ul>
<li>Let n1 be number occurring once (3),</li>
<li>$N$ be total $(18)$.</li>
<li>$p_0=3/18$</li>
</ul>
</li>
<li>How likely is eel? l*<ul>
<li>$n_1 =3, n_2 =1$</li>
<li>$l* =2 \times 1/3 = 2/3$</li>
<li>$P(eel) = l* /N = (2/3)/18 = 1/27$</li>
</ul>
</li>
</ul>
<p>(Look back on the update formula above!)</p>
<h4 id="Smoothing-Katz"><a href="#Smoothing-Katz" class="headerlink" title="Smoothing: Katz"></a>Smoothing: Katz</h4><p>Ideas:</p>
<blockquote>
<p>Use Good-Turing estimation for non-zero distribution, and the counts subtracted from the non-zero counts are then distributed among the zero-count n-grams according to the next lower-order distribution, i.e., the (n-1)-gram model.</p>
</blockquote>
<script type="math/tex; mode=display">P_{Katz}(z|xy)=
\begin{cases}
C*(xyz)/C(xy)& \text{ if C(xyz) > 0}\\
\alpha(xy) P_{Katz}(z|y)& \text{otherwise}
\end{cases}</script><p>$\alpha$ is calculated so probabilities sum to 1, $\alpha(xy) = 1-\sum_{C(xy) &gt; 0} (C*(xyz)/C(xy))$</p>
<ul>
<li>Works pretty well.</li>
<li>Not good for 1 counts</li>
</ul>
<h4 id="Smoothing-Absolute-Discounting"><a href="#Smoothing-Absolute-Discounting" class="headerlink" title="Smoothing: Absolute Discounting"></a>Smoothing: Absolute Discounting</h4><ul>
<li>Assume fixed discount</li>
</ul>
<script type="math/tex; mode=display">P_{abs}(z|xy)=
\begin{cases}
\frac{C(xyz)-D}{C(xy)}& \text{ if C(xyz) > D}\\
\alpha(xy) P_{abs}(z|y)& \text{otherwise}
\end{cases}</script><ul>
<li>Works pretty well and easier than Katz!</li>
<li>Not good for 1 counts</li>
</ul>
<h4 id="Smoothing-Interpolated-Absolute-Discount"><a href="#Smoothing-Interpolated-Absolute-Discount" class="headerlink" title="Smoothing:Interpolated Absolute Discount"></a>Smoothing:Interpolated Absolute Discount</h4><p>Assembly idea:</p>
<ul>
<li>Backoff: ignore bigram if have Trigram</li>
</ul>
<script type="math/tex; mode=display">P_{abs}(z|xy)=
\begin{cases}
\frac{C(xyz)-D}{C(xy)}& \text{ if C(xyz) > D}\\
\alpha(xy) P_{abs}(z|y)& \text{otherwise}
\end{cases}</script><ul>
<li>Interpolated: always combine bigram, trigram</li>
</ul>
<script type="math/tex; mode=display">P_{abs-interp}(z|xy)= \frac{C(xyz)-D}{C(xy)} + \beta(xy) P_{abs-interp}(z|x)</script><h4 id="Smoothing-Interpolated-Multiple-Absolute-Discounts"><a href="#Smoothing-Interpolated-Multiple-Absolute-Discounts" class="headerlink" title="Smoothing: Interpolated Multiple Absolute Discounts"></a>Smoothing: Interpolated Multiple Absolute Discounts</h4><ul>
<li>One discount is good</li>
</ul>
<script type="math/tex; mode=display">P_{abs-interp}(z|xy)= \frac{C(xyz)-D}{C(xy)} + \beta(xy) P_{abs-interp}(z|x)</script><ul>
<li>Different discounts for different counts</li>
</ul>
<script type="math/tex; mode=display">P_{abs-interp}(z|xy)= \frac{C(xyz)-D_{C(xyz)}}{C(xy)} + \beta(xy) P_{abs-interp}(z|x)</script><ul>
<li>Multiple discounts: for 1) $1$ count; 2) $2$ counts; and 3) counts &gt; $2$</li>
</ul>
<h4 id="Smoothing-Smoothing-Kneser-Ney"><a href="#Smoothing-Smoothing-Kneser-Ney" class="headerlink" title="Smoothing: Smoothing: Kneser-Ney"></a>Smoothing: Smoothing: Kneser-Ney</h4><p><em>It is one of the most useful one! Assembly method of the previous ones</em></p>
<p>$P(Francisco | eggplant) ~v.s. ~P(stew | eggplant)$</p>
<ul>
<li>“Francisco” is common, so backoff, interpolated methods say it is likely, but it only occurs in context of “San”</li>
<li>“Stew” is common, and in many contexts</li>
</ul>
<p>Solution:</p>
<ul>
<li>Weight backoff by number of contexts word occurs in</li>
</ul>
<script type="math/tex; mode=display">P_{KN}(z|xy)= \frac{C(xyz)-D_{C(xyz)}}{C(xy)} + \beta(xy) \frac{|\{w|C(wyz)>0\}|}{\sum_v |\{w|C(wyv)>0\}|}</script><p>Features:</p>
<ul>
<li>Interpolated</li>
<li>Absolute-discount</li>
<li>Modified backoff distribution</li>
<li>Consistently best technique</li>
<li>Interpolated Kneser-Ney works much better than Katz on 5-gram, more than on 3-gram</li>
</ul>
<p><img src="/images/smoothing1.png" alt="alt img3"></p>
<hr>
<p><strong>Reference</strong>: Tutorial materials of Joshua Goodman (Microsoft Research) and Michael Collins (MIT) and Zhao Hai(Shanghai Jiao Tong Univ.); <a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="noopener">Wikipedia: n-gram</a>.</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>NLU Language-model</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes for Linear Regression</title>
    <url>/2020/03/10/2020-03-10-linear-reg/</url>
    <content><![CDATA[<p>Linear regression is one of the most important method among all the machine learning stuffs. As a <strong>supervised learning</strong> method, it also shares a large part with traditional predition mannar, say the mathematical ones. The ideas behind it, however, offers great insights into the machine learning methods with its simple and direct form.</p>
<h3 id="What-is-Machine-Learning-doing"><a href="#What-is-Machine-Learning-doing" class="headerlink" title="What is Machine Learning doing?"></a>What is Machine Learning doing?</h3><p>Suppose we have a series of $n-dim$ known data, say $m$ items. Now we need make full use of them to find some latent relations behind, or more specifically, make some prediction work.</p>
<p>Generally speaking, if we let $i=1,2,\dots,n$(below the same), <strong>n-features</strong> $x^{(i)} \in X$ denote the “input” variables, as the ‘source’ data, which means this is where we start. <strong>Targets</strong> $y^{(i)}\in Y$ denote the “output” variables, which can be continuous or discrete.</p>
<p>Here we need to point out, targets are continuous numerical data when it comes to regression, like $4.2, 10.9$; classification are doing “tagging”, for which the output data(of very small number of values) can belong to a specific <strong>name</strong> or another one, like $True, False$, and $positive, negative$.</p>
<p><img src="/images/linear-reg1.png" alt="alt img1"></p>
<p>_Then what do our machine actually learn?_</p>
<p>Let so-called <strong>hypothesis</strong> $h$ be the map, i.e. function, from $X \to Y$. Our purpose is to find (or compute) a good map rule involving the argument of such function, such that the machine can help to predict or classify the unknown data, or at least, provide some useful trail.</p>
<p><img src="/images/linear-reg2.png" alt="alt img2"></p>
<p>That is a brief introduction to the world of machine learning. To avoid redundancy, we skip some derivations of formula, and focus on the core part of linear regression.</p>
<h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>The following figures just show a process of formularizing:</p>
<p><strong>Real data…</strong></p>
<p><img src="/images/linear-reg3.png" alt="alt img3"></p>
<p><strong>Linear relation…</strong></p>
<script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2</script><p><strong>More generally…</strong></p>
<script type="math/tex; mode=display">h(x) = \sum_{i=0}^{n} \theta_i x_i = \theta^T x</script><p><strong>“Evaluation”: cost function</strong></p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2</script><p>The last one is critical because cost function <strong>serves as a Measure</strong> of discrepancy bewteen the real data and machine-supposed one. Also, we can call it a tutor of machine. Here we define the cost function in its <strong>least square(LS)</strong> form.</p>
<p>Think twice, it is exactly the same as the <strong>LS</strong> method in regression, a mathematical one. We give the <strong>normal equation</strong>, which is also mentioned in machine learning, as the conclusive formula in <strong>LS</strong> method.(_Without proof! See it in the reference_)</p>
<p>The <strong>LS</strong> estimation of arguments $\vec{\theta}$ are given as:</p>
<script type="math/tex; mode=display">\vec{\theta} = (X^TX)^{-1} X ^T \vec{y}</script><p>We generally require the positive definiteness of $X$, or at least the invertibility. However, both of them sometimes are hard to achieve or assure.</p>
<p>Here is a solution: <strong>Regularize term addtion</strong>.</p>
<p>By adding a _self inner product_ of $\theta$, the cost function has its form as follow:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \vec{\theta}^T \vec{\theta}$$.

In fact the inner product can be replaced by a specific norm (like $q$-norm )and the second term now changes into:

$$\frac{\lambda}{2} ||\theta_j||_q = \frac{\lambda}{2} \sum_1^n |\theta_j|^q</script><p>Such that,</p>
<script type="math/tex; mode=display">\vec{\theta} = (\lambda I + X^TX)^{-1} X ^T \vec{y}</script><h4 id="Is-that-the-only-way"><a href="#Is-that-the-only-way" class="headerlink" title="Is that the only way?"></a>Is that the only way?</h4><p>Okay, now putting all of them aside, there are alternative tools will also help, let us see.</p>
<p>As we know, the counterpart of <strong>LS</strong> estimation is the <strong>maximum likelihood(ML)</strong> estimation. It works by constructing a _likelihood function_ from the perspective of probability:</p>
<script type="math/tex; mode=display">L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;\theta).</script><p>To choose proper $\theta$, the $L(\theta)$ can finally achieve its maxima, and that is what we want.</p>
<p>Now we might as well assume the discrepancy, say the error $\epsilon^{(i)}$, between the prediction and real one are $IID$(_independently and identically distributed_) and normally distributed : $\epsilon^{(i)} \sim N(0,\sigma^2)$. ( This make sense for most of the cases according to probabilistic view).</p>
<p>Thus, we have</p>
<script type="math/tex; mode=display">p(\epsilon^{(i)}) = p(\vec{y}|X;\theta) = \frac{1}{\sqrt{2\pi} \sigma} exp(-\frac{(y^{(i)} -\theta^T x^{(i)})^2 }{2 \sigma^2})</script><p>On top of independence assumption,</p>
<script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{m} p(\vec{y}|X;\theta) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi} \sigma} exp(-\frac{(y^{(i)} -\theta^T x^{(i)})^2 }{2 \sigma^2})</script><p><strong>$m$ is the number# of samples</strong></p>
<p>To take $log$, with the purpose of decoupling terms, we have <strong>log likelihood $l(\theta)$</strong>:</p>
<script type="math/tex; mode=display">l(\theta) = \dots = m log \frac{1}{\sqrt{2\pi} \sigma} - \frac{1}{\sigma^2} ~\frac{1}{2} \sum_{i=1}^{m} (y^{(i)} -\theta^T x^{(i)})^2</script><p>So maximizing the $l(\theta)$ gives the same answer as minimizing $\frac{1}{2} \sum_{i=1}^{m} (y^{(i)} -\theta^T x^{(i)})^2$, which is the _cost function_ of <strong>LS</strong>.</p>
<h4 id="SGD-algorithm-a-powerful-tool-for-update"><a href="#SGD-algorithm-a-powerful-tool-for-update" class="headerlink" title="SGD algorithm: a powerful tool for update"></a>SGD algorithm: a powerful tool for update</h4><p>What if <strong>normal equation</strong> fails? Let’s consider another tools for choosing the more proper $\theta$, <strong>gradient descent</strong> with its family: <strong>stochastic gradient descent</strong> and <strong>mini-batch stochastic gradient descent</strong>.</p>
<p>In order to update the argument, the key is to find the correct <strong>direction</strong> of the present, so we can <strong>iteratively</strong> perform the update.</p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j } J(\theta) = \theta_j - \alpha (h_{\theta} (x^{(i)}) - y^{(i)} ) x_j^{(i)} )</script><p>where $\alpha$ is the learning rate. We can repeatly do the update step by step, along with the steepest decrease of $J$, <strong>gradient</strong> direction. After choosing proper cost function, all we need is the derivative.</p>
<p>We now turn to distinguish about the manner of using the samples, and here three main algorithms come:</p>
<h5 id="1-batch-gradient-descent"><a href="#1-batch-gradient-descent" class="headerlink" title="1. batch gradient descent"></a>1. batch gradient descent</h5><p>Repeat until convergence {</p>
<script type="math/tex; mode=display">\theta_j :=  \theta_j - \alpha \sum^m_{i=1} (h_{\theta} (x^{(i)}) - y^{(i)} ) x_j^{(i)} )</script><p>}<br>which uses up all the samples to update one single argument at a time.</p>
<h5 id="2-stochastic-gradient-descent"><a href="#2-stochastic-gradient-descent" class="headerlink" title="2. stochastic gradient descent"></a>2. stochastic gradient descent</h5><p>Loop {<br>  for $i=1 \to m$ {</p>
<script type="math/tex; mode=display">\theta_j :=  \theta_j - \alpha \sum^m_{i=1} (h_{\theta} (x^{(i)}) - y^{(i)} ) x_j^{(i)} )$$ (for each $j$ )
  }

}
which only use one sample to update the whole argument.

_Remarks: **operation economical but sometimes not stable**_

##### 3. mini-batch

It is the compromise of the two algorithms above: choose a small number of samples to update all the arguments, iteratively.


#### Locally weighted linear regression(LWR)

![alt img4](/images/linear-reg4.png)

This is interesting that in order to solve the **overfitting** problem, **LWR** is applied. It works by **giving each LS a weight argument** and then summing up. The idea behind it is clear. So we leave out this part.

However, what we have to mention is that **LWR** is one of the examples of **non-parametric** algorithm (the number of parameters is indefinite).

Correspondingly, **parametric algorithm** describes a process with fixed, finite number of parameters(say the $\theta_i$). The very feature of parametric algorithm is that once the model has been trained, the training data is no longer needed; reverse for the non-parametric algorithm(like [kNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)).


### Logistic regression

Here is the last section of this introduction. So-called Logistic regression is **not used for regression but, the classification.** The easiest problem is the **binary classification**,in which y can take on only two values, $0$ and $1$. Now we can call $y^{(i)}$ the **label** of corresponding $x^{(i)}$.

#### Sigmoid function

$$g(z) = \frac{1}{1+e^{-z}}</script><p><img src="/images/linear-reg5.png" alt="alt img5"></p>
<p><strong>Properties</strong></p>
<ul>
<li>Monotonic increasing</li>
<li>Defined on whole real domain</li>
<li>Range (0,1) ~ probability meaning</li>
<li>Derivative:<script type="math/tex; mode=display">g'(z) = g(z)(1-g(z))</script></li>
</ul>
<p>With sigmoid function as our tool, we can now transform a regression problem into a classification problem by setting the <strong>threshold</strong>.</p>
<p>Another thing, sigmoid function is the  perceptron in continuous form.</p>
<p>Now we can change the former $h_\theta (x) = \theta^T x = g(\theta^T x)$.</p>
<hr>
<p><strong>Reference</strong>: <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="noopener">Stanford CS229-note1</a> by Andrew Ng</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Single linkage</title>
    <url>/2020/03/09/2020-03-09-singlelink/</url>
    <content><![CDATA[<h2 id="Single-linkage-clustering"><a href="#Single-linkage-clustering" class="headerlink" title="Single-linkage clustering"></a>Single-linkage clustering</h2><p><strong>Features:</strong></p>
<ul>
<li>hierachical clustering</li>
<li>bottom-up fashion(agglomerative clustering)</li>
<li>combine two clusters(the closest pair) at each step</li>
<li>single element pair -&gt; fusion of two clusters</li>
</ul>
<p><strong>Drawbacks:</strong></p>
<ul>
<li>tend to produce long thin clusters, i.e., nearby elements of the same cluster have small distances, while elements at opposite ends of a cluster may be much farther</li>
<li>on top of that, this may lead to difficulties in defining classes that could usefully subdivide the data.</li>
</ul>
<h4 id="Algorithm-nearest-neighbor-clustering"><a href="#Algorithm-nearest-neighbor-clustering" class="headerlink" title="Algorithm: nearest neighbor clustering"></a>Algorithm: nearest neighbor clustering</h4><p><em>[def] <strong>linkage function</strong></em>: the distance $D(X,Y)$ between clusters X and Y as:</p>
<script type="math/tex; mode=display">D(X,Y) = \min_{x\in X, y\in Y} ~d(x,y)</script><p>where X and Y are any two sets of elements considered as clusters, and d(x,y) denotes the distance between the two elements x and y.</p>
<p>Naive single linkage algorithm:<br>-&gt; <em>erase rows and cols in a proximity matrix as merging process</em></p>
<ol>
<li>Begin with the disjoint clustering, level $L(0)=0$ and sequence number $m=0$.</li>
<li>Find the most similar pair of clusters in the current clustering, say pair $(r), (s)$, according to $d[(r),(s)] = \min d[(i),(j)]$ <em>(minimum <strong>over all pairs</strong> of clusters in the current clustering)</em>.</li>
<li>Increment the sequence number: $m = m+1$. Merge clusters $(r)$ and $(s)$ into a single cluster to form the next clustering $m$. Set the level of this clustering to $L(m) = d[(r),(s)]$.</li>
<li>Update the proximity matrix, $D$, by <strong>deleting</strong> the rows and cols corresponding to cluster, and <strong>adding</strong> a row and col corresponding to the new cluster. The proximity denoted $(r,s)$ and old cluster $(k)$ is defined as $d[(r),(s)] = \min\{d[(k),(r)],d[(k),(s)]\}.$</li>
<li>If all objects are in one cluster, stop. Else, go to step 2.</li>
</ol>
<h3 id="Working-example"><a href="#Working-example" class="headerlink" title="Working example"></a>Working example</h3><h4 id="First-step"><a href="#First-step" class="headerlink" title="First step"></a>First step</h4><p><strong>Clustering-1</strong></p>
<p>Assume we have 5 elements $(a,b,c,d,e)$ and the following matrix $D_1$ of pairwise distances between them:</p>
<p><img src="/images/singlelink1.png" alt="alt img1"></p>
<p>Here, $D_1(a,b)=17$ is the lowest value of $D_1$, so we cluster $a$ and $b$.</p>
<p><strong>Branch length estimation-1</strong></p>
<p>Let $u$ denote the node to which $a$ and $b$ are now connected. Setting $\delta(a,u) = \delta(b,u) = D_1(a,b)/2 = 8.5$ ensures that elements $a$ and $b$ are equidistant from $u$.</p>
<p>This corresponds to the <strong>expectation</strong> of the ultrametricity hypothesis.</p>
<p><strong>Distance matrix update-1</strong></p>
<p>Now initial proximity matrix $D_1$ will be updated into a new one, $D_2$, reduced in size by _one row and one column_.</p>
<p>$D_2((a,b),c) = \min(D_1(a,c),D_1(b,c)) = \min(21,30) = 21$;<br>$D_2((a,b),d) = \min(D_1(a,d),D_1(b,d)) = \min(31,34) = 31$;<br>$D_2((a,b),e) = \min(D_1(a,e),D_1(b,e)) = \min(23,21) = 21$;</p>
<h4 id="Second-step"><a href="#Second-step" class="headerlink" title="Second step"></a>Second step</h4><p><strong>Clustering-2</strong></p>
<p><img src="/images/singlelink2.png" alt="alt img2"></p>
<p>In this table, $D_2((a,b),c) = 21$ and $D_2((a,b),e) = 21$ are the lowest values of $D_2$, thus clustering $(a,b)$ with element $c$ and $e$(<strong>both!</strong>)</p>
<p><strong>Branch length estimation-2</strong></p>
<p>Similarly, let $v$ denote the node to which $(a,b), c$ and $e$ are now connected. Because of the ultrametricity constraint, the branches joining $a$ or $b$ to $v$, and $c$ to $v$, and also $e$ to $v$ are equal and have the following total length:</p>
<script type="math/tex; mode=display">\delta(a,v) = \delta(b,v) =\delta(c,v) =\delta(e,v) = 21/2 = 10.5</script><p>Along with the missing branch length:</p>
<script type="math/tex; mode=display">\delta(u,v) = \delta(c,v) - \delta(a,u) =\delta(c,v) - \delta(b,u) =  10.5 - 8.5 = 2</script><p><strong>Distance matrix update</strong><br>We then proceed to update the $D_{2}$ matrix into a new distance matrix $D_{3}$, reduced in size by two rows and two columns($c$ and $e$).</p>
<h3 id="Final-step"><a href="#Final-step" class="headerlink" title="Final step"></a>Final step</h3><p><img src="/images/singlelink3.png" alt="alt img3"></p>
<p>So we join clusters $((a,b),c,e)$ and $d$.</p>
<p>Let $r$ denote the (root) note to which $((a,b),c,e)$ and $d$ are now connected:</p>
<script type="math/tex; mode=display">\delta( ((a,b),c,e),r ) = \delta(d,r) = 28/2 = 14</script><p>We deduce the remaining branch length:</p>
<script type="math/tex; mode=display">\delta (v,r) = \delta (a,r)- \delta (a,v) = \delta (b,r)-\delta (b,v)=\delta (c,r)-\delta (c,v)=\delta (e,r)-\delta (e,v)=14-10.5=3.5</script><h3 id="The-single-linkage-dendrogram"><a href="#The-single-linkage-dendrogram" class="headerlink" title="The single-linkage dendrogram"></a>The single-linkage dendrogram</h3><p>The result of the clustering can be visualized as a dendrogram, which shows the sequence of cluster fusion and the distance at which each fusion took place.</p>
<p><img src="/images/singlelink4.png" alt="alt img4"></p>
<h3 id="Other-linkage-dendrograms"><a href="#Other-linkage-dendrograms" class="headerlink" title="Other linkage dendrograms"></a>Other linkage dendrograms</h3><h4 id="Complete-linkage-clustering"><a href="#Complete-linkage-clustering" class="headerlink" title="Complete-linkage clustering:"></a>Complete-linkage clustering:</h4><p><img src="/images/singlelink-cl.png" alt="alt img5"></p>
<h4 id="Average-linkage-clustering-WPGMA"><a href="#Average-linkage-clustering-WPGMA" class="headerlink" title="Average linkage clustering-WPGMA:"></a>Average linkage clustering-WPGMA:</h4><p><img src="/images/singlelink-wpgma.png" alt="alt img6"></p>
<h4 id="Average-linkage-clustering-UPGMA"><a href="#Average-linkage-clustering-UPGMA" class="headerlink" title="Average linkage clustering-UPGMA:"></a>Average linkage clustering-UPGMA:</h4><p><img src="/images/singlelink-upgma.png" alt="alt img7"></p>
<hr>
<p><strong>Reference</strong>: <a href="https://en.wikipedia.org/wiki/Single-linkage_clustering" target="_blank" rel="noopener">Wikipedia: Single linkage</a></p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Clustering</tag>
      </tags>
  </entry>
  <entry>
    <title>Academic Writing 2</title>
    <url>/2020/03/08/2020-03-08-academic-writing-2/</url>
    <content><![CDATA[<h3 id="Classifying"><a href="#Classifying" class="headerlink" title="Classifying"></a>Classifying</h3><p>When we talk about classifying things, we group and name them on the basis of something that they have <strong>in common</strong>.</p>
<p>By doing this we can understand certain qualities and features which they share as a class. It helps stress such important properties.</p>
<p>Also, classifying is a effective way of understanding difference.</p>
<p>In writing, classifying is often used as a way of introducing a reader to a new topic. Along with topic introducation, it can appear in the early part of an academic essay.</p>
<ul>
<li>classifying<ul>
<li>$X$ can be classified/categoriesd into $X_i$ and $X_ii$ on the basis of $Y$.</li>
<li>Several taxonomies for $X$ have been developed that …</li>
<li>According to $Y$, $X$ may be divided into several groups: a)…, b) … or c) …</li>
</ul>
</li>
</ul>
<h3 id="Compare-and-Contrast"><a href="#Compare-and-Contrast" class="headerlink" title="Compare and Contrast"></a>Compare and Contrast</h3><p>By understanding similarities and differences between two things, we can increase our understanding and learn more about both. Comparison may also be a preliminary stage of evaluation.</p>
<ul>
<li>General comparison<ul>
<li>conjuction forms<ul>
<li>Oral societies tend to be more concerned with the present whereas/while literate societies have a very definite awareness of the past.</li>
<li>Whereas/While oral societies tend to be more concerned with the present, literate societies have a very definite awareness of the past.</li>
<li>In contrast to oral communities, it is very difficult to get away from calendar time in literate societies.</li>
</ul>
</li>
<li>comparative forms<ul>
<li>Women tend to have greater/less verbal fluency than men.</li>
<li>Women are more/less likely than men to perform well in tests.</li>
<li>The part of the brain connecting the two hemispheres may be more/less extensive in women</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Differences<ul>
<li>introductory:<ul>
<li>X is different from Y in a number of respects.</li>
<li>There exists a number of important differences between X and Y.</li>
<li>David (2020) found/observed notable/only slight differences between X and Y.</li>
</ul>
</li>
<li>indicating difference:<ul>
<li>It is very difficult to get away from calendar time in literate societies. On the other hand/By contrast, many people in oral communities have little idea of the calendar year of their birth.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Similarities<ul>
<li>introductory:<ul>
<li>Both X and Y share a number of key features.</li>
<li>There are a number of similarities between X and Y.</li>
<li>These results are similar to those reported by Smith et al.(2020), which indicates …</li>
</ul>
</li>
<li>indicating similarity:<ul>
<li>Young children learning their first language need simplified input.    Similarly/Likewise, low level adult L2 learners need graded input supplied in most cases by a teacher.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Defining-terms"><a href="#Defining-terms" class="headerlink" title="Defining terms"></a>Defining terms</h3><p>Definition giving of key words and phrases mentioned in academic work is important in order to make readers understand the terms exactly. Otherwise, misinterpretation may result. In fact, many disagreements (academic, legal, diplomatic, personal) arise as a result of different interpretations of the same term.</p>
<ul>
<li>introductory:<ul>
<li>The term ‘X’ was first used by Emma(2020) in her …</li>
<li>Previous studies mostly defined X as …</li>
</ul>
</li>
<li>simple three-part definitions:<ul>
<li>[Research is/may defined as] + [a systematic process] + [which consists of three elements or components: (1) a question, problem, or hypothesis, (2) data, and (3) analysis and interpretation of data.]</li>
</ul>
</li>
<li>general meanings or application of meanings:<ul>
<li>The term ‘X’ often refers to …</li>
<li>X can be defined as … It encompasses …</li>
<li>The term ‘X’ is a relatively new name for a Y, commonly referred to as …</li>
</ul>
</li>
<li>indicating varying definitions<ul>
<li>Several definitions of X have been proposed.</li>
<li>The term ‘X’ embodies a multitude of concepts which …</li>
<li>Despite its common usage, X is used in different disciplines to mean different things.</li>
</ul>
</li>
<li>indicating difficulties in defining a term<ul>
<li>X is challenging to term because …</li>
<li>A generally accepted definition of X is lacking.</li>
<li>There is little consensus about what X actually means.</li>
</ul>
</li>
<li>referring to people’s definitions:<ul>
<li>For Justin(2019), X means …</li>
<li>According to a definition provided by Smith (2020), X is …</li>
</ul>
</li>
<li>commmenting on a definition<ul>
<li>this definition includes/highlights the/takes into account/useful because …</li>
</ul>
</li>
</ul>
<h3 id="Giving-examples"><a href="#Giving-examples" class="headerlink" title="Giving examples"></a>Giving examples</h3><p>Specific examples are powerful tools to support writers’ general claims or arguments. Examples can also be used to help the reader or listener understand unfamilar or difficult concepts.</p>
<ul>
<li><p>examples as the main information</p>
<ul>
<li>A well-known/notable example of X is the study carried by Taylor(2020) in which …</li>
<li>For example, X used to be …</li>
<li>X is a good illustration of …</li>
<li>This is exemplified in the work undertaken by …</li>
<li>This is evident in the case of …</li>
</ul>
</li>
<li><p>examples as additional information</p>
<ul>
<li>Young people begin smoking for a variety of reasons, such as …</li>
<li>Many diseases can result at least in part from stress, including: …, …, and …</li>
</ul>
</li>
<li><p>reporting cases as support</p>
<ul>
<li>This has been seen in the case of …</li>
<li>The case reported here illustrates the …</li>
<li>This case study confirms the importance of …</li>
</ul>
</li>
</ul>
<h3 id="Signalling-transition"><a href="#Signalling-transition" class="headerlink" title="Signalling transition"></a>Signalling transition</h3><p>Previewing what is to follow in a paper or dissertation is like showing a road map to a driver. It must be accurate, but it must be easy to follow.<br>Writers are also expected to indicate to the reader when they are <strong>moving from one topic to another</strong>.</p>
<ul>
<li>previewing sections of text<ul>
<li>The section below describes …</li>
<li>What follows is an account of …</li>
<li>The problem of X is discussed in the following section.</li>
<li>A more detailed account of X is given in the following section.</li>
<li>These analytical procedures and the results obtained from them are described in the next chapter.</li>
</ul>
</li>
</ul>
<ul>
<li>introducing a new topic<ul>
<li>Regarding X, …</li>
<li>With regard/respect to X, ..</li>
<li>In terms of X, …</li>
<li>In the case of X, …</li>
<li>As far as X is concerned, …</li>
</ul>
</li>
</ul>
<ul>
<li><strong>reintroducing</strong> a topic<ul>
<li>As discussed above, …</li>
<li>As explained earlier, …</li>
<li>As previously stated, …</li>
<li>As explained/pointed out in the introduction, it is clear that …</li>
</ul>
</li>
</ul>
<ul>
<li>moving from one section to the next<ul>
<li>Let us now turn to …</li>
<li>Before proceeding to examine X, it is important to …</li>
<li>So far this paper has focused on X. The following section will discuss …</li>
<li>This chapter has demonstrated that … It is now necessary to explain the course of …</li>
</ul>
</li>
</ul>
<ul>
<li><strong>summarising</strong> a section<ul>
<li>Thus far, the thesis has argued that …</li>
<li>To conclude this section, the literature identifies …</li>
<li>In summary, this chapter has described the methods used in this investigation and it has …</li>
</ul>
</li>
</ul>
<hr>
<p><em>To see more</em>: <a href="http://www.phrasebank.manchester.ac.uk/" target="_blank" rel="noopener">Academic Phrasebank</a></p>
]]></content>
      <categories>
        <category>Writing</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Academic Writing 3</title>
    <url>/2020/03/08/2020-03-08-academic-writing-3/</url>
    <content><![CDATA[<h3 id="Explaining-causality"><a href="#Explaining-causality" class="headerlink" title="Explaining causality"></a>Explaining causality</h3><p>A great deal of academic work involves understanding and suggesting solutions to problems. However, solutions cannot be suggested unless the problem is fully analysed, and this involves a thorough understanding of the causes.</p>
<ul>
<li><p>verbs indicating causality</p>
<ul>
<li>… may cause …</li>
<li>… can lead to …</li>
<li>… can result in …</li>
<li>… is a X caused by …</li>
<li>… is a X resulting from …</li>
<li>… stems from …</li>
<li>… can give rise to …</li>
</ul>
</li>
<li><p>verbs indicating contributive agency</p>
<ul>
<li>X has contributed to the decline in …</li>
<li>X plays an important role in …</li>
<li>X exerts a powerful effect upon Z through …</li>
<li>X is only one of many factors that help to …</li>
<li>A number of factors are known to affect …</li>
<li>Several factors are known to<ul>
<li>affect X.</li>
<li>shape X.</li>
<li>predict X.</li>
<li>increase X.</li>
<li>influence X.</li>
<li>be associated with …</li>
<li>increase the risk of …</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>nouns indicating causality<ul>
<li>One reason why … is that …</li>
<li>A consequence of … is …</li>
<li>The causal role of X in Y has been demonstrated by …</li>
<li>X can have profound health consequences for older people.</li>
</ul>
</li>
</ul>
<ul>
<li>nouns indicating contributive agency<ul>
<li>X is a key/major/dominant/underlying factor in …</li>
<li>X has a significant impact on …</li>
<li>X is generally seen as a factor strongly related to Y.</li>
<li>X is a major influence on …</li>
<li>This work has revealed several factors that are responsible for …</li>
</ul>
</li>
</ul>
<ul>
<li>prepositional phrases indicating causality<ul>
<li>… owing to …</li>
<li>… as a result of …</li>
<li>… as a consequence of …</li>
</ul>
</li>
</ul>
<ul>
<li>sentence connectors indicating causality<ul>
<li>Threrfore</li>
<li>Consequently</li>
<li>Because of this</li>
<li>As a result (of this)</li>
</ul>
</li>
</ul>
<ul>
<li>adverbial elements indicating causality<ul>
<li>The warm air rises above the surface of the sea, thus/thereby creating an area of low pressure.</li>
</ul>
</li>
</ul>
<ul>
<li><p>expressing a causal relationship tentatively</p>
<ul>
<li>X may have been an important factor in …</li>
<li>X may have contributed to the increase in …</li>
<li>X may have been caused by an increase in …</li>
<li>X in many cases may be associated with …</li>
<li>There is some evidence that X may affect Y.</li>
<li>This suggests a weak link may exist between X and Y.</li>
</ul>
<h3 id="Describing-quantities"><a href="#Describing-quantities" class="headerlink" title="Describing quantities"></a>Describing quantities</h3><p>This is an important part as a common place needed to express in many fields of study.</p>
<ul>
<li><p>describing fractions</p>
<ul>
<li>over half of those surveyed indicated that …</li>
<li>nearly half of the respondents(48%) agreed that …</li>
<li>approximately half of those surveyed did not comment on …</li>
<li>less than a third of those who responded(32%) indicated that …</li>
<li>of the …, just under a quarter …</li>
<li>around/almost/as many as half of …</li>
</ul>
</li>
<li><p>describing proportions</p>
<ul>
<li>the highest proportion of ….</li>
<li>the birth rate dropped from 44.4 to 38.6 per 1000</li>
<li>the proportion of … reached on in ten in 2019</li>
</ul>
</li>
<li><p>describing percetages</p>
<ul>
<li>80% of those who were interviewed indicated that …</li>
<li>… has experienced an 90% increase in …</li>
<li>just over 5% of … in 2017 were …</li>
<li>recent surveys from … yielded a 34% response rate</li>
<li>the probability of having X increased by 9.6%(p=0.06)</li>
<li>…, of whom 62% were …</li>
</ul>
</li>
<li><p>describing averages</p>
<ul>
<li>the average of …. is</li>
<li>the proposed model suggests a steep decline in mean …</li>
<li>the mean score for X was subjected to …</li>
<li>mean estimated … is ….</li>
</ul>
</li>
<li><p>describing ranges</p>
<ul>
<li>Estimates of X range from … to …</li>
<li>The respondents had practised for an average of 15 years (range 6 to 35 years).</li>
<li>The participants were aged 19 to 25 and were …</li>
<li>… ranged from 2.71 – 0.08 with a mean of 0.97 …</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Describing-trends"><a href="#Describing-trends" class="headerlink" title="Describing trends"></a>Describing trends</h3><p>A trend is the general direction in which something is developing or changing over time. A projection is a prediction of future change. Trends and projections are usually illustrated using line graphs in which the horizontal axis represents time.</p>
<ul>
<li><p>describing trends</p>
<ul>
<li>the graph above shows that there has been a [slight/steep/sharp/steady/marked] [fall/rise/drop/decline/increase/decrease] in the number of …</li>
</ul>
</li>
<li><p>highlighting a trend in a table or chart</p>
<ul>
<li>what is striking in this table is the rapid decrease in …</li>
<li>what stands out in this table is the general pattern of …</li>
<li>what can be clearly seen in this table is the difference between …</li>
</ul>
</li>
<li><p>describing high and low points in figures</p>
<ul>
<li>production of X peaked in 2019</li>
<li>X rose to a high point and peaked in …</li>
<li>the peak … for …</li>
<li>the rate fell to a low point of …</li>
</ul>
</li>
<li><p>projecting trends</p>
<ul>
<li>the rate/amount/number of X [is likely/will probably/is expected to] fall/rise to/decline by/remain steady/grow by more than …</li>
</ul>
</li>
</ul>
<hr>
<p><em>To see more</em>: <a href="http://www.phrasebank.manchester.ac.uk/" target="_blank" rel="noopener">Academic Phrasebank</a></p>
]]></content>
      <categories>
        <category>Writing</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to reinforcement learning</title>
    <url>/2020/03/06/2020-03-06-intro2rl/</url>
    <content><![CDATA[<p>Reinforcement learning(RL) is a branch of machine learning, and as the principle behind <a href="https://en.wikipedia.org/wiki/AlphaGo" target="_blank" rel="noopener">Alpha Go</a>, it differs from supervised learning  and unsupervised learning. Today we will briefly glance at the basic ideas of RL.</p>
<h3 id="How-does-it-work"><a href="#How-does-it-work" class="headerlink" title="How does it work?"></a>How does it work?</h3><p>It basically follows the “game” model below:</p>
<p>Agent &lt;—&gt; Environment</p>
<ul>
<li>observation -&gt; return <strong>State</strong></li>
<li>some actions (changing the env) -&gt; return <strong>Reward</strong></li>
</ul>
<p><img src="/images/rl-intro1.png" alt="alt img1"></p>
<p>Let’s turn our attention to a real-life scenario:</p>
<p>observing a cup of water —&gt; spoiling the water —&gt; reward is negetive((Don’t do that!) —&gt; next state(spoiled water) —&gt; cleaning up —&gt; reward is positive(thank you!) —&gt; …</p>
<h5 id="Agent-learns-to-take-actions-to-maximize-expected-rewards"><a href="#Agent-learns-to-take-actions-to-maximize-expected-rewards" class="headerlink" title="Agent learns to take actions to maximize expected rewards."></a>Agent learns to take actions to maximize expected rewards.</h5><p><em>Remarks</em>:  it often takes a series of actions to obtain one <strong>REWARD</strong> —-&gt; like actually learned something during a whole process.</p>
<h3 id="What’s-new"><a href="#What’s-new" class="headerlink" title="What’s new?"></a>What’s new?</h3><p>Let’s compare RL with classic Labeling behavior: <strong>supervised learning</strong></p>
<h5 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h5><p>We do some labeling stuff in order to indicate _”how to act a single action”_ for a specific given state (Hey! You kinda need do this)</p>
<p>eg: Chatting Bot: “hello”-&gt;”hi”; “bye”-&gt;”bye”</p>
<h5 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h5><p>First move -&gt; second move -&gt; …. -&gt; win(reward=0)  self-summary during a ton of training</p>
<p>eg: Chatting Bot:  “xxxxxyyyy”-&gt;”hi”; “xxxx”-&gt;”bye” (may messed up) —-&gt; give bad rewards —-&gt; change the content of speaking</p>
<p><img src="/images/rl-intro2.png" alt="alt img2"></p>
<h4 id="An-example-for-RL-playing-video-game"><a href="#An-example-for-RL-playing-video-game" class="headerlink" title="An example for RL: playing video game"></a>An example for RL: playing video game</h4><ul>
<li>machine observes pixels</li>
<li>machine learns to take proper action itself</li>
</ul>
<p>If we let the machine(a bot) play games itself, this may be a process like this:</p>
<ol>
<li>start with observation (like <strong>State</strong>) $s_1$</li>
<li>action $a_1$: “turn right”</li>
<li>obtain reward: $r_1=0$ (no enemy being killed)</li>
<li>observation $s_2$ (containing some stochastic elements)</li>
<li>action $a_2$: “fire”</li>
<li>obtain reward: $r_2=5$ (Enemy falls! We kinda get some job done!)</li>
<li>observation $s_3$…<br>…</li>
</ol>
<p>After many turns in the final,<br>action $a_n$ —-&gt; obtain correspnding reward $r_n$ —-&gt; final result(<strong>win</strong> or <strong>lose</strong>, can be a crucial reward)</p>
<p>This is called an <strong>episode</strong>.</p>
<p>In summary, the <strong>mission of the agent</strong>: Learn to maximize the expected cumulative reward per episode (to find the optimal <strong>sequence of actions</strong>)</p>
<h4 id="Difficulties-therein"><a href="#Difficulties-therein" class="headerlink" title="Difficulties therein:"></a>Difficulties therein:</h4><ul>
<li>how to evaluate the “reward” line</li>
<li>reward delay: some actions are important even crucial but there may not be reward immediately</li>
<li>sometimes there exists sacrifice action —-&gt; for the long-term reward</li>
<li>agent’s actions affect the subsequent data it receives</li>
</ul>
<h4 id="Features-about-Reinforcement-learning"><a href="#Features-about-Reinforcement-learning" class="headerlink" title="Features about Reinforcement learning:"></a>Features about Reinforcement learning:</h4><ul>
<li>no supervisor(label), only a reward signal(most of which is <strong>delayed</strong></li>
<li>rewards are often delayed (as mentioned above)</li>
<li>time really matters (sequential data, non-$i.i.d$ data</li>
<li>agent’s actions <strong>affect</strong> the subsequent data it receives</li>
<li>generalizing ability is not good, dependent on the training data (kinda like an overfitting neural network! It only works well by its experienced routine)</li>
</ul>
<h3 id="Another-thing-to-think-about-rewards"><a href="#Another-thing-to-think-about-rewards" class="headerlink" title="Another thing to think about: rewards"></a>Another thing to think about: rewards</h3><p>A reward signal defines the foal in a RL problem</p>
<ul>
<li>a reward $R_t$ is a scalar feedback signal</li>
<li>the agent’s sole objective is to maximize the total reward it receives over the long run</li>
<li>indicates how well the agent is doing at step $t$</li>
</ul>
<p>RL is based on the reward hypothesis.</p>
<p>_Definition_: All goals can be described by the <strong>maximization of expected cumulative reward</strong>.</p>
<h4 id="Example-of-rewards"><a href="#Example-of-rewards" class="headerlink" title="Example of rewards:"></a>Example of rewards:</h4><p><strong>Game</strong>:</p>
<ul>
<li>win +</li>
<li>lost -</li>
</ul>
<p><strong>Portfolio(financial investment)</strong>:</p>
<ul>
<li>profit +</li>
<li>deficit -</li>
</ul>
<p><strong>Robot walk</strong>:</p>
<ul>
<li>forward motion +</li>
<li>falling -</li>
</ul>
<h5 id="Interaction-between-agent-and-environment-the-“Algorithm”"><a href="#Interaction-between-agent-and-environment-the-“Algorithm”" class="headerlink" title="Interaction between agent and environment: the “Algorithm”"></a>Interaction between agent and environment: the “Algorithm”</h5><p><strong>[Each step t]</strong>, like a time time series:</p>
<p>The agent:</p>
<ul>
<li>exec action $A_t$</li>
<li>—-&gt; receive observation $O_t$</li>
<li>—-&gt; receive scalar reward $R_t$</li>
</ul>
<p>The env:</p>
<ul>
<li>receive action $A_t$</li>
<li>emit observation $O_{t+1}$ —-&gt;</li>
<li>emit scalr reward $R_{t+1}$ ——&gt;</li>
</ul>
<p>$t$++;</p>
<p>The core of the training process and its efficiency is: <strong>How to define the state?</strong></p>
<p>Usually, we will give the state from the view of stochastic process:</p>
<p>history $H_t$: as a triad time series (_Markov property_): observation, action, reward, at given time $t$<br>state $S_t$: as the whole information(Markov) stored in $H_t$, state can be a function of $H_t$: $S_t = f(H_t)$</p>
<p><img src="/images/rl-intro3.png" alt="alt img3"></p>
<p>The design of the environment is important for training, for the whole model of reinforcement learning.</p>
<hr>
<p><strong>Reference</strong>: <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener">Wikipedia-reinforcement Learning</a>, <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">Teaching slides of Prof. David Silver at UCL</a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Machine-learning</tag>
        <tag>Reinforcement-leraning</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU-accelerated Matrix-matrix Multiplication</title>
    <url>/2020/03/05/2020-03-05-gpu-dgemm/</url>
    <content><![CDATA[<p>In this post, we will talk about a applied case about high performance computing: how to use GPU(Graphics Processing Unit) to accelerate a computation task, like a matrix-matrix multiplication in double-precision.</p>
<p>As a brief guidance to get to know the cuda programming, this post is divided into several sections. By following the procedure, you can easily cook your own codes.</p>
<p>For generality, we use C language to finish the programming.</p>
<h4 id="Prerequisite-cuda-env"><a href="#Prerequisite-cuda-env" class="headerlink" title="Prerequisite: cuda env"></a>Prerequisite: cuda env</h4><p>In order to get the build-up job done, we need to make sure our hardware, proper GPU is in a good state. In our experiment, <strong><em>NVIDIA [Tesla V100 PCIe 32GB]</em></strong> is adopted.</p>
<p><strong>Checking the GPU information on the present node</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lspci | grep -i nvidia</span><br><span class="line">05:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] (rev a1)</span><br><span class="line">82:00.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] (rev a1)</span><br></pre></td></tr></table></figure><br>By inputting the command <code>lspci</code>, we obtain the information needed: <strong>Tesla V100</strong> serves the purpose.</p>
<p>We can subsequently go visiting the <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA Toolkit</a> to fetch the proper CUDA package.</p>
<p><strong>Check the information of GPU driver</strong><br>Following this command:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;driver&#x2F;nvidia&#x2F;version</span><br></pre></td></tr></table></figure></p>
<p><strong>Installing the CUDA package</strong></p>
<p>It is such a convenient process! See the tutorial video below:</p>
<blockquote>
<p><a href="https://www.youtube.com/watch?v=pB6h_hFpRGo" target="_blank" rel="noopener">Installing the CUDA Toolkit</a></p>
</blockquote>
<p><em>-&gt;To uninstall the cuda?</em><br>We just need only run the script in ${CUDA_HOME}/bin:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;usr&#x2F;local&#x2F;cuda-9.2&#x2F;bin&#x2F;uninstall_cuda-9.2.pl</span><br></pre></td></tr></table></figure><br>Uninstall the driver along with it:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;usr&#x2F;bin&#x2F;nvidia-uninstall</span><br></pre></td></tr></table></figure></p>
<p><strong>Setting the env variables</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64</span><br></pre></td></tr></table></figure><br>(the <em>/path/</em> varys since there exists a ton of different archs, but we need to add the <em>bin</em> dir and <em>lib64</em> dir, respectively)</p>
<p>and then input the command as <strong><em>nvcc -V</em></strong>, if it prints something like this:</p>
<blockquote>
<p>Copyright (c) 2005-2017 NVIDIA Corporation<br>Built on Fri_Sep__1_21:08:03_CDT_2017<br>Cuda compilation tools, release 9.0, V9.0.176</p>
</blockquote>
<p>We have finished the half! The rest of the post will continue to shed some light on the cuda-based code writing.</p>
<h4 id="Coding"><a href="#Coding" class="headerlink" title="Coding !"></a>Coding !</h4><p>Before we dive into the parallelism, the normal kernel must be figured out. Take it easy! It is a super-easy one.</p>
<p><strong>Matrix-matrix Multiplication</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for (int i&#x3D;0;i&lt;m;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j&#x3D;0;j&lt;m;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            for (int k&#x3D;0;k&lt;n;k++)</span><br><span class="line">                res_mat[(i*m) + j] +&#x3D; lmat[ (i*n) + k] * rmat[ (k*n) + j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><br><em>Remarks</em>:</p>
<ul>
<li>The essential operation (in math) is simple as: $AB = C$</li>
<li>The size of each element: lmat: $m\times n$ ; rmat: $n\times m$ ; res_mat: $m\times m$</li>
<li>Using linear array to represent the matrix:$matrix~~ element A[i,j] = array~~arr[(i-1)*n + j]$</li>
<li>This kernel is a <strong>triple loop</strong>.</li>
</ul>
<p><strong>Make the cuda package available to the C code</strong><br>Similarly, by adding the following sentences to the head of your C-code, we can subsequently use the relevant function of cuda.<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;cuda.h&gt;</span><br><span class="line">#include &lt;cuda_runtime.h&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>Initialization of cuda</strong><br>We can get some basic information about the device.<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;the number of devices that support cuda</span><br><span class="line">cudaGetDeviceCount(&amp;count);</span><br><span class="line">if (count &#x3D;&#x3D; 0)</span><br><span class="line">&#123;</span><br><span class="line">    fprintf(stderr, &quot;There is no device.\n&quot;);</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br><span class="line">int i;</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i++)</span><br><span class="line">&#123;</span><br><span class="line">cudaDeviceProp prop;</span><br><span class="line">cudaGetDeviceProperties(&amp;prop, i);</span><br><span class="line">printDeviceProp(prop); &#x2F;&#x2F;prop struct has many properties of device</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>Writing the Computational Kernel</strong></p>
<ul>
<li>Kernel is very important to GPU-accelerated program, because it will be executed by GPU;</li>
<li>Identifier <em>global</em> is used for definition;</li>
<li><strong>Kernel_function &lt;&lt;&lt; BLOCK_NUM, THREAD_NUM, SHARED_MEM &gt;&gt;&gt; (param1, param2,…)</strong> is the standard calling sentence of the computation.*(SHARED_MEM can be left out)</li>
</ul>
<p>Save the file as ./gpu_dgemm.cu. With <strong>.cu</strong> suffix, the C-code can be distinguished by nvcc compiler</p>
<p><strong>Memcpy To and From Device(GPU)</strong><br>There are basically several functions about such routine:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cudaMalloc((void **)&amp;g_lmat,    sizeof(double) * m * n); &#x2F;&#x2F; firstly declaring needed gpu memory space like in main memory.</span><br><span class="line">&#x2F;&#x2F; Remember to declare extra cuda(gpu) pointer for each matrix</span><br><span class="line">cudaMemcpy(g_lmat,lmat, sizeof(double) * m * n, cudaMemcpyHostToDevice); &#x2F;&#x2F; the last param indicate the direction: from main memory to gpu</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; call Kernel_function</span><br><span class="line"></span><br><span class="line">cudaMemcpy(res_mat, g_res_mat, sizeof(double) * m * m, cudaMemcpyDeviceToHost); &#x2F;&#x2F; bring back the result</span><br><span class="line"></span><br><span class="line">cudaFree(g_lmat); &#x2F;&#x2F; free the gpu-memory</span><br></pre></td></tr></table></figure>
<p><strong>Get Rank of Thread and Block</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">const int tid &#x3D; threadIdx.x; &#x2F;&#x2F; from 0, each thread has its specifier</span><br><span class="line">const int bid &#x3D; blockIdx.x; &#x2F;&#x2F; from 0, each thread has its specifier</span><br></pre></td></tr></table></figure></p>
<h4 id="Running-stage"><a href="#Running-stage" class="headerlink" title="Running stage"></a>Running stage</h4><p><strong>Supervisor of the use of GPU</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nvidia-smi</span><br><span class="line">$ watch -n 1 nvidia-smi # real-time</span><br></pre></td></tr></table></figure></p>
<p><strong>Compiling command</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvcc -o main.o .&#x2F;gpu_dgemm.cu -O3 -I &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include -L&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64 -lcudart</span><br></pre></td></tr></table></figure></p>
<p>Finally, run the compiled file:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;main.o</span><br></pre></td></tr></table></figure></p>
<p><strong>Reference</strong>: <a href="https://devblogs.nvidia.com/even-easier-introduction-cuda/" target="_blank" rel="noopener">Introduction to cuda (NVIDIA)</a>; <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA Toolkit</a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>High-performance-computing gpu-acceleration</tag>
      </tags>
  </entry>
  <entry>
    <title>HeapSort</title>
    <url>/2020/03/04/2020-03-04-heapsort/</url>
    <content><![CDATA[<p>_”How to find the $k$th largest of $n$ unsorted numbers?”_</p>
<p>Remember this question? We summarized some common algorithms about _Sorting_ in the previous post, but one of the most efficient way for it is <strong>HeapSort</strong>. And today we are gonna talk about it.</p>
<h3 id="HeapSort-and-its-application"><a href="#HeapSort-and-its-application" class="headerlink" title="HeapSort and its application"></a>HeapSort and its application</h3><p>HeapSort is based on Binary Heap data structure.</p>
<p>Here is a concept to be introduced: <strong>A complete binary tree</strong> is a binary tree in which every level, except possibly the last, is completely filled. And all nodes are as far left as possible.</p>
<p>A max binary heap is a complete binary tree where items are stored in a special order such that value in a parent node is greater than the values in its two children nodes. _The heap can be represented by binary tree or array._</p>
<p>If the parent node is stored at index Idx, the left child can be calculated by $2$ <em> Idx + $1$ and right child by $2$ </em> Idx + $2$ (assuming the indexing starts at 0).</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEFT(index) ( (index &lt;&lt; 1) + 1)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RIGHT(index) ( (index &lt;&lt; 1)+ 2)</span></span><br></pre></td></tr></table></figure>
<p>Let’s move on to the algorithm.</p>
<h4 id="HeapSort-Algorithm"><a href="#HeapSort-Algorithm" class="headerlink" title="HeapSort Algorithm"></a>HeapSort Algorithm</h4><ol>
<li>build a max heap from original array, now the largest item is stored at the root of the heap</li>
<li>Replace the root with the last item of the heap followed by reducing the size of heap by 1, finally, heapify the root of tree (heap_size —shrink—&gt; $1$)</li>
<li>repeat above steps while size of heap is greater than $1$</li>
</ol>
<p><img src="/images/sorting-heapsort.gif" alt="alt img1"><br>But we have not articulated: <strong>how to build a heap?</strong></p>
<p>Heapify procedure can be applied to a node only if <strong>its children nodes</strong> are heapified; otherwise, heapify its children first.</p>
<p>Thus, the heapification <strong>in one time</strong> must be performed in the <strong>bottom up order</strong>.</p>
<p>Recursively heapifying the rest of array will totally sort the whole one.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input data: 4, 10, 3, 5, 1</span><br><span class="line">         4(0)</span><br><span class="line">        &#x2F;   \</span><br><span class="line">     10(1)   3(2)</span><br><span class="line">    &#x2F;   \</span><br><span class="line"> 5(3)    1(4)</span><br><span class="line"></span><br><span class="line">The numbers in bracket represent the indices in the array</span><br><span class="line">representation of data.</span><br><span class="line"></span><br><span class="line">Applying heapify procedure to index 1:</span><br><span class="line">         4(0)</span><br><span class="line">        &#x2F;   \</span><br><span class="line">    10(1)    3(2)</span><br><span class="line">    &#x2F;   \</span><br><span class="line">5(3)    1(4)</span><br><span class="line"></span><br><span class="line">Applying heapify procedure to index 0:</span><br><span class="line">        10(0)</span><br><span class="line">        &#x2F;  \</span><br><span class="line">     5(1)  3(2)</span><br><span class="line">    &#x2F;   \</span><br><span class="line"> 4(3)    1(4)</span><br><span class="line">The heapify procedure calls itself recursively to build heap</span><br><span class="line"> in top down manner.</span><br></pre></td></tr></table></figure>
<p>The illustration above is of a simple procedure of <strong>Heapify.</strong></p>
<h5 id="Notes"><a href="#Notes" class="headerlink" title="Notes:"></a>Notes:</h5><ul>
<li>Heap sort is an <strong>in-place</strong> algorithm.</li>
<li>Its typical implementation is <strong>not stable</strong>, but can be made stable.</li>
</ul>
<h5 id="Time-Complexity"><a href="#Time-Complexity" class="headerlink" title="Time Complexity:"></a>Time Complexity:</h5><p>Time complexity of heapify is $O(logn)$. Time complexity of createAndBuildHeap() is $O(n)$ and,<br>overall time complexity of HeapSort is $O(nlogn)$.</p>
<p>Then we can trace back to our initial question:<br>_”How to find the $k$th largest of $n$ unsorted numbers?”_</p>
<p>Using Max Heap can easily solve this with time complexity by $O(n + klogn)$:</p>
<p>1) Build a Max Heap tree in $O(n)$;<br>2) Use Extract Max k times to get k maximum elements from the Max Heap $O(klogn)$</p>
<p>Or Min Heap can also help, which exhibits some tricky idea:</p>
<p>1) Build a Min Heap MH of the first k elements (arr[0] to arr[k-1]) of the given array. $O(k)$</p>
<p>2) For each element, after the kth element (arr[k] to arr[n-1]), compare it with root of MH.</p>
<ul>
<li>If the element is greater than the root then make it root and call heapify for MH</li>
<li>Else ignore it.<br>// The step 2 is $O((n-k)*logk)$</li>
</ul>
<p>3) Finally, MH has $k$ largest elements and root of the MH is the kth largest element.</p>
<h5 id="Time-Complexity-1"><a href="#Time-Complexity-1" class="headerlink" title="Time Complexity:"></a>Time Complexity:</h5><p>$O(k + (n-k)logk)$ without sorted output. If sorted output is needed then $O(k + (n-k)logk + klogk)$</p>
<h4 id="C-Code-implementation-k-th-largest-number"><a href="#C-Code-implementation-k-th-largest-number" class="headerlink" title="C++ Code implementation: $k$th largest number"></a>C++ Code implementation: $k$th largest number</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// about index of children</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEFT(index) ( (index &lt;&lt; 1) + 1)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> RIGHT(index) ( (index &lt;&lt; 1)+ 2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> &amp;a, <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> tmp = a;a = b;b = tmp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinHeap</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> length;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> *arr;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// construction of heap (with-&gt;length)for one time for one given length</span></span><br><span class="line">    MinHeap(<span class="keyword">int</span> input_length, <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> input_arr[])</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;length = input_length;</span><br><span class="line">    <span class="keyword">this</span>-&gt;arr = input_arr;</span><br><span class="line">    build_heap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(index &gt;= length / <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> smallest = index; <span class="comment">// initialize the smallest as present root</span></span><br><span class="line">        <span class="keyword">int</span> l = LEFT(index);</span><br><span class="line">        <span class="keyword">int</span> r = RIGHT(index);</span><br><span class="line">        <span class="comment">// if left child is larger</span></span><br><span class="line">        <span class="keyword">if</span> (l &lt; length &amp;&amp; arr[l] &lt; arr[smallest])</span><br><span class="line">            smallest = l;</span><br><span class="line">        <span class="comment">// if right child is larger</span></span><br><span class="line">        <span class="keyword">if</span> (r &lt; length &amp;&amp; arr[r] &lt; arr[smallest])</span><br><span class="line">            smallest = r;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//if smallest is not present root, i.e., swap has been done</span></span><br><span class="line">        <span class="keyword">if</span> (smallest != index)</span><br><span class="line">        &#123;</span><br><span class="line">            swap(arr[index], arr[smallest]);</span><br><span class="line">            <span class="comment">//recursively heapify the 'affected' sub-tree (l- r- child as root), because we changed the heapified subtree!</span></span><br><span class="line">            heapify(smallest);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// build a min heap for k-smallest</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">build_heap</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">//  (length &gt;&gt; 1) - 1: the deepest subtree, heapify for smallest -1</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = (length &gt;&gt; <span class="number">1</span>) - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">            heapify(i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// maintain a k's minheap, and traverse the rest of the array</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FirstKlargest</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> arr[], <span class="keyword">int</span> length, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    MinHeap *m = <span class="keyword">new</span> MinHeap(k,arr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=k; i&lt;length;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(arr[<span class="number">0</span>]&gt;arr[i])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            arr[<span class="number">0</span>] = arr[i];</span><br><span class="line">            m-&gt;heapify(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; arr[<span class="number">0</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">free</span>(m);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> length, k;</span><br><span class="line">    <span class="comment">//cerr &lt;&lt; "Please input length and k (both integer): ";</span></span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; length &gt;&gt; k;</span><br><span class="line">    <span class="keyword">if</span>( k &lt; <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cerr</span> &lt;&lt; <span class="string">"please input k &gt; 1"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> <span class="keyword">int</span> arr[length];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; arr[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FirstKlargest(arr, length, k);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>Reference</strong>: <a href="https://en.wikipedia.org/wiki/Heapsort" target="_blank" rel="noopener">Wikipedia: HeapSort</a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3 little features of function</title>
    <url>/2020/03/03/2020-03-03-python-func/</url>
    <content><![CDATA[<p>As note-taking of the book <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener"><em>David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition</em></a>, this post is mainly from the chapter-7: <em>Function</em>.</p>
<h4 id="Function-that-accepts-any-number-of-arguments"><a href="#Function-that-accepts-any-number-of-arguments" class="headerlink" title="Function that accepts any number of arguments"></a>Function that accepts any number of arguments</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">def avg(first, *rest):</span><br><span class="line">    return (first + sum(rest)) &#x2F; (1 + len(rest))</span><br><span class="line"></span><br><span class="line"># Sample use</span><br><span class="line">avg(1, 2) # 1.5</span><br><span class="line">avg(1, 2, 3, 4) # 2.5</span><br></pre></td></tr></table></figure>
<p>By using a ‘<em>‘ parameter, </em>rest<em> is a <strong>tuple</strong> consisting of the rest of parameters(after <em>*first</em></em>).</p>
<p><strong>Dict</strong> type is also supported as <em>**kwargs</em>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import html</span><br><span class="line"></span><br><span class="line">def make_element(name, value, **attrs):</span><br><span class="line">    keyvals &#x3D; [&#39; %s&#x3D;&quot;%s&quot;&#39; % item for item in attrs.items()]</span><br><span class="line">    attr_str &#x3D; &#39;&#39;.join(keyvals)</span><br><span class="line">    element &#x3D; &#39;&lt;&#123;name&#125;&#123;attrs&#125;&gt;&#123;value&#125;&lt;&#x2F;&#123;name&#125;&gt;&#39;.format(</span><br><span class="line">                name&#x3D;name,</span><br><span class="line">                attrs&#x3D;attr_str,</span><br><span class="line">                value&#x3D;html.escape(value))</span><br><span class="line">    return element</span><br><span class="line"></span><br><span class="line"># Example</span><br><span class="line"># Creates &#39;&lt;item size&#x3D;&quot;large&quot; quantity&#x3D;&quot;6&quot;&gt;Albatross&lt;&#x2F;item&gt;&#39;</span><br><span class="line">make_element(&#39;item&#39;, &#39;Albatross&#39;, size&#x3D;&#39;large&#39;, quantity&#x3D;6)</span><br><span class="line"></span><br><span class="line"># Creates &#39;&lt;p&gt;&lt;spam&gt;&lt;&#x2F;p&gt;&#39;</span><br><span class="line">make_element(&#39;p&#39;, &#39;&lt;spam&gt;&#39;)</span><br></pre></td></tr></table></figure>
<p>_Remarks:_ the _**kwargs_ parameter is only allowed to pose in the tail of function definition list.</p>
<h4 id="Force-the-param-passing-as-key-value-way"><a href="#Force-the-param-passing-as-key-value-way" class="headerlink" title="Force the param-passing as key-value way"></a>Force the param-passing as key-value way</h4><p>We can put the parameter after an ‘<em>‘ or </em>args, like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def recv(maxsize, *, block):</span><br><span class="line">    &#39;Receives a message&#39;</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line">recv(1024, True) # TypeError</span><br><span class="line">recv(1024, block&#x3D;True) # Ok</span><br><span class="line"></span><br><span class="line">#----------------- Another example: when number of &#39;value&#39; arguments is indefinite</span><br><span class="line"></span><br><span class="line">def minimum(*values, clip&#x3D;None):</span><br><span class="line">    m &#x3D; min(values)</span><br><span class="line">    if clip is not None:</span><br><span class="line">        m &#x3D; clip if clip &gt; m else m</span><br><span class="line">    return m</span><br><span class="line"></span><br><span class="line">minimum(1, 5, 2, -5, 10) # Returns -5</span><br><span class="line">minimum(1, 5, 2, -5, 10, clip&#x3D;0) # Returns 0</span><br></pre></td></tr></table></figure>
<h4 id="Add-annotation-to-a-function"><a href="#Add-annotation-to-a-function" class="headerlink" title="Add annotation to a function"></a>Add annotation to a function</h4><p>The annotation format is as follows:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def add(x:int, y:int) -&gt; int:</span><br><span class="line">    return x + y</span><br></pre></td></tr></table></figure><br>_(this declaration-like annotation can also appear in help(), thus very useful)_</p>
<p>There is no discrepancy relative to that before annotating.</p>
<p>The annotations are stored in attribute <strong>__annotations__</strong>:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; add.__annotations__</span><br><span class="line">&#123;&#39;y&#39;: &lt;class &#39;int&#39;&gt;, &#39;return&#39;: &lt;class &#39;int&#39;&gt;, &#39;x&#39;: &lt;class &#39;int&#39;&gt;&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="Reduce-the-input-arguments"><a href="#Reduce-the-input-arguments" class="headerlink" title="Reduce the input arguments"></a>Reduce the input arguments</h4><p>Suppose you have a callable object (eg. a function) utilized by other python codes, but it has too many arguments to make mistakes.</p>
<p>By using <strong>functools.partial()</strong>, we can set part of parameters ahead of time:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def spam(a, b, c, d):</span><br><span class="line">    print(a, b, c, d)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; from functools import partial</span><br><span class="line">&gt;&gt;&gt; s1 &#x3D; partial(spam, 1) # a &#x3D; 1</span><br><span class="line">&gt;&gt;&gt; s1(2, 3, 4)</span><br><span class="line">1 2 3 4</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; s2 &#x3D; partial(spam, d&#x3D;42) # d &#x3D; 42</span><br><span class="line">&gt;&gt;&gt; s2(1, 2, 3)</span><br><span class="line">1 2 3 42</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; s3 &#x3D; partial(spam, 1, 2, d&#x3D;42) # a &#x3D; 1, b &#x3D; 2, d &#x3D; 42</span><br><span class="line">&gt;&gt;&gt; s3(3)</span><br><span class="line">1 2 3 42</span><br></pre></td></tr></table></figure>
<p>The wrapper partial() returns a new callable object. It is worth mentioning that, this job can also be done by <strong>lambda</strong>, but it gets more clearer by useing partial().</p>
<p><strong>Applications</strong>:<br>“Relative-like” processing:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">points &#x3D; [ (1, 2), (3, 4), (5, 6), (7, 8) ]</span><br><span class="line"></span><br><span class="line">import math</span><br><span class="line">def distance(p1, p2):</span><br><span class="line">    x1, y1 &#x3D; p1</span><br><span class="line">    x2, y2 &#x3D; p2</span><br><span class="line">    return math.hypot(x2 - x1, y2 - y1)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pt &#x3D; (4, 3) # base</span><br><span class="line">&gt;&gt;&gt; points.sort(key&#x3D;partial(distance,pt))</span><br><span class="line">&gt;&gt;&gt; points</span><br><span class="line">[(3, 4), (1, 2), (5, 6), (7, 8)]</span><br></pre></td></tr></table></figure></p>
<h4 id="Transform-class-with-single-method-into-function"><a href="#Transform-class-with-single-method-into-function" class="headerlink" title="Transform class with single method into function"></a>Transform class with single method into function</h4><p>Generally, the reason to create a class with single method instead of normal function is that we need save some template values/or state values, which will later be fed to the method. But this can be done by create a function closure:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line"></span><br><span class="line">class UrlTemplate:</span><br><span class="line">    def __init__(self, template):</span><br><span class="line">        self.template &#x3D; template</span><br><span class="line"></span><br><span class="line">    def open(self, **kwargs):</span><br><span class="line">        return urlopen(self.template.format_map(kwargs))</span><br><span class="line"></span><br><span class="line"># Example use. Download stock data from yahoo</span><br><span class="line">yahoo &#x3D; UrlTemplate(&#39;http:&#x2F;&#x2F;finance.yahoo.com&#x2F;d&#x2F;quotes.csv?s&#x3D;&#123;names&#125;&amp;f&#x3D;&#123;fields&#125;&#39;) # object created, data feeding</span><br><span class="line">for line in yahoo.open(names&#x3D;&#39;IBM,AAPL,FB&#39;, fields&#x3D;&#39;sl1c1v&#39;):</span><br><span class="line">    print(line.decode(&#39;utf-8&#39;))</span><br><span class="line"></span><br><span class="line"># --------using closure--------</span><br><span class="line">def urltemplate(template):</span><br><span class="line">    def opener(**kwargs):</span><br><span class="line">        return urlopen(template.format_map(kwargs))</span><br><span class="line">    return opener</span><br><span class="line"></span><br><span class="line"># Example use</span><br><span class="line">yahoo &#x3D; urltemplate(&#39;http:&#x2F;&#x2F;finance.yahoo.com&#x2F;d&#x2F;quotes.csv?s&#x3D;&#123;names&#125;&amp;f&#x3D;&#123;fields&#125;&#39;)</span><br><span class="line">for line in yahoo(names&#x3D;&#39;IBM,AAPL,FB&#39;, fields&#x3D;&#39;sl1c1v&#39;):</span><br><span class="line">    print(line.decode(&#39;utf-8&#39;))</span><br></pre></td></tr></table></figure>
<p>or in this way, with callback function, to capture and save the state information:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class ResultHandler:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.sequence &#x3D; 0</span><br><span class="line"></span><br><span class="line">    def handler(self, result):</span><br><span class="line">        self.sequence +&#x3D; 1</span><br><span class="line">        print(&#39;[&#123;&#125;] Got: &#123;&#125;&#39;.format(self.sequence, result))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; r &#x3D; ResultHandler()</span><br><span class="line">&gt;&gt;&gt; apply_async(add, (2, 3), callback&#x3D;r.handler)</span><br><span class="line">[1] Got: 5</span><br><span class="line">&gt;&gt;&gt; apply_async(add, (&#39;hello&#39;, &#39;world&#39;), callback&#x3D;r.handler)</span><br><span class="line">[2] Got: helloworld</span><br><span class="line"></span><br><span class="line"># --------using closure--------</span><br><span class="line">def make_handler():</span><br><span class="line">  sequence &#x3D; 0 # static var</span><br><span class="line">  def handler(result):</span><br><span class="line">      nonlocal sequence</span><br><span class="line">      sequence +&#x3D; 1</span><br><span class="line">      print(&#39;[&#123;&#125;] Got: &#123;&#125;&#39;.format(sequence, result))</span><br><span class="line">  return handler</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; handler &#x3D; make_handler()</span><br><span class="line">&gt;&gt;&gt; apply_async(add, (2, 3), callback&#x3D;handler)</span><br><span class="line">[1] Got: 5</span><br><span class="line">&gt;&gt;&gt; apply_async(add, (&#39;hello&#39;, &#39;world&#39;), callback&#x3D;handler)</span><br><span class="line">[2] Got: helloworld</span><br></pre></td></tr></table></figure>
<p>We can even adopt <strong>Coroutine</strong> to implement:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def make_handler():</span><br><span class="line">    sequence &#x3D; 0</span><br><span class="line">    while True:</span><br><span class="line">        result &#x3D; yield</span><br><span class="line">        sequence +&#x3D; 1</span><br><span class="line">        print(&#39;[&#123;&#125;] Got: &#123;&#125;&#39;.format(sequence, result))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; handler &#x3D; make_handler()</span><br><span class="line">&gt;&gt;&gt; next(handler) # Advance to the yield</span><br><span class="line">&gt;&gt;&gt; apply_async(add, (2, 3), callback&#x3D;handler.send)# method: send() as callback function</span><br><span class="line">[1] Got: 5</span><br><span class="line">&gt;&gt;&gt; apply_async(add, (&#39;hello&#39;, &#39;world&#39;), callback&#x3D;handler.send)</span><br><span class="line">[2] Got: helloworld</span><br></pre></td></tr></table></figure>
<h4 id="Visit-the-variable-in-closure"><a href="#Visit-the-variable-in-closure" class="headerlink" title="Visit the variable in closure"></a>Visit the variable in closure</h4><p>In most of the cases, the variables within closure are totally hidden, but we can do the visiting by extend the closeure.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sample():</span><br><span class="line">    n &#x3D; 0</span><br><span class="line">    # Closure function</span><br><span class="line">    def func():</span><br><span class="line">        print(&#39;n&#x3D;&#39;, n)</span><br><span class="line"></span><br><span class="line">    # Accessor methods for n</span><br><span class="line">    def get_n():</span><br><span class="line">        return n</span><br><span class="line"></span><br><span class="line">    def set_n(value):</span><br><span class="line">        nonlocal n</span><br><span class="line">        n &#x3D; value</span><br><span class="line"></span><br><span class="line">    # Attach as function attributes</span><br><span class="line">    func.get_n &#x3D; get_n</span><br><span class="line">    func.set_n &#x3D; set_n</span><br><span class="line">    return func</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; f &#x3D; sample()</span><br><span class="line">&gt;&gt;&gt; f()</span><br><span class="line">n&#x3D; 0</span><br><span class="line">&gt;&gt;&gt; f.set_n(10)</span><br><span class="line">&gt;&gt;&gt; f()</span><br><span class="line">n&#x3D; 10</span><br><span class="line">&gt;&gt;&gt; f.get_n()</span><br><span class="line">10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ------Another example------</span><br><span class="line"></span><br><span class="line">class ClosureInstance:</span><br><span class="line">    def __init__(self, locals&#x3D;None):</span><br><span class="line">        if locals is None:</span><br><span class="line">            locals &#x3D; sys._getframe(1).f_locals</span><br><span class="line"></span><br><span class="line">        # Update instance dictionary with callables</span><br><span class="line">        self.__dict__.update((key,value) for key, value in locals.items()</span><br><span class="line">                            if callable(value) )</span><br><span class="line">    # Redirect special methods</span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.__dict__[&#39;__len__&#39;]()</span><br><span class="line"></span><br><span class="line"># Example use</span><br><span class="line">def Stack():</span><br><span class="line">    items &#x3D; []</span><br><span class="line">    def push(item):</span><br><span class="line">        items.append(item)</span><br><span class="line"></span><br><span class="line">    def pop():</span><br><span class="line">        return items.pop()</span><br><span class="line"></span><br><span class="line">    def __len__():</span><br><span class="line">        return len(items)</span><br><span class="line"></span><br><span class="line">    return ClosureInstance()</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; s &#x3D; Stack()</span><br><span class="line">&gt;&gt;&gt; s</span><br><span class="line">&lt;__main__.ClosureInstance object at 0x10069ed10&gt;</span><br><span class="line">&gt;&gt;&gt; s.push(10)</span><br><span class="line">&gt;&gt;&gt; s.push(20)</span><br><span class="line">&gt;&gt;&gt; s.push(&#39;Hello&#39;)</span><br><span class="line">&gt;&gt;&gt; len(s)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; s.pop()</span><br><span class="line">&#39;Hello&#39;</span><br><span class="line">&gt;&gt;&gt; s.pop()</span><br><span class="line">20</span><br><span class="line">&gt;&gt;&gt; s.pop()</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<p>Why not use class? Because the closure running is faster a bit. For more understandable, it is better to use class under common circumstances.</p>
<hr>
<p><strong>Reference</strong>: <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener">_David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition_</a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to memory hierarchies</title>
    <url>/2020/03/02/2020-03-02-memory-hierarchies/</url>
    <content><![CDATA[<p>Data can be stored in computer system in different ways. In general, <em>registers</em> inside CPUs are the closest friend to arithmetic units, but however they are small. Then it comes to <em>caches</em>, which are also fast and small and holing copies of recently used data items. <em>Main memory</em> is such a painful “slow guy” that we start to consider the bandwidth of the data transmission, but it owns larger space. And <em>Disk</em> is the final repository of data.</p>
<p>Our topic is focused on the pathway from CPUs to main memory.</p>
<h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h3><p>Caches are low-capacity, high-spped memories.</p>
<p>It is located(integrated) on the CPU tie.</p>
<p>Data transfer rates from CPUs to main memory do great contribute to the whole arithmetic performance.</p>
<p><em>(Term)</em> <strong><em>Latency</em></strong>: waiting time elapsed to transfer a zero-byte message from memory. <em>(minimal single data item)</em></p>
<p><img src="/images/processor-img3.png" alt="alt img1"></p>
<p>Usually there are at least <em>two levels</em> of cache, called <strong>L1 and L2</strong>, which are included by domain of “CPU chip”.</p>
<p><strong>L1</strong> is normally split into two parts, one for instructions(“I-cache”,”L1I”) and one for data(“L1D”). Outer cache levels are normally unified storing data as well as instructions.</p>
<p>In general, the closer a cache(in hierarchy) to the CPU’s registers, i.e., the higher its bandwidth and the lower its latency, the smaller it must be to keep administration overhead low. Second-level cache has usually larger latency but similar bandwidth to L1.</p>
<h4 id="Workflow-of-read"><a href="#Workflow-of-read" class="headerlink" title="Workflow of read:"></a>Workflow of read:</h4><blockquote>
<p>Firstly the CPU issues a read request (“load”) for transferring a data item to a register, first-level cache logic checks whether this item already resides in cache. If it does, this is called a <em>cache hit</em> and the request can be satisfied immediately, with low latency. In case of a <em>cache miss</em>, however, data must be fetched from outer cache levels or, in the worst case, from main memory.</p>
</blockquote>
<p><strong>Remarks</strong>:</p>
<ul>
<li><p>If all cache entries are occupied, hardware-implemented algorithm <strong>evicts old items*</strong>(not necessary the oldest)* from cache and replace them new data. In fact, there are often three strategies for this:</p>
<ul>
<li><em>least recently used<strong>(LRU)</strong></em></li>
<li><em>not recently used<strong>(NRU)</strong></em></li>
<li><em>random replacement</em></li>
</ul>
</li>
<li><p>Instruction caches are usually of minor importance since sci- entific codes tend to be largely loop-based; I-cache misses are rare events compared to D-cache misses.</p>
</li>
<li><p>Caches can only have a positive effect on performance, if the data access pattern of some application shows some <em>locality of reference</em>.(read a specific block of memory)</p>
</li>
</ul>
<p>A simple model is introduced to estimate the performance gain.</p>
<p>$\tau$: a scaling factor (<em>access time to main memory</em> $T_m$ longer than that to cache $T_c$)</p>
<script type="math/tex; mode=display">T_c = T_m/\tau</script><p>$\beta$: _cache reuse ratio_, i.e., the fraction of loads or stores that can be satisfied from cache, because of the existence of recent load/store.</p>
<p>For some finite $\beta$, the average access time:</p>
<script type="math/tex; mode=display">T_{av} = \beta T_{c} + (1-\beta) T_m</script><p>an <strong>access performance gain</strong> of:</p>
<script type="math/tex; mode=display">G(\tau,\beta) = \frac{T_m}{T_{av}} = \frac{\tau T_c}{\beta T_c + (1-\beta) \tau T_c} = \frac{\tau}{\beta + \tau(1-\beta)}</script><p>A cache can only lead to a significant performance advantage if $\beta$ is relatively close to one.</p>
<h4 id="Cache-lines"><a href="#Cache-lines" class="headerlink" title="Cache lines"></a>Cache lines</h4><p>In order to handle the real applications, which show _streaming_ patterns where large amounts of data are loaded into the CPU, caches feature a peculiar organization into cache lines. Streaming pattern (tons of data I/O) disables the potential reuse. Now all data transfers _between caches and main memory_ happen on the <strong>cache line level</strong>, to some degree as a whole. <strong>Neighboring items</strong> can then be loaded from cache (successive accesses), such that increasing the _cache hit ratio $\gamma$_, as well as reducing the _Latency_ problem.</p>
<p>Thus, the latency penalty of a _cache miss_ occurs only on the first miss on an item belonging to a line.</p>
<p>Also remember, the erratic data access can cause cache miss and <strong>pollute the memory bus</strong> at the same time. In such case, the effective bandwidth will thus be very low.</p>
<p>_(Def)<strong>Memory bound</strong>_, if the performance is governed by main memory bandwidth and latency.</p>
<p>_(Def)<strong>Cache bound</strong>_, describes the scenario that $\gamma$ must be larger such that the time it takes to process in-cache data becomes larger than the time reloading it.</p>
<h4 id="Storing-Data"><a href="#Storing-Data" class="headerlink" title="Storing Data"></a>Storing Data</h4><p>Storing data is a little more involved. If data to be written out already resides in cache, a _write hit_ occurs. <strong>Write-back</strong> strategy is proposed to solve it: the cache line is modified in cache and written to memory as a whole when evicted.</p>
<p>On a _write miss_, however, <strong>cache-memory consistency</strong>  dictates that the cache line in question must first be transferred from memory to cache before an entry can be modified.</p>
<p>This is called _write allocate_.</p>
<p>Therefore, a data write stream towards memory uses the bus twice: once for _all the cache line allocation_, once for _evicting modified lines_.</p>
<p>Generally two strategies are adopted on some architectures as _Nontemporal stores_ and _Cache line zero_. We do not talk about them here in detail.</p>
<h4 id="Cache-mapping"><a href="#Cache-mapping" class="headerlink" title="Cache mapping"></a>Cache mapping</h4><p>The basic of the working of cache line is that the cache line can be associated with specific memory location: _fully associative_. For each cache line the cache logic <strong>must store its location</strong> in the CPU’s address space, and each memory access must be checked against the list of all those addresses.</p>
<p>Memory locations lie a multiple of the cache size apart.</p>
<p><img src="/images/memory-cacheline.png" alt="alt img2"></p>
<p>In a _direct-mapped cache_, mapping are done on the full cache size repeatedly into memory as shown in the figure above. No hardware or clock cycles need to be spent for it, which make the cache lines be loaded into and evicted from the cache in rapid succession.</p>
<p>The drawback of that is _cache thrashing_.</p>
<p>_Example_ for a “strided” vector triad code for DP data. (Using the cache size in units of DP words as a stride)</p>
<figure class="highlight fortran"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span> i=<span class="number">1</span>,N,CACHE_SIZE_IN_BYTES/<span class="number">8</span></span><br><span class="line">  A(i) = B(i) + C(i) * D(i)</span><br><span class="line"><span class="keyword">enddo</span></span><br></pre></td></tr></table></figure>
<p>Successive loop iterations hit the same cache line so that <strong>every memory access generates a cache miss</strong>, even though a whole line is loaded each time. This situation is denoted as _conflict miss_.</p>
<p>In order to keep administrative overhead low and also reduce the danger of _conflict misses_ and _cache thrashing_, a _set-associative cache_ is divided into _m_ direct-mapped caches equal in size, so-called _ways_.</p>
<p><img src="/images/memory-setascache.png" alt="alt img3"></p>
<p>The number of ways _m_ is the number of <strong>different cache lines</strong> a memory address can be mapped to.</p>
<p>On each memory access, the <strong>hardware</strong> merely has to determine which way the data resides in.</p>
<h4 id="Prefetch"><a href="#Prefetch" class="headerlink" title="Prefetch"></a>Prefetch</h4><p>There is still the problem of latency on the first miss. Look at the following code:</p>
<figure class="highlight fortran"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span> i=<span class="number">1</span>,N</span><br><span class="line">S = S + A(i)*A(i)</span><br><span class="line"><span class="keyword">enddo</span></span><br></pre></td></tr></table></figure>
<p>Only one load stream exists. Cache misses will lead to latency penalty that occurs on each new miss for this loop.</p>
<p><img src="/images/memory-miss.png" alt="alt img4"></p>
<p>Now latency has thus an even more severe impact on performance than bandwidth.</p>
<p>The latency problem, however, can be solved in many cases by <strong>prefetching</strong>, which supplies the cache with data ahead of the actual requirements of an application.</p>
<p>The compiler can do this by <strong>interleaving special instructions</strong> with the software pipelined instruction stream. The latter will “touch” cache lines early enough to give hardware time to load them into cache <strong>asynchronously</strong>.</p>
<p><img src="/images/memory-prefetch.png" alt="alt img5"></p>
<p>Till now, the latency is completely <strong>hidden</strong> by prefetching! A prerequisite is the hardware prefetcher, which can detect regular access patterns, keep up the continuous data stream.</p>
<p>The number of outstanding prefetches required can be estimated in the following way.</p>
<p>let:<br>$T_l$ be the latency;<br>$B$ be the bandwidth;<br>$L_c$ be the target transfer whole line _(in bytes)_</p>
<p>Time taken of :</p>
<script type="math/tex; mode=display">T = T_l + \frac{L_c}{B}</script><p>Thus the processor must sustain:</p>
<script type="math/tex; mode=display">P = \frac{T}{L_c/B} = 1+ \frac{T_l}{L_c/B}</script><p>prefetches.</p>
<p>If this requirement _(for processors)_ cannot be met, latency will not be hidden completely and the full memory bandwidth will not be utilized.</p>
<p>All in all, prefetching does great job to solve the latency problem but has almost no contribution to the improvement of bandwidth limitation.</p>
<hr>
<p><strong>Reference</strong>: Hager, Wellein, Introduction to High Performance Computing for Scientists and Engineers(2010)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>High-performance-computing</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3-based data operation-1</title>
    <url>/2020/03/01/2020-03-01-python-data-op1/</url>
    <content><![CDATA[<p>In this posts, as note-taking of the book <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener"><em>David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition</em></a>, we are going to talk about at large: how to “master” the python3 programming.</p>
<p>This post is mainly from the chapter-1: <em>Data Structure and Algorithm</em>. Notes of the rest of chapters will be posted in the near future.</p>
<h3 id="Data-Structure-and-Algorithm-Python3-implementation"><a href="#Data-Structure-and-Algorithm-Python3-implementation" class="headerlink" title="Data Structure and Algorithm: Python3 implementation"></a>Data Structure and Algorithm: Python3 implementation</h3><p>Python provides programmers with a lot of built-in data structures, including list, set, dict, etc, and it is convenient to take advantage of such ready interfaces.</p>
<hr>
<h4 id="‘-’-expression-to-generate-list-object-saving-variables-with-indefinite-length"><a href="#‘-’-expression-to-generate-list-object-saving-variables-with-indefinite-length" class="headerlink" title="‘*’ expression to generate list object, saving variables with indefinite length"></a>‘*’ expression to generate list object, saving variables with indefinite length</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; record &#x3D; (&#39;Dave&#39;, &#39;dave@example.com&#39;, &#39;773-555-1212&#39;, &#39;847-555-1212&#39;)</span><br><span class="line">&gt;&gt;&gt; name, email, *phone_numbers &#x3D; record</span><br></pre></td></tr></table></figure>
<p>This can also be used in definition of parameter list or input argument of function, and any iterating elements with indefinite length.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def do_foo(*l):</span><br><span class="line">    for i in l:</span><br><span class="line">      print i</span><br><span class="line"># ---------------</span><br><span class="line">def do_foo2(x, y):</span><br><span class="line">    print(&#39;foo&#39;, x, y)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; l &#x3D; [1,2]</span><br><span class="line">&gt;&gt;&gt; do_foo2(*l)</span><br><span class="line">(&#39;foo&#39;, 1, 2)</span><br></pre></td></tr></table></figure>
<p>Operation of string splitting:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; line &#x3D; &#39;nobody:*:-2:-2:Unprivileged User:&#x2F;var&#x2F;empty:&#x2F;usr&#x2F;bin&#x2F;false&#39;</span><br><span class="line">&gt;&gt;&gt; uname, *fields, homedir, sh &#x3D; line.split(&#39;:&#39;)</span><br><span class="line">&gt;&gt;&gt; uname</span><br><span class="line">&#39;nobody&#39;</span><br><span class="line">&gt;&gt;&gt; homedir</span><br><span class="line">&#39;&#x2F;var&#x2F;empty&#39;</span><br><span class="line">&gt;&gt;&gt; sh</span><br><span class="line">&#39;&#x2F;usr&#x2F;bin&#x2F;false&#39;</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>
<h4 id="Build-in-queue-structure"><a href="#Build-in-queue-structure" class="headerlink" title="Build-in queue structure"></a>Build-in queue structure</h4><p><strong>collections.deque</strong></p>
<blockquote>
<p>using deque(maxlen=N) will construct a length-fixed (or infinity) queue. When appending the new one, the oldest element will be popped out.</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; q &#x3D; deque(maxlen&#x3D;3)</span><br><span class="line">&gt;&gt;&gt; q.append(1)</span><br><span class="line">&gt;&gt;&gt; q.append(2)</span><br><span class="line">&gt;&gt;&gt; q.append(3)</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([1, 2, 3], maxlen&#x3D;3)</span><br><span class="line">&gt;&gt;&gt; q.append(4)</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([2, 3, 4], maxlen&#x3D;3)</span><br><span class="line">#-----------</span><br><span class="line">&gt;&gt;&gt; q.append(1)</span><br><span class="line">&gt;&gt;&gt; q.append(2)</span><br><span class="line">&gt;&gt;&gt; q.append(3)</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; q.appendleft(4)</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([4, 1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; q.pop()</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; q</span><br><span class="line">deque([4, 1, 2])</span><br><span class="line">&gt;&gt;&gt; q.popleft()</span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<p>Although you can also do this by using <strong>list</strong>, but deque is more specialized and able to run faster($O(1) complexity for append/pop of deque versus $O(n) that of list$$).</p>
<h4 id="Using-heap-structures"><a href="#Using-heap-structures" class="headerlink" title="Using heap structures"></a>Using heap structures</h4><p><strong>heapq.nlargest()</strong> and <strong>heapq.nsmallest()</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import heapq</span><br><span class="line">nums &#x3D; [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]</span><br><span class="line">print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]</span><br><span class="line">print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]</span><br><span class="line">#--------conditional param------</span><br><span class="line">portfolio &#x3D; [</span><br><span class="line">    &#123;&#39;name&#39;: &#39;IBM&#39;, &#39;shares&#39;: 100, &#39;price&#39;: 91.1&#125;,</span><br><span class="line">    &#123;&#39;name&#39;: &#39;AAPL&#39;, &#39;shares&#39;: 50, &#39;price&#39;: 543.22&#125;,</span><br><span class="line">    &#123;&#39;name&#39;: &#39;FB&#39;, &#39;shares&#39;: 200, &#39;price&#39;: 21.09&#125;,</span><br><span class="line">    &#123;&#39;name&#39;: &#39;HPQ&#39;, &#39;shares&#39;: 35, &#39;price&#39;: 31.75&#125;,</span><br><span class="line">    &#123;&#39;name&#39;: &#39;YHOO&#39;, &#39;shares&#39;: 45, &#39;price&#39;: 16.35&#125;,</span><br><span class="line">    &#123;&#39;name&#39;: &#39;ACME&#39;, &#39;shares&#39;: 75, &#39;price&#39;: 115.65&#125;</span><br><span class="line">]</span><br><span class="line"># compare values with key:[&#39;price&#39;]</span><br><span class="line">cheap &#x3D; heapq.nsmallest(3, portfolio, key&#x3D;lambda s: s[&#39;price&#39;])</span><br><span class="line">expensive &#x3D; heapq.nlargest(3, portfolio, key&#x3D;lambda s: s[&#39;price&#39;])</span><br></pre></td></tr></table></figure>
<p>How to find the $k$th largest/smallest number?</p>
<p>According to the previous post, we know that heap sort is a good stable method to finish this task. We can conveniently use the heap data structure in Python3.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; nums &#x3D; [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]</span><br><span class="line">&gt;&gt;&gt; import heapq</span><br><span class="line">&gt;&gt;&gt; heap &#x3D; list(nums)</span><br><span class="line">&gt;&gt;&gt; heapq.heapify(heap)</span><br><span class="line">&gt;&gt;&gt; heap</span><br><span class="line">[-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8]</span><br></pre></td></tr></table></figure>
<p><strong>Features of heap</strong>:</p>
<ul>
<li>heap[0] is always the smallest element</li>
<li>we can subsequently obtain the rest element by calling <strong>heapq.heappop(heap)</strong></li>
</ul>
<p><em>Remarks</em>: if the $k$ is $1$, i.e. the unique minimum/maximum of a list, then using min() and max() can be faster; or if the $k$ approaches N, the length of the list, we would rather sorted() the list.</p>
<h4 id="Implement-a-priority-queue-each-element-with-a-priority-index"><a href="#Implement-a-priority-queue-each-element-with-a-priority-index" class="headerlink" title="Implement a priority queue(each element with a priority index)"></a>Implement a priority queue(each element with a priority index)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import heapq</span><br><span class="line"></span><br><span class="line">class PriorityQueue:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self._queue &#x3D; []</span><br><span class="line">        self._index &#x3D; 0</span><br><span class="line"></span><br><span class="line">    def push(self, item, priority):</span><br><span class="line">        heapq.heappush(self._queue, (-priority, self._index, item))</span><br><span class="line">        self._index +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        return heapq.heappop(self._queue)[-1]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; class Item:</span><br><span class="line">...     def __init__(self, name):</span><br><span class="line">...         self.name &#x3D; name</span><br><span class="line">...     def __repr__(self):</span><br><span class="line">...         return &#39;Item(&#123;!r&#125;)&#39;.format(self.name)</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; q &#x3D; PriorityQueue()</span><br><span class="line">&gt;&gt;&gt; q.push(Item(&#39;foo&#39;), 1)</span><br><span class="line">&gt;&gt;&gt; q.push(Item(&#39;bar&#39;), 5)</span><br><span class="line">&gt;&gt;&gt; q.push(Item(&#39;spam&#39;), 4)</span><br><span class="line">&gt;&gt;&gt; q.push(Item(&#39;grok&#39;), 1)</span><br><span class="line">&gt;&gt;&gt; q.pop()</span><br><span class="line">Item(&#39;bar&#39;)</span><br><span class="line">&gt;&gt;&gt; q.pop()</span><br><span class="line">Item(&#39;spam&#39;)</span><br><span class="line">&gt;&gt;&gt; q.pop()</span><br><span class="line">Item(&#39;foo&#39;)</span><br><span class="line">&gt;&gt;&gt; q.pop() # same priority will follow its innate sequence when pushing</span><br><span class="line">Item(&#39;grok&#39;)</span><br></pre></td></tr></table></figure>
<p><em>Remarks</em>:</p>
<ul>
<li>for <strong>heap</strong>, push() and pop() both have complexity of $O(logN)$</li>
<li>in method <strong>push()</strong>, minus priority enable its equence to be descending: from high to low</li>
<li>on top of that, var <em>index</em> enable the same priority to follow its pushing sequence (counting)</li>
</ul>
<h4 id="Multidict-multi-value-mapping"><a href="#Multidict-multi-value-mapping" class="headerlink" title="Multidict: multi-value mapping"></a>Multidict: multi-value mapping</h4><p>Generally, the mapping holds for one-to-one or many-to-one key-value combination. If we need to realize the one-to-many mapping, then we can consider the <strong>collections.defaultdict</strong>.</p>
<p>A distinguishable difference between common dict-list and defaultdict is :</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># dict-list has a hard time initializing is value: we need build empty list for each key</span><br><span class="line">d &#x3D; &#123;</span><br><span class="line">    &#39;a&#39; : [1, 2, 3],</span><br><span class="line">    &#39;b&#39; : [4, 5]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">d &#x3D; defaultdict(list) # param: mapping results&#39; storing type</span><br><span class="line">d[&#39;a&#39;].append(1)</span><br><span class="line">d[&#39;a&#39;].append(2)</span><br><span class="line">d[&#39;b&#39;].append(4)</span><br></pre></td></tr></table></figure>
<h4 id="Dict-sorting"><a href="#Dict-sorting" class="headerlink" title="Dict sorting"></a>Dict sorting</h4><p>If we want to construct a dict and keep the sequence when the elements are pushed, <strong>OrderedDict</strong> is a good choice. The principle of OrderedDict is that inside there is a <em>doubly linked list</em> being maintained, thus occupying <strong>double store space</strong>.<br>It is useful when we want to <strong>serialize</strong> the data(pickle, json).</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import OrderedDict</span><br><span class="line"></span><br><span class="line">d &#x3D; OrderedDict()</span><br><span class="line">d[&#39;foo&#39;] &#x3D; 1</span><br><span class="line">d[&#39;bar&#39;] &#x3D; 2</span><br><span class="line">d[&#39;spam&#39;] &#x3D; 3</span><br><span class="line">d[&#39;grok&#39;] &#x3D; 4</span><br><span class="line"># Outputs &quot;foo 1&quot;, &quot;bar 2&quot;, &quot;spam 3&quot;, &quot;grok 4&quot;</span><br><span class="line">for key in d:</span><br><span class="line">    print(key, d[key])</span><br></pre></td></tr></table></figure>
<h4 id="Key-value-reversal-zip"><a href="#Key-value-reversal-zip" class="headerlink" title="Key-value reversal: zip()"></a>Key-value reversal: zip()</h4><p>For some of the operation on <em>dict</em>, the operand will always come from the dict.keys(). Like:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">prices &#x3D; &#123;</span><br><span class="line">    &#39;ACME&#39;: 45.23,</span><br><span class="line">    &#39;AAPL&#39;: 612.78,</span><br><span class="line">    &#39;IBM&#39;: 205.55,</span><br><span class="line">    &#39;HPQ&#39;: 37.20,</span><br><span class="line">    &#39;FB&#39;: 10.75</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">min(prices) # Returns &#39;AAPL&#39;</span><br><span class="line">max(prices) # Returns &#39;IBM&#39;</span><br><span class="line"></span><br><span class="line"># The results below, however, is also not what we want</span><br><span class="line">min(prices.values()) # Returns 10.75</span><br><span class="line">max(prices.values()) # Returns 612.78</span><br><span class="line"></span><br><span class="line"># solution: zip() constructing a read-once iterator</span><br><span class="line">min_price &#x3D; min(zip(prices.values(), prices.keys()))</span><br><span class="line"># min_price is (10.75, &#39;FB&#39;)</span><br><span class="line">max_price &#x3D; max(zip(prices.values(), prices.keys()))</span><br><span class="line"># max_price is (612.78, &#39;AAPL&#39;)</span><br><span class="line"></span><br><span class="line"># more useful..</span><br><span class="line"></span><br><span class="line">prices_sorted &#x3D; sorted(zip(prices.values(), prices.keys()))</span><br><span class="line"># prices_sorted is [(10.75, &#39;FB&#39;), (37.2, &#39;HPQ&#39;),</span><br><span class="line">#                   (45.23, &#39;ACME&#39;), (205.55, &#39;IBM&#39;),</span><br><span class="line">#                   (612.78, &#39;AAPL&#39;)]</span><br></pre></td></tr></table></figure>
<h4 id="Finding-pairs-in-common-for-two-dicts"><a href="#Finding-pairs-in-common-for-two-dicts" class="headerlink" title="Finding pairs in common for two dicts"></a>Finding pairs in common for two dicts</h4><p>Use operation <strong>&amp;</strong>(common) and <strong>-</strong>(except) like the operations of set:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; &#123;&#39;x&#39; : 1,&#39;y&#39; : 2,&#39;z&#39; : 3&#125;</span><br><span class="line">b &#x3D; &#123;&#39;w&#39; : 10,&#39;x&#39; : 11,&#39;y&#39; : 2&#125;</span><br><span class="line"></span><br><span class="line"># Find keys in a that are not in b</span><br><span class="line">a.keys() - b.keys() # &#123; &#39;z&#39; &#125;</span><br><span class="line"># Find (key,value) pairs in common</span><br><span class="line">a.items() &amp; b.items() # &#123; (&#39;y&#39;, 2) &#125;</span><br><span class="line"></span><br><span class="line"># Make a new dictionary with certain keys removed</span><br><span class="line">c &#x3D; &#123;key:a[key] for key in a.keys() - &#123;&#39;z&#39;, &#39;w&#39;&#125;&#125;</span><br><span class="line"># c is &#123;&#39;x&#39;: 1, &#39;y&#39;: 2&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Transfer-to-set-containing-no-repeating-elements"><a href="#Transfer-to-set-containing-no-repeating-elements" class="headerlink" title="Transfer to set(containing no repeating elements)"></a>Transfer to set(containing no repeating elements)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def dedupe(items, key&#x3D;None): # for hashable obj like list, key&#x3D;1</span><br><span class="line">    seen &#x3D; set()</span><br><span class="line">    for item in items:</span><br><span class="line">        val &#x3D; item if key is None else key(item)</span><br><span class="line">        if val not in seen:</span><br><span class="line">            yield item</span><br><span class="line">            seen.add(val)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a &#x3D; [ &#123;&#39;x&#39;:1, &#39;y&#39;:2&#125;, &#123;&#39;x&#39;:1, &#39;y&#39;:3&#125;, &#123;&#39;x&#39;:1, &#39;y&#39;:2&#125;, &#123;&#39;x&#39;:2, &#39;y&#39;:4&#125;]</span><br><span class="line">&gt;&gt;&gt; list(dedupe(a, key&#x3D;lambda d: (d[&#39;x&#39;],d[&#39;y&#39;])))</span><br><span class="line">[&#123;&#39;x&#39;: 1, &#39;y&#39;: 2&#125;, &#123;&#39;x&#39;: 1, &#39;y&#39;: 3&#125;, &#123;&#39;x&#39;: 2, &#39;y&#39;: 4&#125;]</span><br><span class="line">&gt;&gt;&gt; list(dedupe(a, key&#x3D;lambda d: d[&#39;x&#39;]))</span><br><span class="line">[&#123;&#39;x&#39;: 1, &#39;y&#39;: 2&#125;, &#123;&#39;x&#39;: 2, &#39;y&#39;: 4&#125;]</span><br><span class="line"></span><br><span class="line"># even more useful to reduce repeated lines</span><br><span class="line">with open(somefile,&#39;r&#39;) as f:</span><br><span class="line">for line in dedupe(f):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p><em>To be continued…</em></p>
<hr>
<p><strong>Reference</strong>: <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener"><em>David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition</em></a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title>Python3-based data operation-2</title>
    <url>/2020/03/01/2020-03-01-python-data-op2/</url>
    <content><![CDATA[<p><em>Followed by last post <a href="https://jiaruilu.com/2020/03/01/python-data-op1.html" target="_blank" rel="noopener">Python3-based data operation-1</a>.</em></p>
<h4 id="Naming-slices"><a href="#Naming-slices" class="headerlink" title="Naming slices"></a>Naming slices</h4><p>Like a <strong>re-pattern</strong>, we may give a name to an slice pattern too such that our code can be more readable.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">record &#x3D; &#39;....................100 .......513.25 ..........&#39;</span><br><span class="line">cost &#x3D; int(record[20:23]) * float(record[31:37])</span><br><span class="line"></span><br><span class="line">SHARES &#x3D; slice(20, 23)</span><br><span class="line">PRICE &#x3D; slice(31, 37)</span><br><span class="line">cost &#x3D; int(record[SHARES]) * float(record[PRICE])</span><br></pre></td></tr></table></figure>
<h4 id="How-to-find-the-element-with-the-highest-frequency-of-occurrence"><a href="#How-to-find-the-element-with-the-highest-frequency-of-occurrence" class="headerlink" title="How to find the element with the highest frequency of occurrence?"></a>How to find the element with the highest frequency of occurrence?</h4><p><strong>collections.Counter</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">words&#x3D;[&#39;look&#39;, &#39;into&#39;, &#39;my&#39;, &#39;eyes&#39;, &#39;look&#39;, &#39;into&#39;, &#39;my&#39;, &#39;eyes&#39;,</span><br><span class="line">&#39;the&#39;, &#39;eyes&#39;, &#39;the&#39;, &#39;eyes&#39;, &#39;the&#39;, &#39;eyes&#39;, &#39;not&#39;, &#39;around&#39;, &#39;the&#39;,</span><br><span class="line">&#39;eyes&#39;, &quot;don&#39;t&quot;, &#39;look&#39;, &#39;around&#39;, &#39;the&#39;, &#39;eyes&#39;, &#39;look&#39;, &#39;into&#39;,</span><br><span class="line">&#39;my&#39;, &#39;eyes&#39;, &quot;you&#39;re&quot;, &#39;under&#39;]</span><br><span class="line">from collections import Counter</span><br><span class="line">word_counts &#x3D; Counter(words)</span><br><span class="line">top_three &#x3D; word_counts.most_common(3)</span><br><span class="line">print(top_three)</span><br><span class="line"># Outputs [(&#39;eyes&#39;, 8), (&#39;the&#39;, 5), (&#39;look&#39;, 4)]</span><br></pre></td></tr></table></figure></p>
<p>Counter object is just like a dict, accept any hashable elements to count their frequency. It is useful for some accounting like tasks.<br>Even we can add the count manually.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>morewords = [<span class="string">'why'</span>,<span class="string">'are'</span>,<span class="string">'you'</span>,<span class="string">'not'</span>,<span class="string">'looking'</span>,<span class="string">'in'</span>,<span class="string">'my'</span>,<span class="string">'eyes'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> word <span class="keyword">in</span> morewords:</span><br><span class="line"><span class="meta">... </span>    word_counts[word] += <span class="number">1</span></span><br><span class="line"><span class="comment"># word_counts.update(morewords) # or update a whole list</span></span><br></pre></td></tr></table></figure></p>
<h4 id="Sorting-a-list-of-dict-by-its-specific-keywords"><a href="#Sorting-a-list-of-dict-by-its-specific-keywords" class="headerlink" title="Sorting a list of dict by its specific keywords"></a>Sorting a list of dict by its specific keywords</h4><p><strong>operator.itemgetter</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows &#x3D; [</span><br><span class="line">    &#123;&#39;fname&#39;: &#39;Brian&#39;, &#39;lname&#39;: &#39;Jones&#39;, &#39;uid&#39;: 1003&#125;,</span><br><span class="line">    &#123;&#39;fname&#39;: &#39;David&#39;, &#39;lname&#39;: &#39;Beazley&#39;, &#39;uid&#39;: 1002&#125;,</span><br><span class="line">    &#123;&#39;fname&#39;: &#39;John&#39;, &#39;lname&#39;: &#39;Cleese&#39;, &#39;uid&#39;: 1001&#125;,</span><br><span class="line">    &#123;&#39;fname&#39;: &#39;Big&#39;, &#39;lname&#39;: &#39;Jones&#39;, &#39;uid&#39;: 1004&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">from operator import itemgetter</span><br><span class="line"></span><br><span class="line">rows_by_fname &#x3D; sorted(rows, key&#x3D;itemgetter(&#39;fname&#39;)) # equivalent to:</span><br><span class="line"># rows_by_fname &#x3D; sorted(rows, key&#x3D;lambda r: r[&#39;fname&#39;])</span><br><span class="line"></span><br><span class="line">rows_by_uid &#x3D; sorted(rows, key&#x3D;itemgetter(&#39;uid&#39;))</span><br><span class="line">print(rows_by_fname)</span><br><span class="line">print(rows_by_uid)</span><br><span class="line"></span><br><span class="line">[&#123;&#39;fname&#39;: &#39;Big&#39;, &#39;uid&#39;: 1004, &#39;lname&#39;: &#39;Jones&#39;&#125;,</span><br><span class="line">&#123;&#39;fname&#39;: &#39;Brian&#39;, &#39;uid&#39;: 1003, &#39;lname&#39;: &#39;Jones&#39;&#125;,</span><br><span class="line">&#123;&#39;fname&#39;: &#39;David&#39;, &#39;uid&#39;: 1002, &#39;lname&#39;: &#39;Beazley&#39;&#125;,</span><br><span class="line">&#123;&#39;fname&#39;: &#39;John&#39;, &#39;uid&#39;: 1001, &#39;lname&#39;: &#39;Cleese&#39;&#125;]</span><br><span class="line">[&#123;&#39;fname&#39;: &#39;John&#39;, &#39;uid&#39;: 1001, &#39;lname&#39;: &#39;Cleese&#39;&#125;,</span><br><span class="line">&#123;&#39;fname&#39;: &#39;David&#39;, &#39;uid&#39;: 1002, &#39;lname&#39;: &#39;Beazley&#39;&#125;,</span><br><span class="line">&#123;&#39;fname&#39;: &#39;Brian&#39;, &#39;uid&#39;: 1003, &#39;lname&#39;: &#39;Jones&#39;&#125;,</span><br><span class="line">&#123;&#39;fname&#39;: &#39;Big&#39;, &#39;uid&#39;: 1004, &#39;lname&#39;: &#39;Jones&#39;&#125;]</span><br></pre></td></tr></table></figure>
<p>Keys are passed onto the itemgetter() and a <strong>callable</strong> object is created, and subsequently passed to the <em>key</em> param of sorted() function.</p>
<p>Such function is also applicable to the min() and max() function</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; min(rows, key&#x3D;itemgetter(&#39;uid&#39;))</span><br><span class="line">&#123;&#39;fname&#39;: &#39;John&#39;, &#39;lname&#39;: &#39;Cleese&#39;, &#39;uid&#39;: 1001&#125;</span><br><span class="line">&gt;&gt;&gt; max(rows, key&#x3D;itemgetter(&#39;uid&#39;))</span><br><span class="line">&#123;&#39;fname&#39;: &#39;Big&#39;, &#39;lname&#39;: &#39;Jones&#39;, &#39;uid&#39;: 1004&#125;</span><br></pre></td></tr></table></figure>
<p>Similarly, the <strong>operator.attrgetter()</strong> is for the attribute of an object(same function can be achieved by <em>lambda</em>). Like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># users is a list of objects with attribute &#39;user_id&#39;</span><br><span class="line">&gt;&gt;&gt; from operator import attrgetter</span><br><span class="line">&gt;&gt;&gt; sorted(users, key&#x3D;attrgetter(&#39;user_id&#39;))</span><br><span class="line">[User(3), User(23), User(99)]</span><br></pre></td></tr></table></figure>
<h4 id="Group-the-records-by-some-specific-field"><a href="#Group-the-records-by-some-specific-field" class="headerlink" title="Group the records by some specific field"></a>Group the records by some specific field</h4><p>(<em>these functions are integrated by <strong>pandas</strong>, which is more specialized for processing the data</em>)</p>
<p><strong>itertools.groupby()</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows &#x3D; [</span><br><span class="line">    &#123;&#39;address&#39;: &#39;5412 N CLARK&#39;, &#39;date&#39;: &#39;07&#x2F;01&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;5148 N CLARK&#39;, &#39;date&#39;: &#39;07&#x2F;04&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;5800 E 58TH&#39;, &#39;date&#39;: &#39;07&#x2F;02&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;2122 N CLARK&#39;, &#39;date&#39;: &#39;07&#x2F;03&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;5645 N RAVENSWOOD&#39;, &#39;date&#39;: &#39;07&#x2F;02&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;1060 W ADDISON&#39;, &#39;date&#39;: &#39;07&#x2F;02&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;4801 N BROADWAY&#39;, &#39;date&#39;: &#39;07&#x2F;01&#x2F;2012&#39;&#125;,</span><br><span class="line">    &#123;&#39;address&#39;: &#39;1039 W GRANVILLE&#39;, &#39;date&#39;: &#39;07&#x2F;04&#x2F;2012&#39;&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">from operator import itemgetter</span><br><span class="line">from itertools import groupby</span><br><span class="line"></span><br><span class="line"># Sort by the desired field first</span><br><span class="line">rows.sort(key&#x3D;itemgetter(&#39;date&#39;))</span><br><span class="line"># Iterate in groups: return tags-[] and iterator of records(groups)</span><br><span class="line">for date, items in groupby(rows, key&#x3D;itemgetter(&#39;date&#39;)):</span><br><span class="line">    print(date)</span><br><span class="line">    for i in items:</span><br><span class="line">        print(&#39; &#39;, i)</span><br></pre></td></tr></table></figure>
<p><em>Remarks</em>: <strong>groupby()</strong> will only scan and search for the continuously identical values. So calling <strong>sorted()</strong> according to appointed field is essential.</p>
<h4 id="Filter-data-besides-List-Comprehension"><a href="#Filter-data-besides-List-Comprehension" class="headerlink" title="Filter data besides List Comprehension"></a>Filter data besides List Comprehension</h4><p>If we do not want to generate temporary data occupying tons of memory(if the list is super large), list comprehension is convenient but nevertheless not a good choice.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; mylist &#x3D; [1, 4, -5, 10, -7, 2, 3, -1]</span><br><span class="line">&gt;&gt;&gt; [n for n in mylist if n &gt; 0]</span><br></pre></td></tr></table></figure>
<p>However, we can use <strong>filter()</strong>, which returns a iterator able to be transfered into list:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">values &#x3D; [&#39;1&#39;, &#39;2&#39;, &#39;-3&#39;, &#39;-&#39;, &#39;4&#39;, &#39;N&#x2F;A&#39;, &#39;5&#39;]</span><br><span class="line">def is_int(val):</span><br><span class="line">    try:</span><br><span class="line">        x &#x3D; int(val)</span><br><span class="line">        return True</span><br><span class="line">    except ValueError:</span><br><span class="line">        return False</span><br><span class="line">ivals &#x3D; list(filter(is_int, values))</span><br><span class="line">print(ivals)</span><br><span class="line"># Outputs [&#39;1&#39;, &#39;2&#39;, &#39;-3&#39;, &#39;4&#39;, &#39;5&#39;]</span><br></pre></td></tr></table></figure>
<p><strong>itertools.compress()</strong> can be used for sorting by associated sequence . Looking at the following code:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;5412 N CLARK&#39;,</span><br><span class="line">&#39;5148 N CLARK&#39;,</span><br><span class="line">&#39;5800 E 58TH&#39;,</span><br><span class="line">&#39;2122 N CLARK&#39;,</span><br><span class="line">&#39;5645 N RAVENSWOOD&#39;,</span><br><span class="line">&#39;1060 W ADDISON&#39;,</span><br><span class="line">&#39;4801 N BROADWAY&#39;,</span><br><span class="line">&#39;1039 W GRANVILLE&#39;,</span><br><span class="line">]</span><br><span class="line">counts &#x3D; [ 0, 3, 10, 4, 1, 7, 6, 1]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; from itertools import compress</span><br><span class="line">&gt;&gt;&gt; more5 &#x3D; [n &gt; 5 for n in counts] # boolean sequence -&gt; condition</span><br><span class="line">&gt;&gt;&gt; more5</span><br><span class="line">[False, False, True, False, False, True, True, False]</span><br><span class="line">&gt;&gt;&gt; list(compress(addresses, more5)) # return directly a iterator()</span><br><span class="line">[&#39;5800 E 58TH&#39;, &#39;1060 W ADDISON&#39;, &#39;4801 N BROADWAY&#39;]</span><br></pre></td></tr></table></figure>
<h4 id="Extract-‘sub’-dict"><a href="#Extract-‘sub’-dict" class="headerlink" title="Extract ‘sub’-dict"></a>Extract ‘sub’-dict</h4><p>It is not only convenient but fast to use dict comprehension.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;ACME&#39;: 45.23,</span><br><span class="line">&#39;AAPL&#39;: 612.78,</span><br><span class="line">&#39;IBM&#39;: 205.55,</span><br><span class="line">&#39;HPQ&#39;: 37.20,</span><br><span class="line">&#39;FB&#39;: 10.75</span><br><span class="line">&#125;</span><br><span class="line"># Make a dictionary of all prices over 200</span><br><span class="line">p1 &#x3D; &#123;key: value for key, value in prices.items() if value &gt; 200&#125;</span><br><span class="line"># Make a dictionary of tech stocks</span><br><span class="line">tech_names &#x3D; &#123;&#39;AAPL&#39;, &#39;IBM&#39;, &#39;HPQ&#39;, &#39;MSFT&#39;&#125;</span><br><span class="line">p2 &#x3D; &#123;key: value for key, value in prices.items() if key in tech_names&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Mapping-names-to-elements"><a href="#Mapping-names-to-elements" class="headerlink" title="Mapping names to elements"></a>Mapping names to elements</h4><p><strong>collections.namedtuple()</strong> can help make the operation of “visit element by index” change into “visit element by name”, such that our code will be more readable.</p>
<ul>
<li>param: class-name(str), field(list/set/tuple)</li>
<li>return: class(can be initialized by giving input sequentially)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; Subscriber &#x3D; namedtuple(&#39;Subscriber&#39;, [&#39;addr&#39;, &#39;joined&#39;])</span><br><span class="line">&gt;&gt;&gt; sub &#x3D; Subscriber(&#39;jonesy@example.com&#39;, &#39;2012-10-19&#39;)</span><br><span class="line">&gt;&gt;&gt; sub</span><br><span class="line">Subscriber(addr&#x3D;&#39;jonesy@example.com&#39;, joined&#x3D;&#39;2012-10-19&#39;)</span><br><span class="line">&gt;&gt;&gt; sub.addr</span><br><span class="line">&#39;jonesy@example.com&#39;</span><br><span class="line">&gt;&gt;&gt; sub.joined</span><br><span class="line">&#39;2012-10-19&#39;</span><br><span class="line"></span><br><span class="line"># another example</span><br><span class="line"></span><br><span class="line">Stock &#x3D; namedtuple(&#39;Stock&#39;, [&#39;name&#39;, &#39;shares&#39;, &#39;price&#39;])</span><br><span class="line">def compute_cost(records):</span><br><span class="line">    total &#x3D; 0.0</span><br><span class="line">    for rec in records:</span><br><span class="line">        s &#x3D; Stock(*rec)</span><br><span class="line">        total +&#x3D; s.shares * s.price</span><br><span class="line">    return total</span><br></pre></td></tr></table></figure>
<p>Also, it can be replace the <strong>dict</strong> in order to save the memory. The difference is that <em>namedtuple</em> is basically immutable, like <em>tuple</em>.</p>
<p><em>Remarks</em>: To change the value of namedtuple, just call the private method <strong>_replace()</strong>. Its utility is to serve as <strong>“init-assign”</strong> process of a sequence of data.<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; s._replace(shares&#x3D;75)</span><br></pre></td></tr></table></figure></p>
<h4 id="Logically-merge-several-dicts-into-one"><a href="#Logically-merge-several-dicts-into-one" class="headerlink" title="Logically merge several dicts into one"></a>Logically merge several dicts into one</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; &#123;&#39;x&#39;: 1, &#39;z&#39;: 3 &#125;</span><br><span class="line">b &#x3D; &#123;&#39;y&#39;: 2, &#39;z&#39;: 4 &#125;</span><br></pre></td></tr></table></figure>
<p>If we need to find the value of key <em>‘x’</em>, firstly in a, then in b. We can use <strong>collections.ChainMap</strong>. It maintains a mapping stack(“find/delete” priority for repeated values) of these dicts and return a ChainMap object.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import ChainMap</span><br><span class="line">c &#x3D; ChainMap(a,b)</span><br><span class="line">print(c[&#39;x&#39;]) # Outputs 1 (from a)</span><br><span class="line">print(c[&#39;y&#39;]) # Outputs 2 (from b)</span><br><span class="line">print(c[&#39;z&#39;]) # Outputs 3 (from a)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; len(c)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; list(c.keys())</span><br><span class="line">[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]</span><br><span class="line">&gt;&gt;&gt; list(c.values())</span><br><span class="line">[1, 2, 3]</span><br></pre></td></tr></table></figure>
<p>There is another interesting function:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; values[&#39;x&#39;] &#x3D; 1</span><br><span class="line">&gt;&gt;&gt; # Add a new mapping</span><br><span class="line">&gt;&gt;&gt; values &#x3D; values.new_child()</span><br><span class="line">&gt;&gt;&gt; values[&#39;x&#39;] &#x3D; 2</span><br><span class="line">&gt;&gt;&gt; # Add a new mapping</span><br><span class="line">&gt;&gt;&gt; values &#x3D; values.new_child()</span><br><span class="line">&gt;&gt;&gt; values[&#39;x&#39;] &#x3D; 3</span><br><span class="line">&gt;&gt;&gt; values</span><br><span class="line">ChainMap(&#123;&#39;x&#39;: 3&#125;, &#123;&#39;x&#39;: 2&#125;, &#123;&#39;x&#39;: 1&#125;)</span><br><span class="line">&gt;&gt;&gt; values[&#39;x&#39;]</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; # Discard last mapping</span><br><span class="line">&gt;&gt;&gt; values &#x3D; values.parents</span><br><span class="line">&gt;&gt;&gt; values[&#39;x&#39;]</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; # Discard last mapping</span><br><span class="line">&gt;&gt;&gt; values &#x3D; values.parents</span><br><span class="line">&gt;&gt;&gt; values[&#39;x&#39;]</span><br><span class="line">1</span><br><span class="line">&gt;&gt;&gt; values</span><br><span class="line">ChainMap(&#123;&#39;x&#39;: 1&#125;)</span><br></pre></td></tr></table></figure>
<p><em>Remarks</em>: <strong>ChainMap()</strong> only maintains a mapping stack without creating new dict, like the way of <em>pass by reference</em>. Any revision of its input will contribute to the value of merged dicts.</p>
<hr>
<p><strong>Reference</strong>: <a href="https://www.oreilly.com/catalog/errata.csp?isbn=9781449340377" target="_blank" rel="noopener"><em>David Beazley, Brian K. Jones, Python Cookbook, 3rd Edition</em></a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title>Processor-based Optimization</title>
    <url>/2020/02/29/2020-02-29-processor-opt/</url>
    <content><![CDATA[<p>Followed by <a href="https://jiaruilu.com/2020/02/27/modern-processors.html" target="_blank" rel="noopener">Introduction to Modern processor</a>, we are going to talk about the several optimization strategies for computation performance in details.</p>
<h3 id="Pipelining"><a href="#Pipelining" class="headerlink" title="Pipelining"></a>Pipelining</h3><p>Just like workers in front of assembly lines in manufactory, pipelines in microprocessors serve the same job: they are all highly <strong>skilled and specialized</strong> for a single task, without knowing the details about the final product.</p>
<p>One program, routine, even a single operation, will be divided into a sequence of instructions, i.e., steps. Each unit is responsible for the specific task and execute the same chore over and over again on different objects, and then transfer them to the next “worker”.</p>
<p>It comes no surprise that if all tasks are carefully tuned such that all of the workers are continuously busy, the whole assembly lines achieve its best efficient, where the peak performance is.</p>
<p>Slightly complex instruction will be through a several-step pipeline. For example, the most simple setup is a <em>“fetch-decode-execute”</em> pipeline. We will look at a task in detail, as a combination of elementary subtasks. There are some concepts to be introduced as follows:</p>
<p><img src="/images/processor-img4.png" alt="alt img1"></p>
<ul>
<li><em>latency</em>(or <em>depth</em>): the time after which the first result is available. (latency=5 cycles, as shown in the figure)</li>
<li><em>wind-up</em> phase: the time steps there are idle units existing since the beginning (wind-up phase is 4-cycle long)</li>
<li><em>wind-down</em> phase: similar to the last one (also 4-cycle long)</li>
</ul>
<p><em>after the wind-up and before the wind-down, all units are continuously busy,generating one result per cycle — throughput!</em></p>
<p>It is evident that <strong>the deeper the pipeline. the larger the number of independent operations must be to achieve reasonable throughput</strong>, because of the overhead caused be the <strong>wind-up</strong> phase.</p>
<p>A potential performance bottleneck in codes that use short, tight loops, i.e., the shorter loop lengths can possibly make the performance worse, in particular for superscalar or vector-processor cases.</p>
<p>Some operations tend to have very large latencies, like several tens of cycles for square root or divide, and more than 100 for trigonometric functions. <em>Avoiding such functions is thus a primary goal of the optimization!</em></p>
<p>Then we will focus on the data. For a “real” pipeline, the codes we execute involves more operations like <em>load</em>, <em>store</em>, <em>address calculations</em>, <em>instruction fetch</em> and <em>decode</em>, etc., which must be overlapped with arithmetic. <strong>Operand of an instruction must find its way from memory to a register, with result writen out.</strong></p>
<ul>
<li><p><em>software pipelining</em>: In order to reduce the case that load operation on A(i) <strong>does not provide the data on time.</strong> It is required to <em>interleave</em> different loop iterations to <strong>abridge the latencies and avoid pipeline stalls.</strong></p>
</li>
<li><p><em>loop-carried dependencies</em>: if a loop iteration <strong>depends on the result of some other iteration</strong>, then the compiler and hardware fail to prevent pipeline stalls. This often leads to some kinds of performance drop.</p>
</li>
</ul>
<p>If the loop length is so large that all data has to be fetched from memory, the impact of pipeline stalls is much less significant. They has to wait for off-chip data!</p>
<h3 id="Superscalarity"><a href="#Superscalarity" class="headerlink" title="Superscalarity"></a>Superscalarity</h3><p>Superscalarity is a special form of parallel execution, and a variant of <em>instruction-level parallelism</em>(<strong>ILP</strong>). If our processor is desigened to support this feature, then it is capable of executing more than one instruction or, more generally, <strong>producing more than one “result” per cycle</strong> . The requirement can be embodied as following details:</p>
<ul>
<li>fetching multiple instructions concurrently</li>
<li>address calculations are performed in multiple integer</li>
<li>multiple floating-point pipelines can run in parallel</li>
<li>caches are fast enough to sustain more than on LD/ST per cycle</li>
</ul>
<p>We need to point out, however, that it is extremely hard for compiler-generated code to achieve a throughput of more than <em>2-3</em> instructions per cycle. If high demands for performance is made, programmers may still resort to assembly language!</p>
<h3 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h3><p>Another <strong>ILP</strong> strategy! Many recent cache-based processors are supporting FP SIMD operations: they allow the concurrent execution of arithmetic operation happening, with a “wide” register that can hold several FP word at a time.</p>
<p><img src="/images/simd-1.png" alt="alt img2"></p>
<p>As shown in the above figure, a single instruction can initiate four addition at once. <strong>This is truly parallel!</strong>(<em>if sufficient arithmetic units are available</em>)</p>
<hr>
<p><strong>Reference</strong>: Hager, Wellein, Introduction to High Performance Computing for Scientists and Engineers(2010)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>High-performance-computing</tag>
      </tags>
  </entry>
  <entry>
    <title>Sorting algorithms</title>
    <url>/2020/02/29/2020-02-29-sort-algorithm/</url>
    <content><![CDATA[<p>There exists such a famous question: how to find the $k$th largest of $n$ unsorted numbers?</p>
<p>In short, we can perceive that such problem can be classified as “Sorting problem”, but more simplier — we don’t actually need the whole sorted array.</p>
<p>First things first, we’d better review the common sorting Algorithm.</p>
<h3 id="Common-Sorting-Algorithm"><a href="#Common-Sorting-Algorithm" class="headerlink" title="Common Sorting Algorithm"></a>Common Sorting Algorithm</h3><h4 id="1-Simple-Selection-sort"><a href="#1-Simple-Selection-sort" class="headerlink" title="1. [Simple] Selection sort"></a>1. [Simple] Selection sort</h4><p><strong>Features</strong>: in-place comparison, simplicity, inefficient for most</p>
<p>The algorithm finds the minimum value, swaps it with the value in the first position, and repeats these steps for the remainder of the list. It does no more than n swaps, and thus is useful where swapping is very expensive.</p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(1)$</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Complexity</strong></p>
<p>$(n-1) + (n-2) + \dots + 1 = \sum_{i=1}^{n-1}i = \frac{1}{2}(n^2-n) \to O(n^2)$</p>
<p><img src="/images/sorting-selectionsort.gif" alt="alt img1"></p>
<p><strong>Implementations</strong><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* a[0] to a[aLength-1] is the array to sort */</span></span><br><span class="line"><span class="keyword">int</span> i,j;</span><br><span class="line"><span class="keyword">int</span> aLength; <span class="comment">// initialise to a's length</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* advance the position through the entire array */</span></span><br><span class="line"><span class="comment">/*   (could do i &lt; aLength-1 because single element is also min element) */</span></span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; aLength<span class="number">-1</span>; i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">/* find the min element in the unsorted a[i .. aLength-1] */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* assume the min is the first element */</span></span><br><span class="line">    <span class="keyword">int</span> jMin = i;</span><br><span class="line">    <span class="comment">/* test against elements after i to find the smallest */</span></span><br><span class="line">    <span class="keyword">for</span> (j = i+<span class="number">1</span>; j &lt; aLength; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">/* if this element is less, then it is the new minimum */</span></span><br><span class="line">        <span class="keyword">if</span> (a[j] &lt; a[jMin])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">/* found new minimum; remember its index */</span></span><br><span class="line">            jMin = j;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (jMin != i)</span><br><span class="line">    &#123;</span><br><span class="line">        swap(a[i], a[jMin]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="2-Simple-Insertion-sort"><a href="#2-Simple-Insertion-sort" class="headerlink" title="2. [Simple] Insertion sort"></a>2. [Simple] Insertion sort</h4><p><strong>Features</strong>: efficient for small or almost sorted ones, as a component for sophisticated algorithms</p>
<p>It works by taking elements from the list one by one and inserting them in their correct position into a new sorted list similar to how we put money in our wallet. In arrays, the new list and the remaining elements can share the array’s space, but insertion is expensive, requiring shifting all following elements over by one.</p>
<p><img src="/images/sorting-insertionsort.gif" alt="alt img2"></p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(n)$</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(1)$</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Implementations</strong><br><strong>Pseudo-code</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">i ← 1</span><br><span class="line">while i &lt; length(A)</span><br><span class="line">    j ← i</span><br><span class="line">    while j &gt; 0 and A[j-1] &gt; A[j]</span><br><span class="line">        swap A[j] and A[j-1]</span><br><span class="line">        j ← j - 1</span><br><span class="line">    end while</span><br><span class="line">    i ← i + 1</span><br><span class="line">end while</span><br></pre></td></tr></table></figure>
<p><strong>C implementation</strong><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">struct LIST * <span class="title">SortList1</span><span class="params">(struct LIST * pList)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// zero or one element in list</span></span><br><span class="line">    <span class="keyword">if</span> (pList == <span class="literal">NULL</span> || pList-&gt;pNext == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> pList;</span><br><span class="line">    <span class="comment">// head is the first element of resulting sorted list</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">LIST</span> * <span class="title">head</span> = <span class="title">NULL</span>;</span></span><br><span class="line">    <span class="keyword">while</span> (pList != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">LIST</span> * <span class="title">current</span> = <span class="title">pList</span>;</span></span><br><span class="line">        pList = pList-&gt;pNext;</span><br><span class="line">        <span class="keyword">if</span> (head == <span class="literal">NULL</span> || current-&gt;iValue &lt; head-&gt;iValue) &#123;</span><br><span class="line">            <span class="comment">// insert into the head of the sorted list</span></span><br><span class="line">            <span class="comment">// or as the first element into an empty sorted list</span></span><br><span class="line">            current-&gt;pNext = head;</span><br><span class="line">            head = current;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// insert current element into proper position in non-empty sorted list</span></span><br><span class="line">            struct LIST * p = head;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (p-&gt;pNext == <span class="literal">NULL</span> || <span class="comment">// last element of the sorted list</span></span><br><span class="line">                    current-&gt;iValue &lt; p-&gt;pNext-&gt;iValue) <span class="comment">// middle of the list</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// insert into middle of the sorted list or as the last element</span></span><br><span class="line">                    current-&gt;pNext = p-&gt;pNext;</span><br><span class="line">                    p-&gt;pNext = current;</span><br><span class="line">                    <span class="keyword">break</span>; <span class="comment">// done</span></span><br><span class="line">                &#125;</span><br><span class="line">                p = p-&gt;pNext;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Insertion sort can also bo implemented in a recursive way, but it will take $O(n)$ space consumption to finish.</p>
<h4 id="3-Bubble-sort"><a href="#3-Bubble-sort" class="headerlink" title="3. Bubble sort"></a>3. Bubble sort</h4><p><strong>Features</strong>: compare adjacent elements, perform poorly(educationally common)</p>
<p><img src="/images/sorting-bubblesort.gif" alt="alt img3"></p>
<p>It compares the first two elements, and if the first is greater than the second, it swaps them. It continues doing this for each pair of adjacent elements to the end of the data set. It then starts again with the first two elements, repeating until no swaps have occurred on the last pass.</p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(n)$</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>$O(1)$</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Implementations</strong><br><strong>Pseudo-code</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">n :&#x3D; length(A)</span><br><span class="line">repeat</span><br><span class="line">    swapped :&#x3D; false</span><br><span class="line">    for i :&#x3D; 1 to n - 1 inclusive do</span><br><span class="line">        if A[i - 1] &gt; A[i] then</span><br><span class="line">            swap(A[i - 1], A[i])</span><br><span class="line">            swapped &#x3D; true</span><br><span class="line">        end if</span><br><span class="line">    end for</span><br><span class="line">    n :&#x3D; n - 1</span><br><span class="line">until not swapped</span><br><span class="line">end procedure</span><br></pre></td></tr></table></figure></p>
<p><strong>C++ Implementation</strong><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubbleSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;arr, <span class="keyword">int</span> <span class="built_in">size</span>)</span></span>&#123; <span class="comment">//descending sort</span></span><br><span class="line">    <span class="keyword">bool</span> noChange = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">size</span>; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; <span class="built_in">size</span>; ++j)</span><br><span class="line">           <span class="keyword">if</span> (arr[i] &lt; arr[j]) swap_element(arr[i], arr[j]), noChange = <span class="literal">false</span>;</span><br><span class="line">           <span class="keyword">if</span> (noChange) <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="4-Efficient-Merge-sort"><a href="#4-Efficient-Merge-sort" class="headerlink" title="4. [Efficient] Merge sort"></a>4. [Efficient] Merge sort</h4><p><strong>Features</strong>: efficient, stable, divide and conquer<br>Conceptually, a merge sort works as follows:</p>
<ol>
<li>Divide the unsorted list into $n$ sublists, each containing one element (a list of one element is considered sorted).</li>
<li>Repeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list.</li>
</ol>
<p><img src="/images/sorting-mergesort.gif" alt="alt img4"></p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td>$O(n)$</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/images/sorting-mergesort2.png" alt="alt img4"></p>
<p><strong>Implementations</strong><br><strong>C Implementation</strong></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Array A[] has the items to sort; array B[] is a work array.</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">TopDownMergeSort</span><span class="params">(A[], B[], n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    CopyArray(A, <span class="number">0</span>, n, B);           <span class="comment">// one time copy of A[] to B[]</span></span><br><span class="line">    TopDownSplitMerge(B, <span class="number">0</span>, n, A);   <span class="comment">// sort data from B[] into A[]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Sort the given run of array A[] using array B[] as a source.</span></span><br><span class="line"><span class="comment">// iBegin is inclusive; iEnd is exclusive (A[iEnd] is not in the set).</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">TopDownSplitMerge</span><span class="params">(B[], iBegin, iEnd, A[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(iEnd - iBegin &lt; <span class="number">2</span>)                       <span class="comment">// if run size == 1</span></span><br><span class="line">        <span class="keyword">return</span>;                                 <span class="comment">//   consider it sorted</span></span><br><span class="line">    <span class="comment">// split the run longer than 1 item into halves</span></span><br><span class="line">    iMiddle = (iEnd + iBegin) / <span class="number">2</span>;              <span class="comment">// iMiddle = mid point</span></span><br><span class="line">    <span class="comment">// recursively sort both runs from array A[] into B[]</span></span><br><span class="line">    TopDownSplitMerge(A, iBegin,  iMiddle, B);  <span class="comment">// sort the left  run</span></span><br><span class="line">    TopDownSplitMerge(A, iMiddle,    iEnd, B);  <span class="comment">// sort the right run</span></span><br><span class="line">    <span class="comment">// merge the resulting runs from array B[] into A[]</span></span><br><span class="line">    TopDownMerge(B, iBegin, iMiddle, iEnd, A);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//  Left source half is A[ iBegin:iMiddle-1].</span></span><br><span class="line"><span class="comment">// Right source half is A[iMiddle:iEnd-1   ].</span></span><br><span class="line"><span class="comment">// Result is            B[ iBegin:iEnd-1   ].</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">TopDownMerge</span><span class="params">(A[], iBegin, iMiddle, iEnd, B[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    i = iBegin, j = iMiddle;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// While there are elements in the left or right runs...</span></span><br><span class="line">    <span class="keyword">for</span> (k = iBegin; k &lt; iEnd; k++) &#123;</span><br><span class="line">        <span class="comment">// If left run head exists and is &lt;= existing right run head.</span></span><br><span class="line">        <span class="keyword">if</span> (i &lt; iMiddle &amp;&amp; (j &gt;= iEnd || A[i] &lt;= A[j])) &#123;</span><br><span class="line">            B[k] = A[i];</span><br><span class="line">            i = i + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            B[k] = A[j];</span><br><span class="line">            j = j + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CopyArray</span><span class="params">(A[], iBegin, iEnd, B[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(k = iBegin; k &lt; iEnd; k++)</span><br><span class="line">        B[k] = A[k];</span><br></pre></td></tr></table></figure>
<h4 id="5-Efficient-Quick-sort"><a href="#5-Efficient-Quick-sort" class="headerlink" title="5. [Efficient] Quick sort"></a>5. [Efficient] Quick sort</h4><p><strong>Features</strong>: <em>divide and conquer, efficient, partition-exchange, ‘pivot’ element</em></p>
<p>It works by selecting a ‘pivot’ element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively.</p>
<p><strong>Core</strong>: choice of pivot &amp; repeated elements</p>
<p><img src="/images/sorting-quicksort.png" alt="alt img5"></p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(n)$</td>
<td>$O(nlogn)$</td>
<td>$O(n^2)$</td>
<td>$O(nlogn)$</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Implementations</strong><br><strong>Pseudo-code-1</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">algorithm quicksort(A, lo, hi) is</span><br><span class="line">    if lo &lt; hi then</span><br><span class="line">        p :&#x3D; partition(A, lo, hi)</span><br><span class="line">        quicksort(A, lo, p)</span><br><span class="line">        quicksort(A, p + 1, hi)</span><br><span class="line"></span><br><span class="line">algorithm partition(A, lo, hi) is</span><br><span class="line">    pivot :&#x3D; A[⌊(hi + lo) &#x2F; 2⌋] &#x2F;&#x2F;round the division result towards zero</span><br><span class="line">    i :&#x3D; lo - 1</span><br><span class="line">    j :&#x3D; hi + 1</span><br><span class="line">    loop forever</span><br><span class="line">        do</span><br><span class="line">            i :&#x3D; i + 1</span><br><span class="line">        while A[i] &lt; pivot</span><br><span class="line">        do</span><br><span class="line">            j :&#x3D; j - 1</span><br><span class="line">        while A[j] &gt; pivot</span><br><span class="line">        if i ≥ j then</span><br><span class="line">            return j</span><br><span class="line">        swap A[i] with A[j]</span><br></pre></td></tr></table></figure><br><strong>Pseudo-code-2</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">algorithm quicksort(A, lo, hi) is</span><br><span class="line">    if lo &lt; hi then</span><br><span class="line">        p :&#x3D; partition(A, lo, hi)</span><br><span class="line">        quicksort(A, lo, p - 1)</span><br><span class="line">        quicksort(A, p + 1, hi)</span><br><span class="line"></span><br><span class="line">algorithm partition(A, lo, hi) is</span><br><span class="line">    pivot :&#x3D; A[hi]</span><br><span class="line">    i :&#x3D; lo</span><br><span class="line">    for j :&#x3D; lo to hi do</span><br><span class="line">        if A[j] &lt; pivot then</span><br><span class="line">            swap A[i] with A[j]</span><br><span class="line">            i :&#x3D; i + 1</span><br><span class="line">    swap A[i] with A[hi]</span><br><span class="line">    return i</span><br></pre></td></tr></table></figure><br><strong>C++ Implementation</strong><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span> <span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> pivot = arr[high];    <span class="comment">// pivot</span></span><br><span class="line">    <span class="keyword">int</span> i = (low - <span class="number">1</span>);  <span class="comment">// Index of smaller element</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = low; j &lt;= high- <span class="number">1</span>; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// If current element is smaller than or</span></span><br><span class="line">        <span class="comment">// equal to pivot</span></span><br><span class="line">        <span class="keyword">if</span> (arr[j] &lt;= pivot)</span><br><span class="line">        &#123;</span><br><span class="line">            i++;    <span class="comment">// increment index of smaller element</span></span><br><span class="line">            swap(&amp;arr[i], &amp;arr[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    swap(&amp;arr[i + <span class="number">1</span>], &amp;arr[high]);</span><br><span class="line">    <span class="keyword">return</span> (i + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* The main function that implements QuickSort</span></span><br><span class="line"><span class="comment"> arr[] --&gt; Array to be sorted,</span></span><br><span class="line"><span class="comment">  low  --&gt; Starting index,</span></span><br><span class="line"><span class="comment">  high  --&gt; Ending index */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (low &lt; high)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">/* pi is partitioning index, arr[p] is now</span></span><br><span class="line"><span class="comment">           at right place */</span></span><br><span class="line">        <span class="keyword">int</span> pi = partition(arr, low, high);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Separately sort elements before</span></span><br><span class="line">        <span class="comment">// partition and after partition</span></span><br><span class="line">        quickSort(arr, low, pi - <span class="number">1</span>);</span><br><span class="line">        quickSort(arr, pi + <span class="number">1</span>, high);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="6-Efficient-Heapsort"><a href="#6-Efficient-Heapsort" class="headerlink" title="6. [Efficient] Heapsort"></a>6. [Efficient] Heapsort</h4><p><strong>Features</strong>: finding the next largest element takes O(log n) time</p>
<p>Like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.</p>
<p><img src="/images/sorting-heapsort.gif" alt="alt img6"></p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td>$O(nlogn)$</td>
<td>$O(1)$</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Implementations</strong><br><strong>Pseudo-code</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">procedure heapify(a,count) is</span><br><span class="line">    (end is assigned the index of the first (left) child of the root)</span><br><span class="line">    end :&#x3D; 1</span><br><span class="line"></span><br><span class="line">    while end &lt; count</span><br><span class="line">        (sift up the node at index end to the proper place such that all nodes above</span><br><span class="line">         the end index are in heap order)</span><br><span class="line">        siftUp(a, 0, end)</span><br><span class="line">        end :&#x3D; end + 1</span><br><span class="line">    (after sifting up the last node all nodes are in heap order)</span><br><span class="line"></span><br><span class="line">procedure siftUp(a, start, end) is</span><br><span class="line">    input:  start represents the limit of how far up the heap to sift.</span><br><span class="line">                  end is the node to sift up.</span><br><span class="line">    child :&#x3D; end</span><br><span class="line">    while child &gt; start</span><br><span class="line">        parent :&#x3D; iParent(child)</span><br><span class="line">        if a[parent] &lt; a[child] then (out of max-heap order)</span><br><span class="line">            swap(a[parent], a[child])</span><br><span class="line">            child :&#x3D; parent (repeat to continue sifting up the parent now)</span><br><span class="line">        else</span><br><span class="line">            return</span><br></pre></td></tr></table></figure></p>
<h4 id="7-Efficient-Shellsort"><a href="#7-Efficient-Shellsort" class="headerlink" title="7. [Efficient] Shellsort"></a>7. [Efficient] Shellsort</h4><p><strong>Features</strong>: generalization of insertion sort, gap sequences,</p>
<p><img src="/images/sorting-shellsort.png" alt="alt img7"></p>
<p>The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. Starting with far apart elements, it can move some out-of-place elements into position faster than a simple nearest neighbor exchange</p>
<p><img src="/images/sorting-shellsort2.png" alt="alt img7"></p>
<p>The first pass, 5-sorting, performs insertion sort on five separate subarrays (a1, a6, a11), (a2, a7, a12), (a3, a8), (a4, a9), (a5, a10). For instance, it changes the subarray (a1, a6, a11) from (62, 17, 25) to (17, 25, 62). The next pass, 3-sorting, performs insertion sort on the three subarrays (a1, a4, a7, a10), (a2, a5, a8, a11), (a3, a6, a9, a12). The last pass, 1-sorting, is an ordinary insertion sort of the entire array (a1,…, a12).</p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(nlogn)$</td>
<td>$O(n^{4/3})$</td>
<td>$O(n^{4/3})$</td>
<td>$O(1)$</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Implementations</strong><br><strong>Pseudo-code</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Sort an array a[0...n-1].</span><br><span class="line">gaps &#x3D; [701, 301, 132, 57, 23, 10, 4, 1]</span><br><span class="line"></span><br><span class="line"># Start with the largest gap and work down to a gap of 1</span><br><span class="line">foreach (gap in gaps)</span><br><span class="line">&#123;</span><br><span class="line">    # Do a gapped insertion sort for this gap size.</span><br><span class="line">    # The first gap elements a[0..gap-1] are already in gapped order</span><br><span class="line">    # keep adding one more element until the entire array is gap sorted</span><br><span class="line">    for (i &#x3D; gap; i &lt; n; i +&#x3D; 1)</span><br><span class="line">    &#123;</span><br><span class="line">        # add a[i] to the elements that have been gap sorted</span><br><span class="line">        # save a[i] in temp and make a hole at position i</span><br><span class="line">        temp &#x3D; a[i]</span><br><span class="line">        # shift earlier gap-sorted elements up until the correct location for a[i] is found</span><br><span class="line">        for (j &#x3D; i; j &gt;&#x3D; gap and a[j - gap] &gt; temp; j -&#x3D; gap)</span><br><span class="line">        &#123;</span><br><span class="line">            a[j] &#x3D; a[j - gap]</span><br><span class="line">        &#125;</span><br><span class="line">        # put temp (the original a[i]) in its correct location</span><br><span class="line">        a[j] &#x3D; temp</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="8-Distribution-Bucket-sort"><a href="#8-Distribution-Bucket-sort" class="headerlink" title="8. [Distribution] Bucket sort"></a>8. [Distribution] Bucket sort</h4><p><strong>Features</strong>: <em>divide and conquer</em></p>
<p><strong>Bucket sort works as follows:</strong></p>
<p>Set up an array of initially empty “buckets”.</p>
<p>Scatter: Go over the original array, putting each object in its bucket.</p>
<p>Sort each non-empty bucket.</p>
<p>Gather: Visit the buckets in order and put all elements back into the original array.</p>
<p><img src="/images/sorting-bucket1.png" alt="alt img8"><br><img src="/images/sorting-bucket2.png" alt="alt img8"></p>
<p><strong>Performance</strong>:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>type</th>
<th>best</th>
<th>average</th>
<th>worst</th>
<th>memory</th>
<th>stable</th>
</tr>
</thead>
<tbody>
<tr>
<td>uniform keys</td>
<td>-</td>
<td>$O(n+k)$</td>
<td>$O(n^2+k )$</td>
<td>$O(n*k)$</td>
<td>Yes</td>
</tr>
<tr>
<td>integer keys</td>
<td>-</td>
<td>$O(n+r)$</td>
<td>$O(n+r)$</td>
<td>$O(n+r)$</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Implementations</strong><br><strong>Pseudo-code</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function bucketSort(array, k) is</span><br><span class="line">    buckets ← new array of k empty lists</span><br><span class="line">    M ← the maximum key value in the array</span><br><span class="line">    for i &#x3D; 1 to length(array) do</span><br><span class="line">        insert array[i] into buckets[floor(k × array[i] &#x2F; M)]</span><br><span class="line">    for i &#x3D; 1 to k do</span><br><span class="line">        nextSort(buckets[i])</span><br><span class="line">    return the concatenation of buckets[1], ...., buckets[k]</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>Remarks</strong>: <em>However, for high-efficiency purpose, hybrid algorithm, combining an asymptotically efficient algorithm for overall sort with insertion sort for small list at hte bottom of a recursion. For java and python, built-in implementations adopt more sophisticated variants like Timsort.</em></p>
<hr>
<p><strong>Reference</strong>: <a href="https://en.wikipedia.org/wiki/Sorting_algorithm" target="_blank" rel="noopener">Wikipedia: Sorting Algorithm</a></p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Modern processor</title>
    <url>/2020/02/27/2020-02-27-modern-processors/</url>
    <content><![CDATA[<h3 id="Program-and-processor-generality"><a href="#Program-and-processor-generality" class="headerlink" title="Program and processor: generality"></a>Program and processor: generality</h3><p>When we run the program consisting a number of code lines, what the computer actually do? First things first, we need to get to know the computer architecture. And many of us know, bascially, that a computer roughly contains CPU, memory and I/O interface.</p>
<p>Memory is used for storing data(including instructions), and instructions are read and executed by a control unit inside CPU, like a clear schedule. Meanwhile, a sepatate arithmetic/logic unit takes its responsibility for the actually computation and manipulation of data stored in memory as well as instructions. These two types of units, along with the interfaces to memory and I/O, as whole being, are called <em>Central Processing Unit(CPU)</em>.</p>
<p><img src="/images/processor-img1.png" alt="alt img1"></p>
<p>Such architecture is so-called <strong><em>stored-program digital computer</em></strong>. General programs are a number of instructions feeding the control unit.</p>
<p>Then we will introduce to you another important role: compiler. It translates the constructs of a high-level language like C/C++ into instructions that can be stored in memory and executed directly by a specific computer. Most of the cases, executable programs are binary sequence.</p>
<p>Such simple system is clear. But there are some inherent problems therein:</p>
<ul>
<li><strong><em>von Neumann bottleneck</em></strong>: Instructions and data must be continuously fed to the control and arithmetic units, so that the speed of the memory interface poses a limitation on compute performance.</li>
<li>SISD (Single Instruction Single Data): Apparently, the architecture is inherently sequential, just able to process a single data at a time.</li>
</ul>
<h3 id="Cache-based-arch"><a href="#Cache-based-arch" class="headerlink" title="Cache-based arch"></a>Cache-based arch</h3><p>Let’s get closer to the modern processor and introduce the concept of cache.</p>
<p><img src="/images/processor-img2.png" alt="alt img2"></p>
<p>The figure above shows a very simplified diagram of a modern cache-based general- purpose microprocessor. It has the following features:</p>
<ul>
<li>arithmetic units are composed of floating-point(FP) and integer(INT) operations which occupy very small fraction of the chip area.</li>
<li><em>registers</em> are divided into FP and INT varieties holding operands to be accessed by instructions</li>
<li>the sturcture diagram is a single core, others on the same chip or socket can share resources like cahes or memory interface</li>
<li>load(LD) and store(ST) units handles instructions that transfer data to and from registers.</li>
<li>instructions are sorted into several queues, wanting to be executed <em>(but probably not in the order they were issued)</em></li>
<li>caches are responsible to <strong>hold data and instructions</strong> to be (re-)used soon, and the majority of the chip area is composed of caches</li>
</ul>
<p>Along with these elements, a lot of additional logic including branch prediction, reorder buffers, data shortcuts, transaction queues,etc. is also built into modern processors. Multicore processors, which superseded the single-core designs, appear during the last decade. In such a chip, several processors(cores) execute code concurrently and share resources.</p>
<h3 id="Benchmarks-performance-metrics"><a href="#Benchmarks-performance-metrics" class="headerlink" title="Benchmarks: performance metrics"></a>Benchmarks: performance metrics</h3><p>When the members in the CPU work together and operate at some maximum speed, we call it <em>peak performance</em>. Whether the limit can be reached <strong>with a specific code(program)</strong> depends on many factors. Firstly, we need introduce some basic metrics to quantify the “speed” of a CPU.</p>
<p>$Flops/sec$: floating-points operations per second, describing the performance at which the FP units generate results for <strong>multiply</strong> and <strong>add</strong> operations(more complicated arithmetic operations like divide, sqrt, etc. often share resources with <strong>multiply</strong> and <strong>add</strong> units)</p>
<p>Standard commodity microprocessors are designed to deliver at most 2 or 4 double-precision FP results per clock cycle. With typical clock frequencies between 2 and 3 GHz, the peak performance is often between 4 and 12 GFlops/sec per core</p>
<p>Another metrics is $GBytes/sec$, which measure the <em>bandwidth</em> of the important data paths to and from the caches and main memory. Feeding arithmetic units with operands is often a rather complicated task. A specific algorithm or process of computation is usually defined by <strong>manipulation of data</strong>; a concrete implementation of algorithm must run on real hardware and have to be through all data paths, where the performance is limited, especially those to main memory.</p>
<p><img src="/images/processor-img3.png" alt="alt img3"></p>
<p>The following code block exhibits how the low-level benchmarking is doing.</p>
<figure class="highlight fortran"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> <span class="keyword">precision</span>, <span class="keyword">dimension</span>(N) :: A,B,C,D</span><br><span class="line"><span class="keyword">double</span> <span class="keyword">precision</span> :: S,E,MFLOPS</span><br><span class="line"></span><br><span class="line"><span class="keyword">do</span> i=<span class="number">1</span>,N <span class="comment">!initialize arrays</span></span><br><span class="line">  A(i) = <span class="number">0.d0</span>; B(i) = <span class="number">1.d0</span></span><br><span class="line">  C(i) = <span class="number">2.d0</span>; D(i) = <span class="number">3.d0</span></span><br><span class="line"><span class="keyword">enddo</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> get_walltime(S) <span class="comment">! get time stamp</span></span><br><span class="line"><span class="keyword">do</span> j=<span class="number">1</span>,R</span><br><span class="line">  <span class="keyword">do</span> i=<span class="number">1</span>,N</span><br><span class="line">    A(i) = B(i) + C(i) * D(i) <span class="comment">! 3 loads, 1 store</span></span><br><span class="line">  <span class="keyword">enddo</span></span><br><span class="line">  <span class="keyword">if</span>(A(<span class="number">2</span>).lt<span class="number">.0</span>) <span class="keyword">call</span> dummy(A,B,C,D) <span class="comment">! prevent loop interchange</span></span><br><span class="line"><span class="keyword">enddo</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> get_walltime(E)            <span class="comment">! get time stamp</span></span><br><span class="line">MFLOPS = R*N*<span class="number">2.d0</span>/((E-S)*<span class="number">1.d6</span>)  <span class="comment">! compute MFlop/sec rate</span></span><br></pre></td></tr></table></figure>
<p>Notice:</p>
<ul>
<li>The triple data stream B,C,D are independent</li>
<li>dummy() is to avoid implicit optimization of compiler</li>
<li>walltime is what we need because the CPUtime does not encompass the contribution of I/O, context switches, etc..</li>
<li>vector processor has ascendancy over such domain s of applicability</li>
</ul>
<h3 id="Improvement-of-performance-strategies-on-processors"><a href="#Improvement-of-performance-strategies-on-processors" class="headerlink" title="Improvement of performance: strategies on processors"></a>Improvement of performance: strategies on processors</h3><p>Chip transistor counts and clock speeds are important physical properties for the performance of a CPU. And there are a multitude of concepts have been developed, and the some of them will be discussed at large <em>in the next post</em>.</p>
<p><img src="/images/processor-img4.png" alt="alt img4"></p>
<h4 id="Pipelined-functional-units"><a href="#Pipelined-functional-units" class="headerlink" title="Pipelined functional units:"></a>Pipelined functional units:</h4><p>By subdividing complex operations(like, e.g., FP addition and multiplication) into simple ones and allocating them to specific function units on the CPU, instruction throughput can be improved greatly. This is a brilliant idea of <em>instruction-level parallelism(ILP)</em>.</p>
<h4 id="SIMD-data-parallelism"><a href="#SIMD-data-parallelism" class="headerlink" title="SIMD(data parallelism):"></a>SIMD(data parallelism):</h4><p>In order to improve the performance, the special registers are able to process identical instruction(data-independent loop) at the same time. Do the same thing to an array of data!</p>
<h4 id="Superscalarity"><a href="#Superscalarity" class="headerlink" title="Superscalarity:"></a>Superscalarity:</h4><p>Provide “direct” instruction-level parallelism</p>
<h4 id="Larger-caches"><a href="#Larger-caches" class="headerlink" title="Larger caches:"></a>Larger caches:</h4><p>For those frequent read-and-write program, enlarging the on-chip “memories” manifest a efficient improvement in performance due to the abridge between processor and memory. But there is some tradeoff as big caches tends to be slower.</p>
<h4 id="Simplified-instruction-set"><a href="#Simplified-instruction-set" class="headerlink" title="Simplified instruction set:"></a>Simplified instruction set:</h4><p>If it takes fewer clock cycles to execute one instruction, then the whole burden on programmers is lightened and memory is saved.</p>
<p>More complexity, does not always traslate into more efficiency(<em>Moore’s Law fails!</em>). When more functional units are crammed into a CPU, the average code will be in the traffic jam, due to the limited independent instructions in a sequential instruction.</p>
<p>Then we can consider multicore processors: several CPU cores on a single socket, maybe which can be the main stream of solution to the improvement of hardware-level performance.</p>
<hr>
<p><strong>Reference</strong>: Hager, Wellein, Introduction to High Performance Computing for Scientists and Engineers(2010)</p>
]]></content>
      <categories>
        <category>Computer-science</category>
      </categories>
      <tags>
        <tag>High-performance-computing</tag>
      </tags>
  </entry>
  <entry>
    <title>Academic Writing 1</title>
    <url>/2020/02/26/2020-02-26-academic-writing-1/</url>
    <content><![CDATA[<p>Unlike daily discussion or chatting, academic writing, as one of the most formal communicative tools, has its own features. I decide to make my present study plan encompass the academic writing, for I may need to writing some academic papers in the near future.</p>
<p>Not confined in research paper publishing, academic phrasal system, with its special characteristics, enable the expression to be more accurate, appropriate, neutral, and less aggressive.</p>
<p>In the first place, some typical phrases and sentences in general academic communication, will be divided into several sections, according to their functions.</p>
<hr>
<h3 id="Cautiousness"><a href="#Cautiousness" class="headerlink" title="Cautiousness"></a>Cautiousness</h3><blockquote>
<p>Avoid expressing absolute certainty, for there may be a small degree of uncertainty;<br>Avoid making over-generalisations, for a small number of exceptions might exist.</p>
</blockquote>
<p>In most of the cases, such statement or claim is mitigated in some way. “Rich in hedging devices” can be used to decribe such style of writing, which is tended to be applied in discussion section of research papers.</p>
<h4 id="How-to-distant-the-author-from-a-proposition"><a href="#How-to-distant-the-author-from-a-proposition" class="headerlink" title="How to distant the author from a proposition?"></a>How to distant the author from a proposition?</h4><ul>
<li>It is believed that…</li>
<li>It has been reported that…</li>
<li>It has commonly been assumed that…</li>
<li>According to Thomas(1999), ….</li>
<li>There is some evidence to suggest that…</li>
</ul>
<h4 id="When-giving-explanations-writing-about-future…"><a href="#When-giving-explanations-writing-about-future…" class="headerlink" title="When giving explanations/ writing about future…"></a>When giving explanations/ writing about future…</h4><h4 id="Be-cautious-of-certainty"><a href="#Be-cautious-of-certainty" class="headerlink" title="(Be cautious of certainty!)"></a>(Be cautious of certainty!)</h4><p>It may be &lt; It could be<br>It is possible &lt; It is probable &lt; It is almost certain</p>
<h4 id="When-explaining-the-results…"><a href="#When-explaining-the-results…" class="headerlink" title="When explaining the results…"></a>When explaining the results…</h4><ul>
<li>This inconsistency <strong>may</strong> be due to…</li>
<li>This discrepancy <strong>could</strong> be attributed to…</li>
<li>A <strong>possible</strong> explanation for these results <strong>may</strong> be the lack of adaquate…</li>
</ul>
<h4 id="Advising-cautious-interpretation-of-results"><a href="#Advising-cautious-interpretation-of-results" class="headerlink" title="Advising cautious interpretation of results"></a>Advising cautious interpretation of results</h4><ul>
<li>These findings cannot be extrapolated to all…</li>
<li>However, with a small sample size, caution must be applied, as the findings might not be…</li>
<li>Such account must be approached with some caution…</li>
<li>These results do not rule out the influence of other factors in…</li>
</ul>
<h4 id="When-discussing-implications"><a href="#When-discussing-implications" class="headerlink" title="When discussing implications.."></a>When discussing implications..</h4><ul>
<li>The findings of this study suggest that</li>
<li><strong>Initial observations</strong> suggest that there <strong>may</strong> be a link between..</li>
<li>The data reported here <strong>appear to support</strong> the assumption that…</li>
</ul>
<h4 id="When-discussing-recommendations…"><a href="#When-discussing-recommendations…" class="headerlink" title="When discussing recommendations…"></a>When discussing recommendations…</h4><ul>
<li>Another possible area of future research would be investigate why…</li>
<li>A reasonable approach to tackle this issue could be to…</li>
<li>These results would seem to suggest that the…</li>
</ul>
<h4 id="Statements-avoiding-over-generalisation"><a href="#Statements-avoiding-over-generalisation" class="headerlink" title="Statements avoiding over-generalisation"></a>Statements avoiding over-generalisation</h4><ul>
<li>In general terms, this means/requires…</li>
<li>X is generally assumed to play a role in…</li>
<li>Generally accepted methods for X include: …</li>
</ul>
<p>occasionally, sometimes &lt; often, frequently &lt; generally, nearly always</p>
<p>certain types of, some of &lt; many of, the majority of &lt; almost all, most</p>
<hr>
<h3 id="Being-critical"><a href="#Being-critical" class="headerlink" title="Being critical"></a>Being critical</h3><p>Being critical mean that you do not necessarily agreeing with what you read just because the information has been published. You can also look for reasons why we should not just accept something as being true, which requires your ability to identify latent problems therein. Such constructive criticism suggests that being against alone is often not enough.</p>
<h4 id="Highlighting-inadequacies-of-previous-studies"><a href="#Highlighting-inadequacies-of-previous-studies" class="headerlink" title="Highlighting inadequacies of previous studies"></a>Highlighting inadequacies of previous studies</h4><p><em>(appear in <strong>Introduction</strong>)</em></p>
<ul>
<li>Researchers have not treated <em>X</em> in much detail.</li>
<li>Previous studies of <em>X</em> have not dealt with…</li>
<li>Such approaches, however, have failed to address…</li>
<li>The expreimental data are rather controversial, and there is no general agreement about…</li>
<li>Most studies of <em>X</em> have only been carried out in a small number of areas.</li>
<li>Much of the research up to now failed to explain.</li>
</ul>
<h4 id="Describe-the-weakness-in-a-study"><a href="#Describe-the-weakness-in-a-study" class="headerlink" title="Describe the weakness in a study"></a>Describe the weakness in a study</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Pre-verb</th>
<th style="text-align:center">verb</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">fails to</td>
<td style="text-align:center">specify</td>
</tr>
<tr>
<td style="text-align:center">doesnot</td>
<td style="text-align:center">quantify</td>
</tr>
<tr>
<td style="text-align:center">makes no attempts</td>
<td style="text-align:center">compare</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">separate</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">account for</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">suggest why</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">analyse how</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">ascertain whether</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">distinguish bewteen</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">provide information on</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">address the question of</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">assess the effectiveness of</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">use a standardised method of</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">consider the long term impact of</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">determain the underlying causes of</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">overlook the fact that</td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">offer an adequate explanation for</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Offering-constructive-suggestions"><a href="#Offering-constructive-suggestions" class="headerlink" title="Offering constructive suggestions"></a>Offering constructive suggestions</h4><p>The study/findings/conclusions +<br>would/might have been +<br>(far) more +<br><strong>{useful, original ,relevant, convincing, interesting, persuasive}</strong> +<br>if he/she had +<br><strong>{used, adopted, assessed, included, addressed, considered}</strong></p>
<p><em>eg.</em> A better study would include all the groups of samples.</p>
<h4 id="Introducing-limitations-of-methods"><a href="#Introducing-limitations-of-methods" class="headerlink" title="Introducing limitations of methods"></a>Introducing limitations of methods</h4><ul>
<li>This method of analysis has a number of limitations;</li>
<li>This method does involve potential measurement error.</li>
<li>There are limits to how far the idea/concept of X can be taken.</li>
<li>Approaches of this kind carry with them various well known limitations.</li>
</ul>
<h4 id="Evaluative-adjectives-to-comment-on-research"><a href="#Evaluative-adjectives-to-comment-on-research" class="headerlink" title="Evaluative adjectives to comment on research"></a>Evaluative adjectives to comment on research</h4><div class="table-container">
<table>
<thead>
<tr>
<th>—-</th>
<th>—-</th>
<th>—-</th>
</tr>
</thead>
<tbody>
<tr>
<td>useful</td>
<td>timely</td>
<td>seminal</td>
</tr>
<tr>
<td>detailed</td>
<td>thorough</td>
<td>excellent</td>
</tr>
<tr>
<td>influential</td>
<td>important</td>
<td>innovative</td>
</tr>
<tr>
<td>pioneering</td>
<td>wide-ranging</td>
<td>ground-breaking</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><em>To see more</em>: <a href="http://www.phrasebank.manchester.ac.uk/" target="_blank" rel="noopener">Academic Phrasebank</a></p>
]]></content>
      <categories>
        <category>Writing</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Pólya counting theory</title>
    <url>/2020/02/25/2020-02-25-polya-theorem/</url>
    <content><![CDATA[<p>Pólya counting theory, also known as Pólya enumeration theorem, is one the most important theroem in combinatorics. It offers a method (formula) to calculate the number of orbits of a group action on a given set. It is discovered by George Pólya in 1937. In particular, its application to the enumeration of chemical compounds exhibits great significance.</p>
<p>Now let’s have a brief look at Pólya counting formula, which answer the question: <em>how many different necklace with $n$ beads using $m$ kinds of colors?</em></p>
<h3 id="Narration-of-the-theorem"><a href="#Narration-of-the-theorem" class="headerlink" title="Narration of the theorem"></a>Narration of the theorem</h3><p>Let $X$ be a finite set and let $G$ be a group of permutations of $X$ (or a finite symmetry group that acts on $X$). The set $X$ may represent a finite set of beads, and $G$ may be a chosen group of permutations of the beads. For example, if $X$ is a necklace of n beads in a circle, then rotational symmetry is relevant so $G$ is the cyclic group $C_n$, while if $X$ is a bracelet of n beads in a circle, rotations and reflections are relevant so $G$ is the dihedral group $D_n$ of order $2n$. Suppose further that $Y$ is a finite set of colors — the colors of the beads — so that $Y^X$ is the set of colored arrangements of beads (more formally: $Y^X$ is the set of functions $X\,\to\,Y$.) Then the group $G$ acts on $Y^X$. The Pólya enumeration theorem counts the number of orbits under $G$ of colored arrangements of beads by the following formula:</p>
<script type="math/tex; mode=display">L~=~\frac{1}{|G|}(m^{c(g_1)}+m^{c(g_2)}+m^{c(g_3)}+\dots)</script><p>or more formally,</p>
<script type="math/tex; mode=display">|Y^X/G| = \frac{1}{|G|}(m^{c(g)}</script><p>By scanning the paragraph above, we can find:</p>
<ul>
<li>finite set “beads” $X$</li>
<li>permutation group $G$</li>
<li>finite set “colors” $Y$</li>
</ul>
<p>before diving into the detail, we might as well think again about the “bead” and “color” stuff. If we take every possibility, free of permutation $G$ confinement, into account, now how about the number of choices?</p>
<p>In fact, for each bead, it has $m$-color option. So totally it will be $m^n$ choices of bead arrangements.</p>
<p>Then we turn to the real scenario: if two arrangements is substantially equivalent, for example, overlapping after some operations like rotation, or flipping. We are supposed to treat them as the same necklaces, i.e. increment of counter is $1$.</p>
<h3 id="Definition-birds-of-a-feather"><a href="#Definition-birds-of-a-feather" class="headerlink" title="Definition: birds of a feather"></a>Definition: birds of a feather</h3><p>First things first, we need to conceive a clear picture of equivalent colored bead arrangements.</p>
<p>Suppose we have $5$ beads, with index $1,2,3,4,5$ respectively. We can write down $(1 2 3 4 5)$ as one kind of arrangement of colored beads. At the same time, please keep the original position of each bead in mind, denoted as $(1 2 3 4 5)$.<br>Then we define the permutation, an adjustment of arrangement, as follows:</p>
<script type="math/tex; mode=display">\binom{1,2,3,4,5}{5,1,2,3,4}</script><p>it means we put the Bead-1 at Pos-2, Bead-2 at Pos-3, and so on.</p>
<p>How many premutations for these colored beads in total? We might as well hold the upper line of indices. Then it is easy to know there are $n!(n=5)=120$ permutations.</p>
<p>Then we can move on to think, among all the $120$ permutations, which are the “effective” permutations? Or on the contrary, which are the “uniform” permutations? We have to draw the line between them.</p>
<p>:tada: Luckily, there exists a powerful tool, <a href="https://en.wikipedia.org/wiki/Group_theory" target="_blank" rel="noopener">Group theory</a>, to handle these task.</p>
<p>Consider the set of all the “uniform” permutations. It is easy to see, combination of these permutations will also be tagged as “uniform”. To avoid redundancy, we stop branching out the content into group theory. We can say that, as a well-defined structure of the set of these permuations, they form a permutation group $G_n$. Operation between any elements are closed, which is of great consistency.</p>
<p>So, among all the $m^n$ orbits, we are now going to find out the “uniform” ones, relative to position (1,2,3,4,5), which are called <a href="https://en.wikipedia.org/wiki/Equivalence_class" target="_blank" rel="noopener">Equivalence class</a> $R_1$. Putting $R_1$ aside, the rest of our task is to successively divide all the permutations of group $G$ into different quivalence $R_1,R_2,R_3…$, an exhaustion. Thus, we can answer the initial question by knowing how many equivalece classes can group $G$ be divided?</p>
<p>Leaving out the detailed principle, we now look at the core of the counting method.</p>
<h3 id="Counting-from-Burnside-lemma-to-Polya-Group-theory"><a href="#Counting-from-Burnside-lemma-to-Polya-Group-theory" class="headerlink" title="Counting: from Burnside lemma to Pólya Group theory"></a>Counting: from Burnside lemma to Pólya Group theory</h3><p>Let $G = \{g_1,g_2,\dots,g_s\}$ be the target premutation group(finite) acting on a set $X$, and the number of cycles of a specific element $g_i$, are denoted as $c(g_i)$.</p>
<p>For each $g$ in $G$, let $X^g$ denote the set of elements in X that are fixed by $g$(left invariant by $ g $，fix-point), i.e. $ X^g = \{ x \in X:g.x = x \}$.<br>And the number of orbits are denoted $|X/G|$ :</p>
<p><strong>(Burnside’s Lemma)</strong></p>
<script type="math/tex; mode=display">|X/G| = \frac{1}{|G|}\sum_{g\in G}|X^g|</script><p>or another form,</p>
<script type="math/tex; mode=display">|G||X/G| = \sum_{g\in G}|X^g|</script><p>Thus the number of orbits is equal to the average number of points fixed by an element of $G$.</p>
<p>On top of that, we further decompose any premutation into the combination of the simplest form.</p>
<blockquote>
<p>Like $\binom{1,2,3,4,5}{5,3,2,1,4}$, it can reduce into $(1,5,4)*(2,3)$ -&gt; $2$ cycles</p>
</blockquote>
<p>Then we introduce the following conclusion:</p>
<p>The number of $x_i$-induced fix-points of $g$ = the number of orbits :</p>
<script type="math/tex; mode=display">|X^g| = |m^{c(x_i)}|</script><p>Therefore, we have the Pólya theory:</p>
<script type="math/tex; mode=display">|Y^X/G| = \frac{1}{|G|}(m^{c(g)})</script><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The core of Pólya theory is the treatment of symmetry(rotate, flip, etc). And we use group to describe different permutations, obtaining the form of Pólya counting in the end.</p>
<hr>
<p><strong>Reference</strong>: <a href="https://en.wikipedia.org/wiki/P%C3%B3lya_enumeration_theorem" target="_blank" rel="noopener">Wikipedia: Pólya_enumeration_theorem</a></p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Combinatorics</tag>
      </tags>
  </entry>
</search>
